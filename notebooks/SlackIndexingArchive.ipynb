{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4fa2da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os \n",
    "\n",
    "data_path = \"../data/slack/slack_export_Janelia-Software_30days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "777945fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 users\n"
     ]
    }
   ],
   "source": [
    "id2username = {}\n",
    "id2realname = {}\n",
    "\n",
    "with open(f\"{data_path}/users.json\", 'r') as f:\n",
    "    users = json.load(f)\n",
    "    for user in users:\n",
    "        id = user['id']\n",
    "        id2username[id] = user['name']\n",
    "        id2realname[id] = user['profile']['real_name']\n",
    "\n",
    "print(f\"{len(id2username)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3562756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C011TMUB3UP python\n",
      "C011W6YDV99 random\n",
      "C0128K68NE5 general\n",
      "C013E4ULBFU storage\n",
      "C013EB3CZM1 scientific-visualization\n",
      "C01430CRBHT git-github\n",
      "C0146BJ38PQ wednesday_web_workshop\n",
      "C015MJGSM2S julia\n",
      "C01H5PYR4TW hpc\n",
      "C01J3KE45LG rust\n",
      "C02CTFPCTDM cplusplus\n",
      "C02HDABKNAE code-review\n",
      "C02K818Q3B6 java\n",
      "C031U6KUMNU how-to\n",
      "C032XSC2CJC image_benchmarks\n",
      "C03DJGPC69K programming_languages\n",
      "C03S782CCMD architecture\n",
      "C041XB9U8BX applied-deep-learning\n",
      "C045UGQB4LX globus\n",
      "C049U3BDYPL mastodon\n",
      "C04UUTQVB61 wiki-improvement\n",
      "C057Z7J7F29 easi-fish-pipeline\n",
      "C02K252DJ86 chromatix\n"
     ]
    }
   ],
   "source": [
    "channel2id = {}\n",
    "with open(f\"{data_path}/channels.json\", 'r') as f:\n",
    "    channels = json.load(f)\n",
    "\n",
    "    for channel in channels:\n",
    "        print(f\"{channel['id']} {channel['name']}\")\n",
    "        channel2id[channel['name']] = channel['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1ba5d840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tiago Ferreira has joined Tiago Ferreira'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"<@(.*?)>\", lambda m: id2realname[m.group(1)], \"<@W97623DK2> has joined <@W97623DK2>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dffe86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681844751.750139\n",
      "1681844765.907739\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1681844751.750139]\n",
      "Mark Kittisopikul said: Is it just me or did the standard Github layout change?\n",
      "Mark Kittisopikul said: I kept trying to find the green button by muscle memory, and it wasn't there.\n",
      "Jody Clements said: Yes, Code and Overview buttons are new as well\n",
      "Jody Clements said: Green code button moved to top right, annoyingly\n",
      "Mark Kittisopikul said: and it's smaller\n",
      "Jody Clements said: Obviously developers want to get to the README before the code\n",
      "Mark Kittisopikul said: And the text is overflowing:\n",
      "<image.png>\n",
      "Jody Clements said: you should probably rebase that branch \n",
      "Mark Kittisopikul said: It's not mine\n",
      "Mark Kittisopikul said: That's from here:\n",
      "https://github.com/eschnett/Yggdrasil\n",
      "eschnett/Yggdrasil\n",
      "Collection of builder repositories for BinaryBuilder.jl\n",
      "William Katz said: Definitely highlights crappy READMEs \n",
      "Davis Bennett said: i noticed something similar\n",
      "Others reacted to the previous message with + a total of 1 times.\n",
      "\n",
      "1682018703.146099\n",
      "1682018931.061479\n",
      "1682019028.653599\n",
      "1682019030.486809\n",
      "1682019036.646849\n",
      "1682019132.668309\n",
      "1682019234.665839\n",
      "1682019412.048639\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682018703.146099]\n",
      "William Patton said: whats the best practice on large cluster jobs? I have a job that will take approximately 8 days using all 248 gpu_rtx nodes. Its a blockwise convolutional neural network prediction job so its embarrassingly parallel and scales almost linearly with number of gpus. Is it better to take 120 for 16 days ish or somewhere in between?\n",
      "Ben Arthur said: how long does each of your jobs take?\n",
      "William Patton said: It's just one job. I just need to start a bunch of workers to run prediction in parallel.\n",
      "Ben Arthur said: how long does each worker take to process one task?\n",
      "William Patton said: Oh only a few seconds per block\n",
      "Ken Carlile said: There is a limit on how many gpus you can use at one time. It's programatic, so just submit everything at once.\n",
      "William Patton said: Ah whats the limit?\n",
      "Ken Carlile said: the limit is 150\n",
      "Ken Carlile said: sorry, had to look that up\n",
      "William Patton said: Ok, perfect. I'll stick to 150 then\n",
      "Robert Lines said: That is the limit today.  It can be varied based on the mix of jobs running but generally it is at ~80% of the total gpus.  Also of note there are only 176 gpus available in the gpu_rtx queue.  22 nodes * 8 gpus.  1 of the rtx2080ti nodes is upgraded to OL9 and in the test queue.\n",
      "William Patton said: Oh I see. If I'm using ~80% and cluster usage goes up will it kill any of my workers?\n",
      "\n",
      "1682019947.863859\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682019947.863859]\n",
      "Robert Lines said: It won't kill your jobs we just may reduce the cap to relieve a backlog should it come up.  Right now it is a little higher at ~86% of the gpus because we had another node fail recent and the upgraded os node being out of the queue.  But there hasn't been high contention to encourage reducing it.  All of that is to say that the 150 number may change without notice but it won't kill your jobs if you end up exceeding the limit it will just not start new jobs until you are below the limit.\n",
      "I would encourage you to submit the jobs in the most logical work unit.  Though that may not fit if you are just running a bunch of workers who are waiting around to grab tasks off a work queue.\n",
      "\n",
      "1682020310.047969\n",
      "1682020390.210469\n",
      "1682020506.304319\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682020310.047969]\n",
      "William Patton said: Exactly, it's just a bunch of workers I'm starting that will be grabbing tasks, (in this case just spatial blocks) to read, predict, write.\n",
      "No point in trying to start more workers than the limit if they will just be sitting waiting for a gpu until the rest of the workers have finished the task queue.\n",
      "William Patton said: And it sounds like the only down side to requesting all 150 currently available would be that if one fails, it might not be able to restart\n",
      "Robert Lines said: yep.  If a worker dies does it have to start a new one or will it continue with n-1 workers?  And does it have to have all N workers running before it starts processing?\n",
      "\n",
      "1682602905.366859\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682602905.366859]\n",
      "Virginia Scarlett said: Good morning all! Hope to see you in Photon at 1pm for the How To... interest group. Mark Kittisopikul will present \"How to... embed an Outlook calendar in a Confluence wiki page\". See Hughes Hub for Zoom info and more: https://hhmionline.sharepoint.com/Pages/Calendar/ScheduledEvent.aspx?EventId=104364\n",
      "Others reacted to the previous message with calendar a total of 1 times.\n",
      "\n",
      "1683113400.267969\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683113400.267969]\n",
      "Mark Kittisopikul said: What is Omega?\n",
      "Omega is a LLM-based and tool-armed autonomous agent that demonstrates the potential for Large Language Models (LLMs) to be applied to image processing, analysis and visualisation. Can LLM-based agents write image processing code and napari widgets, correct its coding mistakes, perform follow-up analysis, and control the napari viewer? The answer appears to be yes.https://twitter.com/loicaroyer/status/1653600252807757824\n",
      "Loïc A. Royer :computer::microscope::alembic: :flag-ua: on Twitter\n",
      ":rotating_light: #ChatGPT + @napari_imaging :rotating_light:\n",
      "Releasing my latest weekend project: Omega, an autonomous LLM agent that writes image processing and analysis code, fixes its mistakes, accesses the napari viewer, makes widgets, &amp; more!\n",
      "<https://t.co/5btZq05ro1>\n",
      "@LangChainAI @OpenAI \n",
      "#OmegaAgent\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Philip Hubbard said: \"Do not use this software lightly, it will download libaries by its own volition, write any code that it deems nescessary, it might actually do what you ask, even if it is a bad idea. Also, beware that it might misundertand what you ask and then do something bad. For example, it is unwise to use Omega to delete 'some' files from your system, it might end up deleteing more than that if you are unclear in your request. To be 100% safe, we recommend that you use this software from within a sandboxed virtual machine.\"\n",
      "Philip Hubbard said: It sounds like using it heavily might be risky, too!\n",
      "Mark Kittisopikul said: Perhaps we should screen WarGames (1983)\n",
      "https://m.imdb.com/title/tt0086567/\n",
      "WarGames (1983) - IMDb\n",
      "WarGames: Directed by John Badham. With Matthew Broderick, Dabney Coleman, John Wood, Ally Sheedy. A young man finds a back door into a military central computer in which reality is confused with game-playing, possibly starting World War III.\n",
      "Larissa Heinrich said: The Alamo is already on it https://drafthouse.com/northern-virginia/show/wargames\n",
      "WarGames | Alamo Drafthouse Cinema\n",
      "A potent and thoroughly ‘80s Cold War thriller\n",
      "Mark Kittisopikul said: Wonderful. Who wants to go see it?\n",
      "\n",
      "1683205862.707639\n",
      "1683205993.699479\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683205862.707639]\n",
      "Mark Kittisopikul said: On Tuesday May 9th at 10 am, the EMBL-Janelia Bioimaging Series will be hosting the following talk by Daniele Ancora.\n",
      " has been rescheduled for 1 pm that day for Loic Royer's ChatGPT Napari plugin.\n",
      "Title: Autocorrelation for image fusion of multiply aberrated detections\n",
      "Abstract: Cross-correlation represents a commonly used mathematical tool to estimate statistical properties in images, ranging from studying spatial distributions or temporal fluctuations of objects to estimation (and correction) of misalignments. In this seminar, we will briefly cover the special case of autocorrelation, which we employ to fuse multiple views of the same object subject to different point spread functions. Our idea stems from the fact that autocorrelation could be helpful for implicit realignment in optical computed tomography but can also enable access to reconstructions having a resolution higher than those obtained in real space. We discuss the logic behind the resolution improvement, validating the process experimentally using multi-view light-sheet imaging and SPAD array detection. With this work, we try to set a novel framework for image reconstruction that could be generically applied in any multi-view detection scheme, highlighting future research directions and current limitations of the method.\n",
      "\n",
      "Mark Kittisopikul said: Virtual Info:\n",
      "https://embl-org.zoom.us/j/96306653238?pwd=MEJ1V0laemtWMW5KMDkvcjJWWTJwZz09\n",
      "Meeting ID: 963 0665 3238\n",
      "Passcode: 008788\n",
      "Philip Hubbard said: Here is the Zoom link for Loic Royer's talk on Tuesday, May 9, 1 PM, on the ChatGPT Napari plugin: https://hhmi.zoom.us/j/6829491045?pwd=cEFEZkRQdzRieGIySWtrTWhqRGVTQT09\n",
      "\n",
      "1683212645.692399\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683212645.692399]\n",
      "John Bogovic said: sharepoint dead for anyone else?\n",
      "<Screenshot from 2023-05-04 10-54-38.png>\n",
      "Others reacted to the previous message with + a total of 2 times.\n",
      "Dominique Harajchi said: Yes. IT and Business Solutions are aware and working on it. \n",
      "Others reacted to the previous message with pray a total of 1 times, and with +1 a total of 1 times, and with pray::skin-tone-3 a total of 1 times.\n",
      "\n",
      "1683656357.556339\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683656357.556339]\n",
      "Mark Kittisopikul said: If it's useful at all, I know https://www.linkedin.com/in/logankilpatrick/ who works on developer relations for OpenAI. I asked him to get Loic GPT 4 access.\n",
      "\n",
      "1683728053.268489\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683728053.268489]\n",
      "Stephan Preibisch said: Hi, I just uploaded the recording of yesterday's code-review session on Napari-ChatGPT by Loic Royer to YouTube: https://www.youtube.com/watch?v=JMo6Sn-L_j4\n",
      "Code review of Napari-ChatGPT by Loic Royer (CZI Biohub)\n",
      "Others reacted to the previous message with pray::skin-tone-3 a total of 1 times.\n",
      "\n",
      "1684244275.308909\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684244275.308909]\n",
      "Davis Bennett said: besides myself, how many people use https://en.wikipedia.org/wiki/Tmux?\n",
      "Tmux\n",
      "tmux is an open-source terminal multiplexer for Unix-like operating systems. It allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time. It can also be used to detach processes from their controlling terminals, allowing remote sessions to remain active without being visible.\n",
      "Others reacted to the previous message with + a total of 4 times.\n",
      "Jody Clements said: I still use screen, too much muscle memory to switch over\n",
      "Mark Kittisopikul said: I generally go to screen first\n",
      "John Bogovic said: I use screen for the same reason as Jody Clements\n",
      "Mark Kittisopikul said: The main reason I need a split terminal is usually to look at two editor buffers, and for that I usually use within vim.\n",
      "Others reacted to the previous message with vim a total of 3 times.\n",
      "Konrad Rokicki said: I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.\n",
      "Others reacted to the previous message with exploding_head a total of 1 times, and with octopus a total of 1 times.\n",
      "Kristin Branson said: thanks, seems like a nice alternative to screen, will try it out!\n",
      "Stuart Berg said: +1 for iTerm2, though it sounds like I'd need to up my game to match Konrad.\n",
      "I like it for its \"triggers\" capability, which lets you type something into your remote session that triggers a command on your local MacBook.\n",
      "Adam Taylor said: I have used screen/tmux in the past, but don't use it regularly.  I prefer to use X11 forwarding or NoMachine.\n",
      "Others reacted to the previous message with joy a total of 1 times.\n",
      "Ken Carlile said: I've used all 3. tmux occasionally when I remember, screen more often, and iTerm2 exclusively for when I'm working on nearline or the backup qumulo. i do batches of 20 nodes at once\n",
      "Ken Carlile said: Even at 4K, my screen isn't big enough to make it feasible to do more at a time\n",
      "Davis Bennett said:  I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.Konrad Rokicki can we get a demo of this?\n",
      "Jody Clements said: I used to use csshX to input commands into multiple terminals at the same time, but iTerm2 seems like a better solution now.\n",
      "Ken Carlile said: just make sure the response you get on all of the terminals is the same...\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1684254047.775209\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684254047.775209]\n",
      "Konrad Rokicki said: Just so everyone is aware, there was a hardware failure on NRS and it’s currently down. Systems is working with the vendor to bring it back up.\n",
      "Others reacted to the previous message with +1 a total of 5 times, and with +1::skin-tone-3 a total of 1 times.\n",
      "Konrad Rokicki said: NRS is back up. Thanks to Systems!\n",
      "Others reacted to the previous message with floppy_disk a total of 3 times.\n",
      "\n",
      "1684276589.246029\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684276589.246029]\n",
      "Boaz Mohar said: Anyone having issues with VPN? I get this weird issue from my Mac:\n",
      "SAML Transfer failed. Please contact your system administrator.\n",
      "Detail: FAILURE: No valid assertion found in SAML response\n",
      "\n",
      "Loaded 13 documents\n"
     ]
    }
   ],
   "source": [
    "from decimal import *\n",
    "from llama_index import Document\n",
    "\n",
    "ignored_subtypes = set(['channel_join','channel_leave'])\n",
    "\n",
    "\n",
    "def fix_text(text):\n",
    "    text = re.sub(\"&lt;\", \"<\", text)\n",
    "    text = re.sub(\"&gt;\", \">\", text)\n",
    "    text = re.sub(\"\\n+\", \"\\n\", text)\n",
    "    return text\n",
    "\n",
    "def get(element, key):\n",
    "    if element and key in element:\n",
    "        return element[key]\n",
    "    return None\n",
    "\n",
    "def extract_text(elements):\n",
    "    text = ''\n",
    "    for element in elements:\n",
    "        if 'elements' in element:\n",
    "            text += extract_text(element['elements'])\n",
    "        el_type = get(element, 'type')\n",
    "        if el_type == 'text':\n",
    "            if get(get(element, 'style'), 'code'): text += '`'\n",
    "            text += element['text']\n",
    "            if get(get(element, 'style'), 'code'): text += '`'\n",
    "        elif el_type == 'link':\n",
    "            text += get(element, 'url')\n",
    "        elif el_type == 'rich_text_preformatted':\n",
    "            text += \"\\n\"\n",
    "        elif el_type == 'user':\n",
    "            user_id = element['user_id']\n",
    "            try:\n",
    "                text += id2realname[user_id]\n",
    "            except KeyError:\n",
    "                print(f\"ERROR: no such user {user_id}\")\n",
    "                text += user_id\n",
    "\n",
    "    return text\n",
    "\n",
    "def parse_message(message):\n",
    "    thread_id, text_msg = None, None\n",
    "    if get(message, 'type') == 'message':\n",
    "        if 'subtype' in message and get(message, 'subtype') in ignored_subtypes:\n",
    "            pass\n",
    "        else:\n",
    "            ts = message['ts']\n",
    "            thread_ts = get(message, 'thread_ts') or ts\n",
    "            msg_user = message['user']\n",
    "            try:\n",
    "                realname = id2realname[msg_user]\n",
    "            except KeyError:\n",
    "                realname = message['user_profile']['display_name']\n",
    "                \n",
    "            if 'blocks' in message:\n",
    "                text = extract_text(message['blocks'])\n",
    "            else:\n",
    "                text = message['text']\n",
    "            \n",
    "            text_msg = re.sub(\"<@(.*?)>\", lambda m: id2realname[m.group(1)], text)\n",
    "            text_msg = fix_text(text_msg)\n",
    "\n",
    "            if 'attachments' in message:\n",
    "                for attachment in message['attachments']:\n",
    "                    if 'title' in attachment: text_msg += f\"\\n{fix_text(attachment['title'])}\"\n",
    "                    if 'text' in attachment: text_msg += f\"\\n{fix_text(attachment['text'])}\"\n",
    "                    \n",
    "            if 'files' in message:\n",
    "                for file in message['files']:\n",
    "                    text_msg += f\"\\n<{file['name']}>\"\n",
    "\n",
    "            if 'reactions' in message:\n",
    "                text_msg += f\"\\nOthers reacted to the previous message with \"\n",
    "                r = [f\"{reaction['name']} a total of {reaction['count']} times\" for reaction in message['reactions']]\n",
    "                text_msg += \", and with \".join(r) + \".\"\n",
    "\n",
    "            text_msg = f\"{realname} said: {text_msg}\\n\"\n",
    "            thread_id = Decimal(thread_ts)\n",
    "\n",
    "    return thread_id, text_msg\n",
    "\n",
    "\n",
    "def create_document(channel_id, ts, doc_text):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Document[channel={channel_id},ts={ts}]\")\n",
    "    print(doc_text)\n",
    "    return Document(doc_text, extra_info={\"channel\": channel_id, \"ts\": ts})\n",
    "\n",
    "DOCUMENT_PAUSE_SECS = 300\n",
    "\n",
    "def index_channel(channel_name):\n",
    "    channel_id = channel2id[channel_name]\n",
    "    messages = {}\n",
    "    for messages_file in glob.glob(f\"{data_path}/{channel_name}/*.json\"):\n",
    "        with open(messages_file, 'r') as f:\n",
    "            for message in json.load(f):\n",
    "                #print(message)\n",
    "                try:\n",
    "                    thread_id, text_msg = parse_message(message)\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing\", message)\n",
    "                    raise e\n",
    "                    \n",
    "                if thread_id and text_msg:\n",
    "                    if thread_id not in messages:\n",
    "                        messages[thread_id] = []\n",
    "                    messages[thread_id].append(text_msg)\n",
    "\n",
    "    prev_id = Decimal(0)\n",
    "    thread_ids = list(messages.keys())\n",
    "    thread_ids.sort()\n",
    "\n",
    "    documents = []\n",
    "    doc_text = \"\"\n",
    "    start_ts = None\n",
    "\n",
    "    for thread_id in thread_ids:\n",
    "\n",
    "        # Create a new document whenever messages are separated by a longer pause\n",
    "        if doc_text and thread_id-prev_id > DOCUMENT_PAUSE_SECS:\n",
    "            doc = create_document(channel_id, start_ts, doc_text)\n",
    "            documents.append(doc)\n",
    "            doc_text = \"\"\n",
    "            start_ts = None\n",
    "\n",
    "        print(thread_id)\n",
    "        if not start_ts:\n",
    "            start_ts = str(thread_id)\n",
    "\n",
    "        for text_msg in messages[thread_id]:\n",
    "            doc_text += text_msg\n",
    "\n",
    "        prev_id = thread_id\n",
    "\n",
    "    # Add final document\n",
    "    doc = create_document(channel_id, start_ts, doc_text)\n",
    "    documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = index_channel(\"general\")\n",
    "print(f\"Loaded {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "de7faaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hostname': 'http://[::]:8080', 'modules': {}, 'version': '1.19.2'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify weviate-client is installed and the database is live and ready\n",
    "import weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "assert client.is_live()\n",
    "assert client.is_ready()\n",
    "client.get_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7c417967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! Delete data in Weaviate\n",
    "client.schema.delete_class(\"Slack_Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a5cc25f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681841967.726019\n",
      "1681842140.916879\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1681841967.726019]\n",
      "Davis Bennett said: looks like the guy who made `ruff` (very good linter) got some funding? https://astral.sh/\n",
      "Astral: Next-gen Python tooling\n",
      "Astral builds high-performance developer tools for the Python ecosystem, starting with Ruff, an extremely fast Python linter, written in Rust.\n",
      "Davis Bennett said: not clear how this works as a business \n",
      "William Katz said: Here's what one of their seed investors say:\n",
      "https://www.accel.com/noteworthy/our-seed-investment-in-astral-accelerating-python-development\n",
      "So they are using Vercel as a comparison.\n",
      "Accel - Our Seed Investment in Astral: Accelerating Python Development\n",
      "\n",
      "1682102619.372109\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682102619.372109]\n",
      "Cristian Goina said: I am using zarr library to write to an N5 container. Is it possible to add additional attributes to the n5 dataset when I create the dataset. I am trying to add \"pixelResolution' and 'downsamplingFactors' when I create it but I see these warnings and the attribute is not added to the json attributes:\n",
      "/opt/mambaforge/lib/python3.11/site-packages/zarr/creation.py:241: UserWarning: ignoring keyword argument 'downsamplingFactors'\n",
      "  warn('ignoring keyword argument %r' % k)\n",
      "\n",
      "Davis Bennett said: at the moment it's not possible to add attributes when you create the array/group. you have to create the array / group, and then set the attributes as a separate operation\n",
      "Davis Bennett said: i have a stalled PR that would fix this\n",
      "Cristian Goina said: what is the easiest way to do that?\n",
      "Davis Bennett said: probably `attrs.put` https://github.com/zarr-developers/zarr-python/blob/main/zarr/attrs.py#L123\n",
      "<https://github.com/zarr-developers/zarr-python/blob/main/zarr/attrs.py | attrs.py>\n",
      "```\n",
      "    def put(self, d):\n",
      "```\n",
      "Davis Bennett said: you can also use `attrs.update` if you want to add new key: value pairs\n",
      "https://github.com/zarr-developers/zarr-python/blob/8f11656959c920099d8a6dec5c0abf4663a862b5/zarr/attrs.py#L177\n",
      "<https://github.com/zarr-developers/zarr-python/blob/8f11656959c920099d8a6dec5c0abf4663a862b5/zarr/attrs.py | attrs.py>\n",
      "```\n",
      "    def update(self, *args, **kwargs):\n",
      "```\n",
      "Cristian Goina said: thanks\n",
      "\n",
      "1682338779.842839\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682338779.842839]\n",
      "Davis Bennett said: this project is cool, it's a react frontend for pandas dataframes: https://github.com/man-group/dtale\n",
      "man-group/dtale\n",
      "Visualizer for pandas data structures\n",
      "\n",
      "1682359697.032229\n",
      "1682359703.716459\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682359697.032229]\n",
      "Davis Bennett said: https://github.com/mitsuhiko/rye\n",
      "mitsuhiko/rye\n",
      "an experimental alternative to poetry/pip/pipenv/pyenv/venv/virtualenv/pdm/hatch/…\n",
      "Mark Kittisopikul said: I just saw this independently.\n",
      "Davis Bennett said: from the flask guy\n",
      "\n",
      "1682826101.767059\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682826101.767059]\n",
      "Mark Kittisopikul said: Michael Innerberger, more SVD code:\n",
      "https://github.com/bwlewis/irlbpy\n",
      "bwlewis/irlbpy\n",
      "Truncated SVD by implicitly restarted Lanczos bidiagonalization for Python Numpy\n",
      "\n",
      "1682961981.177319\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682961981.177319]\n",
      "Davis Bennett said: colormap fans might enjoy this package from talley lambert: https://cmap-docs.readthedocs.io/en/latest/\n",
      "cmap\n",
      "Colors and scientific colormaps for python, no dependencies\n",
      "Others reacted to the previous message with +1 a total of 4 times.\n",
      "Stuart Berg said: I also enjoy this one:\n",
      "https://colorcet.holoviz.org/\n",
      "Stuart Berg said: And colormap fans who write neuroglancer annotation shaders might also enjoy this one:\n",
      "https://github.com/kbinani/colormap-shaders\n",
      "kbinani/colormap-shaders\n",
      "A collection of shaders to draw color maps.\n",
      "\n",
      "1682971790.431029\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1682971790.431029]\n",
      "Mark Kittisopikul said: What's the advantage of this being a magic extension?\n",
      "Mark Kittisopikul said: Why not just call `watermark()`?\n",
      "Davis Bennett said: https://github.com/rasbt/watermark\n",
      "rasbt/watermark\n",
      "An IPython magic extension for printing date and time stamps, version numbers, and hardware information\n",
      "\n",
      "1683059223.262349\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683059223.262349]\n",
      "Mark Kittisopikul said: Can anyone figure out a way to do this without starting a thread? I was thinking asyncio at first, but I'm not sure how to make that work with the callback.\n",
      "In [ 1]: import threading, queue, h5py\n",
      "In [ 2]: class ChunkIterator:\n",
      "    ...:     def __init__(self, dataset):\n",
      "    ...:         self.dsid = dataset.id\n",
      "    ...:         self.q = queue.Queue(1)\n",
      "    ...:         self.running = False\n",
      "    ...: \n",
      "    ...:     def _get_chunks(self):\n",
      "    ...:         self.dsid.chunk_iter(self._callback)\n",
      "    ...:         self.q.put(None)\n",
      "    ...: \n",
      "    ...:     def _callback(self, chunkinfo):\n",
      "    ...:         if self.running:\n",
      "    ...:             self.q.put(chunkinfo)\n",
      "    ...:             return None\n",
      "    ...:         else:\n",
      "    ...:             return True\n",
      "    ...: \n",
      "    ...:     def __iter__(self):\n",
      "    ...:         self.running = True\n",
      "    ...:         t = threading.Thread(target=self._get_chunks, daemon=True)\n",
      "    ...:         t.start()\n",
      "    ...:         return iter(self.q.get, None)\n",
      "    ...: \n",
      "In [ 3]: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     citer = ChunkIterator(f[\"test\"])\n",
      "    ...:     for c in citer:\n",
      "    ...:         print(c)\n",
      "This is how I created the test file:\n",
      "In [45]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "    ...:     dset = f.create_dataset(\"test\", (1024,1024), chunks=(16,16))\n",
      "    ...:     dset[:] = 1\n",
      "\n",
      "\n",
      "1683059529.187199\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683059529.187199]\n",
      "Davis Bennett said: nice!\n",
      "Stuart Berg said: Mark Kittisopikul I like the solution you landed on, but now I'm intrigued by the question of whether `async` was an option.  I certainly can't figure it out.  Did you think of a way?\n",
      "Mark Kittisopikul said: I have one way to explore. Run the event loop from the callback.\n",
      "Davis Bennett said: but can you explain what you need concurrency for here?\n",
      "Mark Kittisopikul said: We don't. I'm just trying to get two coroutines to work together to simplify usage.\n",
      "Stuart Berg said: It would allow you to give the user one chunkinfo at a time, without allocating all of them at once.\n",
      "I don't think it's actually important for this use-case.  I think allocating all of it at once is clearly the simpler and (probably) faster solution.  I'm just curious about how it might have worked if you were forced to use async, in a code-golf kinda way.\n",
      "Mark Kittisopikul said: Basically the outline is that I need a return future.  The callback will retrieve the chunk information and then run the event loop.\n",
      "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_until_complete\n",
      "The `_next__` will then yield the chunk info, and then set the future.\n",
      "Event Loop\n",
      "Source code: Lib/asyncio/events.py, Lib/asyncio/base_events.py Preface The event loop is the core of every asyncio application. Event loops run asynchronous tasks and callbacks, perform network IO ...\n",
      "Davis Bennett said: but won't the user then be responsible for awaiting that future to get the results?\n",
      "Mark Kittisopikul said: That would then return control to the callback, which will then return to HDF5, which will then run a new callback restarting the event loop.\n",
      "Mark Kittisopikul said: The user would just iterate using an `async for`\n",
      "Davis Bennett said: ok but then the async bubbles up to the user\n",
      "Davis Bennett said: so this would be a pretty big API change\n",
      "Mark Kittisopikul said: Or we wrap the traditional iterator interface around it.\n",
      "Davis Bennett said: does python async allow nested event loops?\n",
      "Davis Bennett said: because you wouldn't want a method on a class to have conflicts with an outer event loop\n",
      "Mark Kittisopikul said: Technically, I think that is possible. See the link I posted above.\n",
      "Mark Kittisopikul said: A serious impediment here is that async functions fall into a completely different class than normal functions in Python and it's hard to interface between.\n",
      "Davis Bennett said: yep, that's what I mean above when I said the async bubbles up\n",
      "Davis Bennett said: the clean way to use async i think is to just have 1 global event loop\n",
      "Mark Kittisopikul said: An early prototype in Julia I did for an attribute iterator did not require me to use async of threads explicitly. Implicitly, I did launch a Task bound to a Channel (a synchronized thread safe queue).\n",
      "https://github.com/JuliaIO/HDF5.jl/pull/1045/files\n",
      "Mark Kittisopikul said: I think I might be able to capture the current event loop. I might try this later, but it's just a curiosity now.\n",
      "Mark Kittisopikul said: Kerchunk merged my PR:\n",
      "https://github.com/fsspec/kerchunk/pull/331\n",
      "I'm a bit surprised I ended having to so this myself and that they merged it so quickly.\n",
      "I better do some more testing. Does my detection method work if I have h5py 3.8 installed and HDF5 1.12.2?\n",
      "#331 Use H5Dchunk_iter to get chunk information from HDF5 files\n",
      "*Use H5Dchunk_iter to get chunk information from HDF5 files*\n",
      "This pull request makes chunk information retrieval from HDF5 files more efficient, especially when more than 16K chunks are involved. Translation times drop from tens of seconds to fractions of a second.\n",
      "Fix #286\n",
      "Here we replace `dsid.get_chunk_info` with `dsid.chunk_iter` which is available with HDF5 1.14.0 and will soon be available with HDF5 1.12.3. The HDF5 C call is <https://docs.hdfgroup.org/hdf5/develop/group___h5_d.html#title6|`H5Dchunk_iter`>.\n",
      "The is particularly more efficient with a large number of chunks.\n",
      "*Before this pull request: N^2 scaling with the number of chunks*\n",
      "With 16,384 chunks translating takes 13 seconds. With 32,768 chunks, twice as many chunks, translating takes 74 seconds.\n",
      "```\n",
      "In [1]: import kerchunk.hdf, fsspec, h5py\n",
      "In [2]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*2,1024*2), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...:\n",
      "16384\n",
      "In [3]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...:\n",
      "CPU times: user 13.3 s, sys: 6.91 ms, total: 13.3 s\n",
      "Wall time: 13.3 s\n",
      "In [4]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*4,1024*2), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...:\n",
      "32768\n",
      "In [5]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...:\n",
      "CPU times: user 1min 14s, sys: 34.1 ms, total: 1min 14s\n",
      "Wall time: 1min 14s\n",
      "In [6]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*4,1024*4), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...: \n",
      "65536\n",
      "In [7]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...: \n",
      "CPU times: user 6min 33s, sys: 275 ms, total: 6min 33s\n",
      "Wall time: 6min 33s\n",
      "```\n",
      "*After this pull request: Linear scaling with the number of chunks*\n",
      "With 16,384 chunks, translating takes 0.131 seconds. With 32,768 chunks, twice as many chunks, translating takes 0.214 seconds.\n",
      "```\n",
      "In [1]: import kerchunk.hdf, fsspec, h5py\n",
      "In [2]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*2,1024*2), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...: \n",
      "16384\n",
      "In [3]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...: \n",
      "CPU times: user 131 ms, sys: 23.8 ms, total: 155 ms\n",
      "Wall time: 154 ms\n",
      "In [4]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*4,1024*2), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...: \n",
      "32768\n",
      "In [5]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...: \n",
      "CPU times: user 214 ms, sys: 4.39 ms, total: 218 ms\n",
      "Wall time: 217 ms\n",
      "In [6]: with h5py.File(\"pytest.h5\", \"w\") as f:\n",
      "   ...:     dset = f.create_dataset(\"test\", (1024*4,1024*4), chunks=(16,16))\n",
      "   ...:     dset[:] = 1\n",
      "   ...:     print(dset.id.get_num_chunks())\n",
      "   ...: \n",
      "65536\n",
      "In [7]: %%time\n",
      "   ...: with fsspec.open(\"pytest.h5\") as inf:\n",
      "   ...:     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, \"pytest.h5\", inline_threshold=100)\n",
      "   ...:     h5chunks.translate()\n",
      "   ...: \n",
      "CPU times: user 472 ms, sys: 32.4 ms, total: 504 ms\n",
      "Wall time: 503 ms\n",
      "```\n",
      "*Summary*\n",
      "Time for `SingleHdf5ToZarr.translate()`:\n",
      "Davis Bennett said: i think martin durant has a lot of repos, and he's not afraid to move fast \n",
      "Mark Kittisopikul said: This does raise some questions for me though. Do we really need Zarr shards?\n",
      "Davis Bennett said: yes. an hdf5 dependency would suck\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Mark Kittisopikul said: I'm  not saying that. It seems like fsspec already has a general case covered.\n",
      "Davis Bennett said: ah I see, I think martin durant has made this point before, i.e. that kerchunk basically gets you sharding\n",
      "Davis Bennett said: but i'm not up to date on the latest sharding conversations / developments\n",
      "Davis Bennett said: you could come to the zarr meeting today and talk about it \n",
      "Mark Kittisopikul said: If fsspec can just map out the chunks in a HDF5 file and make a Zarr compatible structure, then do we really need another format.\n",
      "What time?\n",
      "Davis Bennett said: 2:00 pm\n",
      "Davis Bennett said: https://zarr.dev/community-calls/\n",
      "home\n",
      "Zarr Community Calls\n",
      "Mark Kittisopikul said: One thing I realized is that the difference between the Zarr shard chunk table and the HDF5 fixed array table is just 4 bytes per chunk.\n",
      "https://docs.hdfgroup.org/hdf5/develop/_f_m_t3.html#FixedArray\n",
      "Davis Bennett said: can you explain first what you want\n",
      "Mark Kittisopikul said: Basically this iterates over chunks.\n",
      "This uses a low-level routine via `chunk_iter` . That callsback into Python to iterate over the chunks. But the https://github.com/h5py/h5py/issues/2249#issuecomment-1513572907.\n",
      "So I'm trying to turn the callback based iteration into a normal looking Python iterator.\n",
      "Comment on #2249 Method or option to iterate over only allocated chunks\n",
      "I kind of dislike all the HDF5 'iterator' functions that take callbacks. There's no good way I can figure out to turn these into a Python iterator. You'd either have to load everything into memory up front, or spin out a separate thread to run the callback-based function, which is a problem.\n",
      "Given that it's pretty easy to implement this based on `get_num_chunks()` and `get_chunk_info()`, I'd probably go for that over another callback-based solution.\n",
      "Davis Bennett said: so you basically want to load chunks in parallel?\n",
      "Davis Bennett said: or is mere concurrency fine\n",
      "Mark Kittisopikul said: That's not the immediate goal actually. All this does is return where the chunks are.\n",
      "Mark Kittisopikul said: This is a simpler version that does the same thing\n",
      "In [13]: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     ds = f[\"test\"]\n",
      "    ...:     ds.id.chunk_iter(print)\n",
      "It outputs the following:\n",
      "...\n",
      "StoreInfo(chunk_offset=(4080, 3152), filter_mask=0, byte_offset=70113184, size=1024)\n",
      "StoreInfo(chunk_offset=(4080, 3168), filter_mask=0, byte_offset=70114208, size=1024)\n",
      "StoreInfo(chunk_offset=(4080, 3184), filter_mask=0, byte_offset=70115232, size=1024)\n",
      "StoreInfo(chunk_offset=(4080, 3200), filter_mask=0, byte_offset=70116256, size=1024)\n",
      "StoreInfo(chunk_offset=(4080, 3216), filter_mask=0, byte_offset=70117280, size=1024)\n",
      "...\n",
      "\n",
      "Davis Bennett said: what's wrong with the simple version then?\n",
      "Davis Bennett said: and yeah, the callback API is stupid\n",
      "Mark Kittisopikul said: h5py dude does not like callbacks and one may want to do something else with the iteration\n",
      "Davis Bennett said: but if `chunk_iter` requires callbacks I don't see how you avoid callbacks here?\n",
      "Mark Kittisopikul said: We're not going to avoid them. We're just not going to expose them as a high level interface\n",
      "Davis Bennett said: so what if your callback is f(x): x\n",
      "Davis Bennett said: it doesn't look like `chunk_iter` returns anything?\n",
      "Davis Bennett said: ahh I see. it doesn't return anything, so you need to create a place to put the data\n",
      "Mark Kittisopikul said: Yup\n",
      "Davis Bennett said: this is a trash API\n",
      "Davis Bennett said: walk away\n",
      "Davis Bennett said: but if you're stuck with it, I think the queue is fine?\n",
      "Davis Bennett said: not sure that you even need a thread\n",
      "Davis Bennett said: does `chunk_iter` run on multiple threads? if not, then you can just use a list or a dict\n",
      "Mark Kittisopikul said: chunk_iter will block\n",
      "Davis Bennett said: ok so then you're fine with mutating a class property of `self` , which can be a non-thread safe data structure\n",
      "Davis Bennett said: if you use a list, you might want to protect access to it by using @property or just implementing setters and getters\n",
      "Davis Bennett said: (i would love to know why `chunk_iter` is callback based instead of a normal python iterator that yields values)\n",
      "Mark Kittisopikul said: Because the C API is callback based\n",
      "Davis Bennett said: but does the python API need to look like the C API?\n",
      "Mark Kittisopikul said: https://docs.hdfgroup.org/hdf5/develop/group___h5_d.html#title6\n",
      "Mark Kittisopikul said: No, that's what I'm trying to sort out here.\n",
      "Davis Bennett said: at least in this case, the amount of data is small, so would it not be possible to use the C API with a callback that fills a dynamically-sized array, and then return that to python?\n",
      "Mark Kittisopikul said: Yea, that interface needs to be written for kerchunk.\n",
      "Mark Kittisopikul said: https://github.com/fsspec/kerchunk/issues/286\n",
      "#286 Efficient HDF5 chunk iteration via HDF5 1.14, h5py 3.8, and H5Dchunk_iter\n",
      "Currently, kerchunk iterates over HDF5 chunks by looping over a linear index passed to `get_chunk_info`. This uses `H5Dget_chunk_info` from the HDF5 C API.\n",
      "<https://github.com/fsspec/kerchunk/blob/ff16c05e3946d75871c047719d2d0138a724b2ca/kerchunk/hdf.py#L523-L524|https://github.com/fsspec/kerchunk/blob/ff16c05e3946d75871c047719d2d0138a724b2ca/kerchunk/hdf.py#L523-L524>\n",
      "Support for `H5Dchunk_iter` was recently released as part of HDF5 1.14.0:  \n",
      "<https://docs.hdfgroup.org/hdf5/v1_14/group___h5_d.html#gac482c2386aa3aea4c44730a627a7adb8|https://docs.hdfgroup.org/hdf5/v1_14/group___h5_d.html#gac482c2386aa3aea4c44730a627a7adb8>  \n",
      "This method iterates through all the chunks contained with a dataset, visiting each chunk once.\n",
      "`H5Dchunk_iter` was incorporated into h5py 3.8.0 as `h5py.h5d.DatasetID.chunk_iter()` when used with HDF5 1.14:  \n",
      "<https://github.com/h5py/h5py/pull/2202|h5py/h5py#2202>\n",
      "Support for `H5Dchunk_iter` is expected in HDF5 <https://github.com/HDFGroup/hdf5/pull/1970|1.12.3> and <https://github.com/HDFGroup/hdf5/pull/1968|1.10.9>.\n",
      "<https://github.com/ajelenak|@ajelenak> implemented a test which checks the equivalence of that call with the current iteration method implemented here:  \n",
      "<https://github.com/h5py/h5py/blob/d2e84badfa5e4d8095bcc5d3db81f8548c340919/h5py/tests/test_dataset.py#L1800-L1826|https://github.com/h5py/h5py/blob/d2e84badfa5e4d8095bcc5d3db81f8548c340919/h5py/tests/test_dataset.py#L1800-L1826>\n",
      "Kerchunk should take advantage of `H5Dchunk_iter` so that it can efficiently iterate over chunks with linear scaling.\n",
      "Mark Kittisopikul said: The main advantage comes when there are lots of chunks (> 10K). kerchunk scales pretty badly in that case.\n",
      "Davis Bennett said: would this work?\n",
      "class ChunkIterator:\n",
      "    def __init__(self, dataset):\n",
      "        self.dsid = dataset.id\n",
      "        self._chunk_info = self._get_chunk_info()\n",
      "    def _get_chunk_info(self):\n",
      "        chunk_info = []\n",
      "        self.dsid.chunk_iter(lambda v: chunk_info.append(v))\n",
      "        return chunk_info\n",
      "    def __iter__(self):\n",
      "        return self._chunk_info.__iter__()\n",
      "\n",
      "Davis Bennett said: dumb solution that just fills a list\n",
      "Mark Kittisopikul said: That kind of works, but we could have a function return the list then. No need to build the class.\n",
      "Mark Kittisopikul said: Thomas Kluyver also does not like \"to load everything into memory up front\"\n",
      "Mark Kittisopikul said: You might as well just do:\n",
      "def get_all_chunk_info(dataset):\n",
      "    chunk_info = []\n",
      "    dataset.id.chunk_iter(lambda v: chunk_info.append(v))\n",
      "    return chunk_info\n",
      "\n",
      "Davis Bennett said: in this case, what's so bad about loading it all at once? it's not a lot of data, no?\n",
      "Mark Kittisopikul said: It's not very big, depending on the number of chunks, linearly.\n",
      "Mark Kittisopikul said: I guess it could be a problem if we a bunch of very small chunks.\n",
      "Mark Kittisopikul said: I'm starting to see a path using asyncio although it's questionable. I run the event loop from the callback.\n",
      "Davis Bennett said: using asyncio seems pretty weird here\n",
      "Davis Bennett said: i don't know how iterators are implemented in C, but that would be the ideal fix for this\n",
      "Mark Kittisopikul said: In C? callbacks. That's why we're in this mess.\n",
      "Davis Bennett said: but normal python iterators work\n",
      "Davis Bennett said: and they are implemented in C\n",
      "Davis Bennett said: so there's some way to use C to make normal python iterators, so I wonder why that can't be done here\n",
      "Davis Bennett said: it sounds like this is an old issue with the hdf5 library, given thomas kluyver's comment, so there must be some blocker\n",
      "Mark Kittisopikul said: Not really, he just likes `get_chunk_info` and doesn't see why this doesn't scale with a large number of chunks:\n",
      "In [62]: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     dsid = f[\"test\"].id\n",
      "    ...:     n = dsid.get_num_chunks()\n",
      "    ...:     for i in range(n):\n",
      "    ...:         print(dsid.get_chunk_info(i))\n",
      "    ...: \n",
      "StoreInfo(chunk_offset=(0, 0), filter_mask=0, byte_offset=4016, size=1024)\n",
      "StoreInfo(chunk_offset=(16, 16), filter_mask=0, byte_offset=5040, size=1024)\n",
      "\n",
      "Mark Kittisopikul said: The problem is that scales very badly.\n",
      "In [78]: %%time\n",
      "    ...: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     dsid = f[\"test\"].id\n",
      "    ...:     n = dsid.get_num_chunks()\n",
      "    ...:     print(\"Number of chunks: \", n)\n",
      "    ...:     ci = []\n",
      "    ...:     for i in range(n):\n",
      "    ...:         ci.append(dsid.get_chunk_info(i))\n",
      "    ...: \n",
      "Number of chunks:  16384\n",
      "CPU times: user 13.1 s, sys: 1.58 ms, total: 13.1 s\n",
      "Wall time: 13.1 s\n",
      "...\n",
      "In [81]: %%time\n",
      "    ...: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     dsid = f[\"test\"].id\n",
      "    ...:     n = dsid.get_num_chunks()\n",
      "    ...:     print(\"Number of chunks: \", n)\n",
      "    ...:     ci = []\n",
      "    ...:     for i in range(n):\n",
      "    ...:         ci.append(dsid.get_chunk_info(i))\n",
      "    ...: \n",
      "Number of chunks:  32768\n",
      "CPU times: user 1min 12s, sys: 37.8 ms, total: 1min 12s\n",
      "Wall time: 1min 12s\n",
      "\n",
      "Mark Kittisopikul said: `chunk_iter` is much faster for that number of chunks:\n",
      "In [85]: %%time\n",
      "    ...: with h5py.File(\"pytest.h5\", \"r\") as f:\n",
      "    ...:     print(\"Number of chunks: \", f[\"test\"].id.get_num_chunks())\n",
      "    ...:     out = get_all_chunk_info(f[\"test\"])\n",
      "    ...: \n",
      "Number of chunks:  32768\n",
      "CPU times: user 84.4 ms, sys: 4.01 ms, total: 88.4 ms\n",
      "Wall time: 87.1 ms\n",
      "\n",
      "Mark Kittisopikul said: How many chunks do you usually deal with?\n",
      "Davis Bennett said: hundreds of thousands, but I don't think I would ever use hdf5 for my data\n",
      "Mark Kittisopikul said: You inspired me to work on kerchunk.\n",
      "https://github.com/fsspec/kerchunk/pull/331\n",
      "<image.png>\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1683128070.008769\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683128070.008769]\n",
      "William Katz said: New language, Mojo, for AI from Chris Lattner that works well with python. \n",
      "https://news.ycombinator.com/item?id=35790367\n",
      "Above also has comments by Lattner.\n",
      "William Katz said: Here’s a short video by Jeremy Howard showing example of matrix multiply from naive python to Mojo. The autotune for underlying hardware is cool.\n",
      "https://youtu.be/6GvB5lZJqcE\n",
      "Jeremy Howard demo for Mojo launch\n",
      "Davis Bennett said: this looks pretty cool! i think the idea of making a superset of python is a great angle\n",
      "William Katz said: And it has ownership for you \n",
      "William Katz said: The team championing Taichi must be having a heart attack.\n",
      "Mark Kittisopikul said: The \"superset of Python\" thing is interesting. They basically just embedded CPython.\n",
      "https://docs.modular.com/mojo/why-mojo.html#intentional-differences-from-python\n",
      "Modular Docs - Why Mojo:fire:\n",
      "A backstory and rationale for why we created the Mojo language.\n",
      "Mark Kittisopikul said: Mojo also does not seem to be open source, yet...\n",
      "https://github.com/modularml/mojo\n",
      "modularml/mojo\n",
      "The Mojo Programming Language\n",
      "Mark Kittisopikul said: I posted another link on Mojo from Jeremy Howard in .\n",
      "https://janelia-dev.slack.com/archives/C03DJGPC69K/p1683163787122779\n",
      "More on Mojo, this time from Jeremy Howard:\n",
      "<https://www.fast.ai/posts/2023-05-03-mojo-launch.html>\n",
      "Mark Kittisopikul said: Why does this seem like an https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish approach to me?\n",
      "Embrace, extend, and extinguish\n",
      "\"Embrace, extend, and extinguish\" (EEE), also known as \"embrace, extend, and exterminate\", is a phrase that the U.S. Department of Justice found that was used internally by Microsoft to describe its strategy for entering product categories involving widely used standards, extending those standards with proprietary capabilities, and then using those differences in order to strongly disadvantage its competitors.\n",
      "The phrase is no longer used by Microsoft, or describes its current position toward Linux or open source generally. Microsoft has \"changed since the days of branding Linux a cancer\" and is currently the largest firm contributing to open-source projects.\n",
      "Davis Bennett said: who is doing EEE here?\n",
      "Davis Bennett said: and what would get extinguished?\n",
      "William Katz said: Not sure EEE applies to open source efforts and it does seem like that's the intention:\n",
      "https://docs.modular.com/mojo/faq.html#will-mojo-be-open-sourced\n",
      "William Katz said: If it is successful, it might shore up a number of the biggest weaknesses in python that might lead to migration from the core language. Embrace, extend, and solidify?\n",
      "William Katz said: The reason for their not being open from the start is understandable, particularly for a relatively  young/small company. It's expensive dealing with a lot of people of varying ability who want to contribute.\n",
      "Why not develop Mojo in the open from the beginning?\n",
      "Mojo is a big project and has several architectural differences from previous languages. We believe a tight-knit group of engineers with a common vision can move faster than a community effort. This development approach is also well-established from other projects that are now open source (such as LLVM, Clang, Swift, MLIR, etc.).\n",
      "William Katz said: I wonder how quickly they can get to a stable, relatively complete first pass language. This roadmap has some pretty big line items: https://docs.modular.com/mojo/roadmap.html\n",
      "Modular Docs - Mojo:fire: roadmap &amp; sharp edges\n",
      "A summary of our Mojo plans, including upcoming features and things we need to fix.\n",
      "Mark Kittisopikul said: \"Yes, we expect that Mojo will be open-sourced\" is not quite the same as being open source. I have no idea what license they will release it under. Until they do so, I think we should treat this as the proprietary software that it is at the moment.They explicitly cite the Objective-C to Swift transition as precedent for what they are doing. Swift did not enhance or \"solidify\" Objective-C. It replaced it completely in the Apple ecosystem. \"Objective-C\" was described as a \"strict superset\" of C while Swift was described as \"Objective-C without the C\".\n",
      "What I'm reading here is they are currently offering a bridge from CPython to Mojo, and eventually they want you to move completely to Mojo and not use CPython at all. If successful, they want you to be using Mojo completely at the end of the day.\n",
      "It looks to be me like Mojo wants to replace Python for AI.\n",
      "Mark Kittisopikul said: Hmm, I wonder if there is any bias here.\n",
      "Since we use R on the Stack Overflow Data Team, we certainly enjoyed examining how the R ecosystem is changing, and seeing that it’s been a part of the rapid expansion of the data science field.\n",
      "https://stackoverflow.blog/2017/10/10/impressive-growth-r/\n",
      "The Impressive Growth of R\n",
      "This article analyzes the growth of R, and discusses how this growth has changed the ecosystem around the programming language.\n",
      "Stuart Berg said: Revisiting the linked plots from above, I think we missed `pandas`, which is the more appropriate tag to compare against when considering python vs. R.\n",
      "https://insights.stackoverflow.com/trends/?tags=matlab%2Cnumpy%2Cr%2Cpandas\n",
      "Adam Taylor said: So are pandas and numpy really declining in popularity?  This is news to me, if so...\n",
      "Adam Taylor said: I guess what I'm asking is if anyone has an explanation of why they seem to be declining in popularity.\n",
      "Mark Kittisopikul said: I suspect folks are shifting to https://github.com/pola-rs/polars\n",
      "pola-rs/polars\n",
      "Fast multi-threaded, hybrid-out-of-core DataFrame library in Rust | Python | Node.js\n",
      "Mark Kittisopikul said: It's weird that you cannot query for polars: https://stackoverflow.com/questions/tagged/python-polars\n",
      "Newest 'python-polars' Questions\n",
      "Stack Overflow | The World’s Largest Online Community for Developers\n",
      "Stuart Berg said: I don't really trust the very end of the pandas trend.  There's no good reason why pandas usage would drop off so abruptly.  It smells like a data issue.\n",
      "I have a hard time believing that the world is shifting to polars.\n",
      "Davis Bennett said: i agree that the dropoff looks artifactual\n",
      "Stuart Berg said: Yeah, there seems to be a tag called `python-polars`, but it isn't searchable in the plots.\n",
      "Mark Kittisopikul said: If it were an artifact, then why does it not affect the other lines? Why does it seem specific to pandas?\n",
      "Stuart Berg said: Who knows?  But it's so striking that it is not reasonable to trust it.  Strong claims require strong evidence...\n",
      "Davis Bennett said: yeah, which is more likely? that the largest dataframe library in python is experiencing a demographic collapse that none of us have observed, or there's something funny going on with the data collection over at stack overflow\n",
      "Davis Bennett said: i'm betting on the second option until i get some other evidence that pandas is losing users. if anything, I would expect an uptick in pandas questions with the breaking changes in pandas 2.0\n",
      "Adam Taylor said: What about numpy?  Are people just shifting to things implemented on top of numpy, so fewer 'raw' numpy questions?\n",
      "Stuart Berg said: Just to be pedantic: A decrease in this graph does not imply a decrease in absolute question counts.  It implies a decrease in percentage terms.  So if suddenly 1M new programmers were born tomorrow and they all preferred VisualBasic, then the `python` curve would go down, despite the fact that no `python` programmers stopped using it.\n",
      "Davis Bennett said:  What about numpy? Are people just shifting to things implemented on top of numpy, so fewer 'raw' numpy questions?\n",
      "that, or they are using the numpy API implemented for pytorch, cupy, tensorflow arrays\n",
      "Mark Kittisopikul said: Maybe there is a better source for pandas help?\n",
      "Mark Kittisopikul said: or numpy help?\n",
      "Adam Taylor said: It's the ChatGPT effect!\n",
      "Others reacted to the previous message with thinking_face a total of 1 times.\n",
      "Stuart Berg said: There are other factors at play, too.  Do these plots count \"closed\" questions (such as questions tagged as \"duplicate\" or just poor)?  If such questions are excluded, then these graphs are also baking in features about the most active moderators in these programming communities.\n",
      "Stuart Berg said: Like, if suddenly it became fashionable to close questions more quickly, then these graphs would go down!\n",
      "Stuart Berg said: And even if they do count \"closed\" questions, presumably closed questions are probably not tagged as accurately as active questions.  (Or so I would speculate.)\n",
      "Also, as the total number of questions increases, then it becomes harder and harder to ask a question that hasn't already been asked.  So you would expect to see fewer non-duplicate questions on a month-by-month basis.\n",
      "Stuart Berg said: I do like the ChatGPT theory.  That's a definite contender as an explanation.  But then you'd still have to ask why it doesn't seem to affect the other plots as strongly...\n",
      "Davis Bennett said: chatpgpt theory is actually pretty good -- there's also a recent drop in sql\n",
      "Davis Bennett said: https://insights.stackoverflow.com/trends/?tags=sql%2Cpandas\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "Davis Bennett said: sql is exactly the kind of thing I expect people to use chat gpt for\n",
      "Stuart Berg said: I wonder what that weird hump is about in the SQL plot: 2013-2015.  Huh??\n",
      "Stuart Berg said: Really makes me wonder if it's a change in moderation policy w.r.t. proper tagging.\n",
      "Adam Taylor said: My theory is that they started re-tagging all the Matlab questions as \"Octave\", and that Matlab is actually super-popular, and growing like a rocket.  I will not be testing this theory.\n",
      "Others reacted to the previous message with laughing a total of 1 times.\n",
      "Stuart Berg said: Uhh...\n",
      "<image.png>\n",
      "Adam Taylor said: (To be clear, any snark in that last was directed at me only.  I think y'all have made a lot of good points in the above.)\n",
      "Adam Taylor said: You weren't supposed to test the theory!\n",
      "Stuart Berg said: I am not disputing that all Octave questions rightfully belong to Matlab...\n",
      "Mark Kittisopikul said: Finally, something within Julia's order of magnitude.\n",
      "https://insights.stackoverflow.com/trends/?tags=julia%2Coctave\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "<image.png>\n",
      "Others reacted to the previous message with joy a total of 1 times.\n",
      "Davis Bennett said: for julia, is it possible that most of the conversation happens somewhere else?\n",
      "Mark Kittisopikul said: TBH, I never ask Julia questions on SO. It's a terrible place for that. I only answer them there. Or ask and then answer my own question.\n",
      "Mark Kittisopikul said: They don't even do Julia syntax highlighting\n",
      "Stuart Berg said: I do hate the StackOverflow UI for editing markdown.\n",
      "https://meta.stackexchange.com/a/356279/258645\n",
      "Mark Kittisopikul said: Julia syntax highlighting is `status-deferred`\n",
      "https://meta.stackexchange.com/questions/355928/julia-syntax-highlighting\n",
      "Julia syntax highlighting\n",
      "Since the syntax highlighter is now highlight.js it would be great to finally get some syntax highlighting for Julia code on Stack Overflow and other sites that might need this tag. Julia is one of...\n",
      "Donald Olbris said: I'm also reading Mojo as, \"take that, Julia\".\n",
      "My meme reaction: big, if true. The approach makes a lot of sense, and Lattner's got a solid track record. I'd love to have that kind of performance available whether it's for AI or something else without losing everything I love about Python.\n",
      "Donald Olbris said: Plus maybe all the people trying to turn Python into a strongly statically-typed language will go over there and stop bothering those of us who got into Python fleeing all that type nonsense. \n",
      "Davis Bennett said: isn't python strongly typed?\n",
      "Donald Olbris said: Yeah, can't believe I made that error after correcting people for years...statically typed is what I meant.\n",
      "Davis Bennett said: but yeah, i get the sentiment, although I think typescript shows how an expressive type system can tame the wilder parts of a dynamic language\n",
      "Stuart Berg said: Seems like this project is still in its nascent form.  If you watch the launch video, it even opens with a little awkward joke about how Lattner doesn't want to launch yet.\n",
      "I don't blame them for not being open source yet.  You could view that as a good sign: They understand that open sourcing it will be a lot of work, not to be taken lightly.\n",
      "Stuart Berg said: As for citing \"their\" (possibly just Lattner's?) prior track record... They do have a degree of credibility.  Aside from Lattner, the only other person I'd trust more to pull of something like this is Anders Hejlsberg.\n",
      "The Swift to Objective-C transition isn't a perfect analogy, but it strikes me as about as good as you could hope for.  Funny, if I were to object to the analogy, it would be in the opposite direction as the objection Mark raised.  Swift uses the same runtime library as Objective-C, making interoperability arguably even tighter than what Mojo seems to be aiming for with Python.\n",
      "Davis Bennett said: if projects like mojo, triton, taichi etc are any indication, it seems like the most attractive strategy for fixing performance problems in python is to make additions / extensions / enhancements to cpython, as opposed to minting entirely new languages (like julia)\n",
      "Davis Bennett said: (I mean \"attractive\" here in the empirical sense, i.e. what orgs seem to be trying to do)\n",
      "Davis Bennett said: *or, if making an new language (like taichi I guess?), it ends up looking just like python\n",
      "Mark Kittisopikul said: It is not clear to me that Mojo is an extension or enhancement to Python or specifically CPython. It seems to be co-opting the syntax and API for Mojo itself while embedding CPython for backwards compatibility.\n",
      "The demo clearly shows that there is distinct Python code and Mojo code, and Lattner wants to encourage you to port your Python code to Mojo. It may look similar, but it's not quite the same. Mojo is a new language.\n",
      "Davis Bennett said: yeah but the strategy is essentially to wrap python, and add new features\n",
      "Mark Kittisopikul said: At some point, you're not going to be able to take Mojo code and run it with CPython.\n",
      "Stuart Berg said: So can't Julia do more-or-less the same thing, then?  Wrap Python and make it available in Julia?  Or am I thinking of the opposite (wrap Julia from Python)?\n",
      "Mark Kittisopikul said: Yes, both options are available.\n",
      "Davis Bennett said: i think the problem for julia is that it made no effort to win over python devs\n",
      "Mark Kittisopikul said: The syntax is quite distinct, and then there is the indexing issue.\n",
      "Davis Bennett said: the strategy for most other accelerators is to provide something familiar to python developers, but with more performance\n",
      "Davis Bennett said: i suspect if julia was being made today, they would do the same thing, since deep learning has exploded and it's squarely a python show\n",
      "Stuart Berg said: But I'll admit that Julia's approach for writing vectorized functions is WAY more elegant than Python's.  However, it relies on new syntax.  Not sure how you'd shoehorn Python code into doing that.\n",
      "Mark Kittisopikul said: Perhaps. Julia has been in communication with Lattner since it's inception in 2012/2013. Many Julia core devs have commit privileges on LLVM. Lattner is very much building on a lot of work that the LLVM community has collectively built.\n",
      "Davis Bennett said: but i think it's safe to say that, in hindsight, stuff like indexing with 1 was probably a mistake, in terms of creating an artificial barrier to adoption\n",
      "Mark Kittisopikul said: For example, see the exchange between Keno and Lattner on HN: https://news.ycombinator.com/item?id=35791125\n",
      "Mark Kittisopikul said: The initial target audience was scientific computing in 2012. For the most part in 2012, a large part of scientific computing was still MATLAB and Fortran.\n",
      "Davis Bennett said: yes, but if you looked at the growth rates of matlab and fortran in 2012\n",
      "Davis Bennett said: vs python\n",
      "Davis Bennett said: it was clear that python should be the target\n",
      "Davis Bennett said: i'm not faulting julia for singing its own song, it's great that there are lots of languages. but if their goal was to be a big player in data science, they probably should have made the on-ramp as simple as possible for python devs\n",
      "Mark Kittisopikul said: We're going off on a tangent here. You can see Karpinski's response here:\n",
      "https://discourse.julialang.org/t/julia-motivation-why-werent-numpy-scipy-numba-good-enough/2236/10\n",
      "Julia motivation: why weren't Numpy, Scipy, Numba, good enough?\n",
      "I guess I can give some historical motivation here. When we started working on Julia in 2009, the Python ecosystem was nowhere near where it is today: PyPy was pretty new: 1.0 was released in 2007 – it was the first version to be able to run any substantial Python apps and there were no plans at the time for supporting any of NumPy; Cython was even newer: initial release was 2007; Numba didn’t exist: initial release was in 2012. Even NumPy itself was a bit rough back then. It’s entirely possi...\n",
      "Davis Bennett said: that post isn't a response to the point i'm making\n",
      "Mark Kittisopikul said: How so? He clearly points out they felt they needed a new language to fit their needs while also pointing out they didn't feel like Python core was making numerical computing a priority. They wanted a language built around a new type system in 2009. PEP 483 for type annotations didn't come until 2014.\n",
      "Mark Kittisopikul said: What I really want to know is what is Modular's business model? I'm not completely sure Mojo is actually the product, especially if they do plan to open source it.\n",
      "Mark Kittisopikul said: My suspicion is that that Modular's business model is compilation as a service. That's perhaps not too different than what the Azul folks talked to us about.\n",
      "Mark Kittisopikul said: Julia aim was never really to replace Python out right. MATLAB was more clearly the target.\n",
      "In this case, Mojo seems more clearly intended to replace Python. They event use the word \"replacement\" https://docs.modular.com/mojo/why-mojo.html#how-compatible-is-mojo-with-python-really.\n",
      "It is hard to make a direct comparison, but the complexity of the Clang problem appears to be an order of magnitude bigger than implementing a compatible replacement for Python.\n",
      "Modular Docs - Why Mojo:fire:\n",
      "A backstory and rationale for why we created the Mojo language.\n",
      "Mark Kittisopikul said: We also think the relationship with CPython can build from both directions - wouldn’t it be cool if the CPython team eventually reimplemented the interpreter in Mojo instead of C?Does anyone actually think that's going to happen? I think someone might do it, but I doubt it will be the CPython developers.\n",
      "Mark Kittisopikul said: Has anyone seen a reaction from Guido van Rossum?\n",
      "Stuart Berg said: Does anyone actually think that's going to happen?\n",
      "I agree with your sentiment.  It sounds ludicrous.  On the other hand, I have to admit I never would have predicted that a project like Nuitka could exist.  Maybe I need to recalibrate my sense of what's possible.  There's also a project out there that implements Python in Rust, albeit without ABI compatibility with CPython.  If you are willing to break backwards compatibility, then that makes the problem a lot simpler -- but then what's the point?\n",
      "Davis Bennett said:  How so? He clearly points out they felt they needed a new language to fit their needs while also pointing out they didn't feel like Python core was making numerical computing a priority. They wanted a language built around a new type system in 2009. PEP 483 for type annotations didn't come until 2014.\n",
      "My point is that stealing mindshare from python would have been much much easier if they made a language that looked like python. Instead, they did their own thing, which is totally fine, but hurt the adoption of the language\n",
      "Davis Bennett said: this is an empirical observation, based on seeing many many attempts to make python faster for numerical computing, which all use syntax that looks like python, and relatively few attempts to make new numerical computing languages from scratch\n",
      "Davis Bennett said:  On the other hand, I have to admit I never would have predicted that a project like Nuitka could exist. Maybe I need to recalibrate my sense of what's possible.lots of things become possible when someone pours money on a problem -- just look at how fast javascript got thanks to v8.\n",
      "Stuart Berg said: To my knowledge, Nuitka is a labor of love from a single developer.\n",
      "Davis Bennett said: right, i don't mean to say that nuitka is a big corporate endeavor\n",
      "Davis Bennett said: i was thinking more of the situation machine learning is in, where a lot of companies are willing to spend huge sums on it\n",
      "Davis Bennett said: and in that case, they might be able to achieve massive engineering efforts, like porting cpython to a different backend\n",
      "Donald Olbris said: And, if your goal is to be a proper superset of Python, to keep porting it with every new release.\n",
      "Stuart Berg said: Yeah.  For that reason, I do think it will be a lot easier for them if they can get the community involved.  At a minimum, there may come a day when they need to ask the CPython folks to add a few special features that will make Mojo's life simpler.  For example, PyTorch now benefits from some new feature in Python called \"Frame Evaluation Hooks\", which was added specifically to allow JITs to work with Python.\n",
      "I'm not saying Mojo is a JIT, but rather that's an example of how CPython can \"cooperate\" with outsiders...  if they're friendly!\n",
      "William Katz said: What I really want to know is what is Modular's business model? I'm not completely sure Mojo is actually the product, especially if they do plan to open source it.Their business plan is likely to make money off the Mojo ecosystem, particularly cloud-based services and maybe serving as the \"AWS for AI\". Companies like kaggle could outsource some costly parts of their infrastructure to Modular. They are closed for now except getting people to work on their cloud using notebooks, which is good way to test their cloud service. It's quite possible a focus will be on selling their \"Modular Inference Engine\" that will go on IoT, phones, etc.\n",
      "William Katz said: The 3rd person shown in their Team web page (behind the two Co-Founders) is the Cloud Infrastructure Lead who helped build out Microsoft services for corporations.\n",
      "William Katz said: They've got a good sales pitch already for their inference engine since it can work with TensorFlow and PyTorch models: https://docs.modular.com/engine/\n",
      "Modular Docs - Modular Inference Engine\n",
      "The world's fastest unified inference engine, supercharging any model from TensorFlow or PyTorch on a wide range of hardware.\n",
      "William Katz said: So if you can run AI more efficiently than competitors, you can make money from companies either licensing your Inference Engine or simply integrating your cloud service into theirs. Getting people to write Mojo increases the size of the moat if you assume your team is better at optimizing performance of language you wrote, even if it's open source.\n",
      "Mark Kittisopikul said: The multiple attempts of trying to do the same thing, accelerating Python, suggests to me that approach may not be very successful. There is some limitation. If it were a successful strategy, I would have expected one of them to have achieve sufficient dominance by now that it would exclude the other approaches. Instead, we have yet another attempt with Mojo.\n",
      "Davis Bennett said: yeah, I would have posed this question the the mojo people -- \"why will your thing succeed, and not be pypy 2.0\"\n",
      "Mark Kittisopikul said: If indexing were a very important factor, then perhaps something like https://chapel-lang.org/ might get more attention.\n",
      "Mark Kittisopikul said: Well the reason why Mojo might succeed is they seem much more willing to break things than Pypy.\n",
      "Mark Kittisopikul said: I see no intention to make cpython better. Rather they are just making it easier to switch to Mojo from cpython.\n",
      "William Katz said: From what I can tell, they can accelerate the running of TensorFlow and PyTorch models without any change to those models themselves. Then they can say, hey AI dev, if you do these additional things we can reward you with much better performance.\n",
      "Davis Bennett said: if it works, that's super compelling, and worth money\n",
      "Mark Kittisopikul said: Oh yea, they are wholesale copying the API, but not actually enhancing TF or PyTorch though.\n",
      "Mark Kittisopikul said: It's like what M$ try to do to Java and what Google did with Android.\n",
      "William Katz said: I think from your posts I'm looking at this very differently. I'm not personally attached to the python language per se. I do have a stake that the work people did before can be used going forward (backward compatibility) and the intellectual capital people got from studying python can still be used.\n",
      "Davis Bennett said: worth noting that performance in numerical applications / ML is simply not a central focus of the core cpython devs, and that's fine\n",
      "Davis Bennett said: so I think it's reasonable to work around cpython for this kind of thing\n",
      "William Katz said: Mojo is also selling that advantage in a very focused way.  They literally put at the top of their web page they are \"The future of AI development\". They don't say \"We've made multithreading and GPU acceleration simple for python.\"  That seems like smart positioning and puts focus on really what they're trying to sell.\n",
      "Davis Bennett said: e.g., adding a new keyword `fn` for defining functions that are more performant would be a non-starter as a patch to cpython\n",
      "Davis Bennett said: but it makes sense in mojo\n",
      "Mark Kittisopikul said: I thought they would go with `func` but that would have made it too obvious that this is Swift in disguise.\n",
      "Mark Kittisopikul said: \"no global variables\" is probably going to be a pain point.\n",
      "https://docs.modular.com/mojo/roadmap.html#no-global-variables\n",
      "It's interesting that they do not say why, although from I think the Julia experience with globals is probably quite informative.\n",
      "Modular Docs - Mojo:fire: roadmap &amp; sharp edges\n",
      "A summary of our Mojo plans, including upcoming features and things we need to fix.\n",
      "Mark Kittisopikul said: I think from your posts I'm looking at this very differently. I'm not personally attached to the python language per se. I do have a stake that the work people did before can be used going forward (backward compatibility) and the intellectual capital people got from studying python can still be used.I'm not sure how you think I'm looking at this. This looks like a very deliberate and strategic attempt to move people away from Python to more efficient programming practices. I'm quite in favor of this. The end result of this will probably look quite distinct from Python, however. In fact, that sharp edges list already very clearly does reveal significant, breaking differences. It has different scoping, semantics, and syntax. The marketing and the CPython embedding is really a Trojan Horse. I'm in favor of the Greeks here.\n",
      "Others reacted to the previous message with laughing a total of 1 times.\n",
      "Adam Taylor said: I don't think it was so clear in the early teens that Python's trend line was different from Matlab's:\n",
      "https://insights.stackoverflow.com/trends/?tags=python%2Cmatlab\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "Davis Bennett said: that's a good point, it actually looks like matlab peaked at 2014\n",
      "Davis Bennett said: https://insights.stackoverflow.com/trends/?tags=matlab%2Cnumpy\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "Adam Taylor said: Also, there's always going to be a tension between innovation and familiarity in language design, and finding the sweet spot there is tough.  And also there is always going to be a certain amount of randomness in what languages take off and which don't...\n",
      "Others reacted to the previous message with +1::skin-tone-2 a total of 1 times.\n",
      "Adam Taylor said: So did numpy peak in 2021?  ;)\n",
      "Mark Kittisopikul said: When I was working on this paper in 2009 / 2010, I picked R after looking into various open source efforts at the time:\n",
      "https://www.pnas.org/doi/10.1073/pnas.1003975107\n",
      "Mark Kittisopikul said: How do we add R to that plot?\n",
      "Mark Kittisopikul said: https://insights.stackoverflow.com/trends/?tags=matlab%2Cnumpy%2Cr\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "Adam Taylor said: I, for one, welcome our rlang overlords:\n",
      "https://insights.stackoverflow.com/trends/?tags=matlab%2Cnumpy%2Cr\n",
      "Stack Overflow\n",
      "Use Stack Overflow Insights and get information required to understand, reach, and attract developers.Improve tech hiring, recruiting, developer marketing, and and planning initiatives.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Davis Bennett said: that growth is wild! i had no idea R was so big\n",
      "Stuart Berg said: I am struggling to make sense of that plot. This is the part where someone claims that stackoverflow trends are biased towards things people have trouble with, so confusing or just plain nonsensical languages will be over-represented.\n",
      "Since I cannot accept a reality in which R overtakes Python, I am invoking that claim at this juncture. \n",
      "Mark Kittisopikul said: R is now incumbent in the pharmaceutical industry.\n",
      "Mark Kittisopikul said: They have a whole conference just for that.\n",
      "https://rinpharma.com/\n",
      "R/Pharma\n",
      "R/Pharma. The conference for all things R related in the Pharma industry\n",
      "Mark Kittisopikul said: https://www.bioconductor.org/ is also big in bioinformatics\n",
      "Adam Taylor said: That's right a**holes: You wanted to get rid of Matlab sooo bad, and this is what you get.  You have only yourselves to blame.  Have fun toiling in the R pits, jerks.  How about a little one-based indexing with your choice of two (or maybe three?) roughly-equally-popular OOP frameworks?  Oh you thought that was a field selector, or maybe a message send?  Nope!  That period is just part of the identifier!  Have fun!\n",
      "Stuart Berg said:  \n",
      "Mark Kittisopikul said: Who wants to guess how one gets the first element of a vector in R?\n",
      "Adam Taylor said: `car()`?\n",
      "Others reacted to the previous message with rolling_on_the_floor_laughing a total of 1 times, and with lips a total of 1 times.\n",
      "Mark Kittisopikul said: No, that's a Julia easter egg.\n",
      "$ julia --lisp\n",
      ";  _\n",
      "; |_ _ _ |_ _ |  . _ _\n",
      "; | (-||||_(_)|__|_)|_)\n",
      ";-------------------|----------------------------------------------------------\n",
      "> (car (list 4 5 6))\n",
      "4\n",
      "> (cdr (list 4 5 6))\n",
      "(5 6)\n",
      "\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Mark Kittisopikul said: There are no zeros involved.\n",
      "> v = c(4,5,6)\n",
      "> v[1]\n",
      "[1] 4\n",
      "> v[2]\n",
      "[1] 5\n",
      "> v[3]\n",
      "[1] 6\n",
      "> v[1:2]\n",
      "[1] 4 5\n",
      "\n",
      "\n",
      "1683231475.746299\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683231475.746299]\n",
      "Mark Kittisopikul said: How do I tell matplotlib where to find latex on macOS:\n",
      "RuntimeError: Failed to process string with tex because latex could not be found\n",
      "\n",
      "Davis Bennett said: https://matplotlib.org/stable/tutorials/text/usetex.html#text-rendering-with-latex\n",
      "Davis Bennett said: is all the `tex` stuff in `PATH`?\n",
      "Mark Kittisopikul said: FileNotFoundError                         Traceback (most recent call last)\n",
      "File ~/miniforge3-arm64/envs/fimhuyvo2022/lib/python3.9/site-packages/matplotlib/texmanager.py:233, in TexManager._run_checked_subprocess(self, command, tex, cwd)\n",
      "    232 try:\n",
      "--> 233     report = subprocess.check_output(\n",
      "    234         command, cwd=cwd if cwd is not None else self.texcache,\n",
      "    235         stderr=subprocess.STDOUT)\n",
      "    236 except FileNotFoundError as exc:\n",
      "File ~/miniforge3-arm64/envs/fimhuyvo2022/lib/python3.9/subprocess.py:424, in check_output(timeout, *popenargs, **kwargs)\n",
      "    422     kwargs['input'] = empty\n",
      "--> 424 return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "    425            **kwargs).stdout\n",
      "...\n",
      "File ~/miniforge3-arm64/envs/fimhuyvo2022/lib/python3.9/site-packages/matplotlib/texmanager.py:237, in TexManager._run_checked_subprocess(self, command, tex, cwd)\n",
      "    233     report = subprocess.check_output(\n",
      "    234         command, cwd=cwd if cwd is not None else self.texcache,\n",
      "    235         stderr=subprocess.STDOUT)\n",
      "    236 except FileNotFoundError as exc:\n",
      "--> 237     raise RuntimeError(\n",
      "    238         'Failed to process string with tex because {} could not be '\n",
      "    239         'found'.format(command[0])) from exc\n",
      "    240 except subprocess.CalledProcessError as exc:\n",
      "    241     raise RuntimeError(\n",
      "    242         '{prog} was not able to process the following string:\\n'\n",
      "    243         '{tex!r}\\n\\n'\n",
      "   (...)\n",
      "    247             tex=tex.encode('unicode_escape'),\n",
      "    248             exc=exc.output.decode('utf-8'))) from exc\n",
      "RuntimeError: Failed to process string with tex because latex could not be found\n",
      "<Figure size 432x432 with 4 Axes>\n",
      "\n",
      "Mark Kittisopikul said: Hmm, let me try rebooting this Jupyter server\n",
      "Mark Kittisopikul said: ok that helped\n",
      "Mark Kittisopikul said: Is it just me or is very difficult to install latex via conda? I just ended up installing macTeX directly.\n",
      "Davis Bennett said: my experience with the whole latex toolchain was not positive\n",
      "Davis Bennett said: it's hard to install, bloated, generates loads of intermediate files, etc\n",
      "Davis Bennett said: but using it seems to be a kind of hazing ritual\n",
      "Davis Bennett said: (less flippantly, i think typesetting is hard, and tex does a good enough job, despite all the warts)\n",
      "John Bogovic said: I 100% have https://comp-theory.slack.com/archives/C0132TRD06N/p1675198896727129?thread_ts=1675190490.754119&cid=C0132TRD06N, but overleaf hides a lot of the toolchain pain from you, and has been great in my experience so far.\n",
      "John Bogovic said: (but thats unrelated to matplotlib...)\n",
      "Davis Bennett said: regarding matplotlib, i thought they had some kind of tex-like functionality without relying on a system installation of latex\n",
      "Davis Bennett said: https://matplotlib.org/stable/tutorials/text/mathtext.html#writing-mathematical-expressions\n",
      "Others reacted to the previous message with eyes a total of 1 times.\n",
      "\n",
      "1683232894.081369\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683232894.081369]\n",
      "Mark Kittisopikul said: I'm having a very hard time solving this environment on Linux for a review:\n",
      "https://github.com/voduchuy/NoisyMeasurementFIM/blob/v0.0.2-biorxiv/environment.yml\n",
      "I get messages such as\n",
      "Encountered problems while solving:\n",
      "  - package fonttools-4.29.1-py37h6b43d8f_0 requires python_abi 3.7 *_pypy37_pp73, but none of the providers can be installed\n",
      "\n",
      "<https://github.com/voduchuy/NoisyMeasurementFIM/blob/v0.0.2-biorxiv/environment.yml | environment.yml>\n",
      "```\n",
      "name: fimhuyvo2022\n",
      "channels:\n",
      "  - conda-forge\n",
      "dependencies:\n",
      "  - aiohttp=3.8.1\n",
      "  - aiohttp-cors=0.7.0\n",
      "  - aiosignal=1.2.0\n",
      "  - appnope=0.1.2\n",
      "  - argon2-cffi=21.3.0\n",
      "  - argon2-cffi-bindings=21.2.0\n",
      "  - asttokens=2.0.5\n",
      "  - async-timeout=4.0.2\n",
      "  - attrs=21.4.0\n",
      "  - backcall=0.2.0\n",
      "  - backports=1.0\n",
      "  - backports.functools_lru_cache=1.6.4\n",
      "  - black=22.1.0\n",
      "  - blackd=22.1.0\n",
      "  - bleach=4.1.0\n",
      "  - boost-cpp=1.74.0\n",
      "  - brotli=1.0.9\n",
      "  - brotli-bin=1.0.9\n",
      "  - bzip2=1.0.8\n",
      "  - ca-certificates=2022.12.7\n",
      "  - certifi=2022.12.7\n",
      "  - cffi=1.15.0\n",
      "  - charset-normalizer=2.0.11\n",
      "  - click=8.0.3\n",
      "  - colorama=0.4.4\n",
      "  - cycler=0.11.0\n",
      "  - cython=0.29.23\n",
      "  - dataclasses=0.8\n",
      "  - debugpy=1.5.1\n",
      "  - defusedxml=0.7.1\n",
      "  - entrypoints=0.4\n",
      "  - executing=0.8.2\n",
      "  - flit-core=3.6.0\n",
      "  - fonttools=4.29.1\n",
      "  - freetype=2.10.4\n",
      "  - fribidi=1.0.10\n",
      "  - frozenlist=1.3.0\n",
      "  - giflib=5.2.1\n",
      "  - gmp=6.2.1\n",
      "  - gmpy2=2.1.2\n",
      "  - icu=69.1\n",
      "  - importlib-metadata=4.10.1\n",
      "  - importlib_resources=5.4.0\n",
      "  - ipykernel=6.8.0\n",
      "  - ipyparallel=8.1.0\n",
      "  - ipython=8.0.1\n",
      "  - ipython_genutils=0.2.0\n",
      "  - ipywidgets=7.6.5\n",
      "  - jbig=2.1\n",
      "  - jedi=0.18.1\n",
      "  - jinja2=3.0.3\n",
      "  - jpeg=9e\n",
      "  - jsonschema=4.4.0\n",
      "  - jupyter=1.0.0\n",
      "  - jupyter_client=7.1.2\n",
      "  - jupyter_console=6.4.0\n",
      "  - jupyter_core=4.9.1\n",
      "  - jupyterlab_pygments=0.1.2\n",
      "  - jupyterlab_widgets=1.0.2\n",
      "  - kiwisolver=1.3.2\n",
      "  - lcms2=2.12\n",
      "  - lerc=3.0\n",
      "  - libblas=3.9.0\n",
      "  - libbrotlicommon=1.0.9\n",
      "  - libbrotlidec=1.0.9\n",
      "  - libbrotlienc=1.0.9\n",
      "  - libcblas=3.9.0\n",
      "  - libcxx=12.0.1\n",
      "  - libdeflate=1.8\n",
      "  - libffi=3.4.2\n",
      "  - libgfortran=5.0.0.dev0\n",
      "  - libgfortran5=11.0.1.dev0\n",
      "  - liblapack=3.9.0\n",
      "  - libopenblas=0.3.18\n",
      "  - libpng=1.6.37\n",
      "  - libsodium=1.0.18\n",
      "  - libtiff=4.3.0\n",
      "  - libwebp=1.2.2\n",
      "  - libwebp-base=1.2.2\n",
      "  - libxcb=1.13\n",
      "  - libzlib=1.2.11\n",
      "  - llvm-openmp=12.0.1\n",
      "  - lz4-c=1.9.3\n",
      "  - markupsafe=2.0.1\n",
      "  - matplotlib=3.5.1\n",
      "  - matplotlib-base=3.5.1\n",
      "  - matplotlib-inline=0.1.3\n",
      "  - mistune=0.8.4\n",
      "  - mpc=1.2.1\n",
      "  - mpfr=4.1.0\n",
      "  - mpmath=1.2.1\n",
      "  - multidict=6.0.2\n",
      "  - munkres=1.1.4\n",
      "  - mypy_extensions=0.4.3\n",
      "  - nbclient=0.5.10\n",
      "  - nbconvert=6.4.1\n",
      "  - nbformat=5.1.3\n",
      "  - ncurses=6.3\n",
      "  - nest-asyncio=1.5.4\n",
      "  - networkx=2.6.3\n",
      "  - notebook=6.4.8\n",
      "  - numpy=1.22.3\n",
      "  - openjpeg=2.4.0\n",
      "  - openssl=3.1.0\n",
      "  - packaging=21.3\n",
      "  - pagmo=2.18.0\n",
      "  - pandas=1.4.1\n",
      "  - pandocfilters=1.5.0\n",
      "  - parso=0.8.3\n",
      "  - pathspec=0.9.0\n",
      "  - patsy=0.5.2\n",
      "  - pexpect=4.8.0\n",
      "  - pickleshare=0.7.5\n",
      "  - pillow=9.0.0\n",
      "  - pip=22.0.3\n",
      "  - platformdirs=2.3.0\n",
      "  - prometheus_client=0.13.1\n",
      "  - prompt-toolkit=3.0.26\n",
      "  - prompt_toolkit=3.0.26\n",
      "  - psutil=5.9.0\n",
      "  - pthread-stubs=0.4\n",
      "  - ptyprocess=0.7.0\n",
      "  - pure_eval=0.2.2\n",
      "  - pybind11-abi=4\n",
      "  - pycparser=2.21\n",
      "  - pygments=2.11.2\n",
      "  - pygmo=2.18.0\n",
      "  - pyparsing=3.0.7\n",
      "  - pyrsistent=0.18.1\n",
      "  - python=3.9.10\n",
      "  - python-dateutil=2.8.2\n",
      "  - python_abi=3.9\n",
      "  - pytz=2021.3\n",
      "  - pyzmq=22.3.0\n",
      "  - readline=8.1\n",
      "  - scipy=1.7.3\n",
      "  - seaborn=0.11.2\n",
      "  - seaborn-base=0.11.2\n",
      "  - send2trash=1.8.0\n",
      "  - setuptools=60.7.1\n",
      "  - six=1.16.0\n",
      "  - sqlite=3.37.0\n",
      "  - stack_data=0.1.4\n",
      "  - statsmodels=0.13.1\n",
      "  - sympy=1.9\n",
      "  - tbb=2021.5.0\n",
      "  - terminado=0.13.1\n",
      "  - testpath=0.5.0\n",
      "  - tk=8.6.11\n",
      "  - tomli=2.0.0\n",
      "  - tornado=6.1\n",
      "  - tqdm=4.62.3\n",
      "  - traitlets=5.1.1\n",
      "  - typed-ast=1.5.2\n",
      "  - typing-extensions=4.0.1\n",
      "  - typing_extensions=4.0.1\n",
      "  - tzdata=2021e\n",
      "  - unicodedata2=14.0.0\n",
      "  - wcwidth=0.2.5\n",
      "  - webencodings=0.5.1\n",
      "  - wheel=0.37.1\n",
      "  - widgetsnbextension=3.5.2\n",
      "  - xorg-libxau=1.0.9\n",
      "  - xorg-libxdmcp=1.1.3\n",
      "  - xz=5.2.5\n",
      "  - yarl=1.7.2\n",
      "  - zeromq=4.3.4\n",
      "  - zipp=3.7.0\n",
      "  - zlib=1.2.11\n",
      "  - zstd=1.5.2\n",
      "  - pip:\n",
      "    - cachetools==4.2.2\n",
      "    - chardet==4.0.0\n",
      "    - cloudpickle==1.6.0\n",
      "    - decorator==5.0.9\n",
      "    - dm-tree==0.1.6\n",
      "    - docopt==0.6.2\n",
      "    - flatbuffers==1.12\n",
      "    - google-auth==1.32.0\n",
      "    - google-auth-oauthlib==0.4.4\n",
      "    - idna==2.10\n",
      "    - markdown==3.3.4\n",
      "    - mpi4py==3.1.4\n",
      "    - oauthlib==3.1.1\n",
      "    - pipreqs==0.4.11\n",
      "    - plotly==5.0.0\n",
      "    - pyasn1==0.4.8\n",
      "    - pyasn1-modules==0.2.8\n",
      "    - pystan==2.19.1.1\n",
      "    - requests==2.25.1\n",
      "    - requests-oauthlib==1.3.0\n",
      "    - rsa==4.7.2\n",
      "    - tenacity==7.0.0\n",
      "    - urllib3==1.26.6\n",
      "    - werkzeug==2.0.1\n",
      "    - wget==3.2\n",
      "    - yarg==0.1.9\n",
      "prefix: /Users/huyvo/miniforge3/envs/fimhuyvo2022\n",
      "```\n",
      "Mark Kittisopikul said: I can solve it on macOS\n",
      "Stuart Berg said: Yeah, looks like that environment was originally constructed on macOS.  I guess the linux setup isn't the same.\n",
      "I know it's ugly and annoying, but...  Have you tried just stripping the minor version patch version from every requirement and hoping for the best?\n",
      "Stuart Berg said: Replace this:\n",
      "dependencies:\n",
      "  - aiohttp=3.8.1\n",
      "  - aiohttp-cors=0.7.0\n",
      "with this:\n",
      "dependencies:\n",
      "  - aiohttp=3.8\n",
      "  - aiohttp-cors=0.7\n",
      "\n",
      "Stuart Berg said: and see what happens?\n",
      "Mark Kittisopikul said: I made them strip the build numbers versus master, but apparently that only partially helped\n",
      "Mark Kittisopikul said: where is that pypy requirement coming from\n",
      "Stuart Berg said: That is confusing me as well...  As far as I can tell, there is no pypy version of `fonttools-4.29.1` on conda-forge.\n",
      "https://anaconda.org/conda-forge/fonttools/files?version=4.29.1\n",
      "Which makes me think we're interpreting that error message incorrectly.\n",
      "Stuart Berg said: Sorry, I gotta run.\n",
      "Mark Kittisopikul said: Thanks. Well I deleted a few versions. mamba seems to be working on something now\n",
      "Stuart Berg said: ¯\\_(ツ)_/¯\n",
      "Mark Kittisopikul said:  git diff\n",
      "diff --git a/environment.yml b/environment.yml\n",
      "index b15ad3f..0c14e16 100644\n",
      "--- a/environment.yml\n",
      "+++ b/environment.yml\n",
      "@@ -2,10 +2,10 @@ name: fimhuyvo2022\n",
      " channels:\n",
      "   - conda-forge\n",
      " dependencies:\n",
      "-  - aiohttp=3.8.1\n",
      "-  - aiohttp-cors=0.7.0\n",
      "+  - aiohttp\n",
      "+  - aiohttp-cors\n",
      "   - aiosignal=1.2.0\n",
      "-  - appnope=0.1.2\n",
      "+  - appnope\n",
      "   - argon2-cffi=21.3.0\n",
      "   - argon2-cffi-bindings=21.2.0\n",
      "   - asttokens=2.0.5\n",
      "@@ -23,19 +23,19 @@ dependencies:\n",
      "   - bzip2=1.0.8\n",
      "   - ca-certificates=2022.12.7\n",
      "   - certifi=2022.12.7\n",
      "-  - cffi=1.15.0\n",
      "+  - cffi\n",
      "   - charset-normalizer=2.0.11\n",
      "-  - click=8.0.3\n",
      "+  - click\n",
      "   - colorama=0.4.4\n",
      "   - cycler=0.11.0\n",
      "   - cython=0.29.23\n",
      "   - dataclasses=0.8\n",
      "-  - debugpy=1.5.1\n",
      "+  - debugpy\n",
      "   - defusedxml=0.7.1\n",
      "   - entrypoints=0.4\n",
      "   - executing=0.8.2\n",
      "   - flit-core=3.6.0\n",
      "-  - fonttools=4.29.1\n",
      "+  - fonttools\n",
      "   - freetype=2.10.4\n",
      "   - fribidi=1.0.10\n",
      "   - frozenlist=1.3.0\n",
      "@@ -47,7 +47,7 @@ dependencies:\n",
      "   - importlib_resources=5.4.0\n",
      "   - ipykernel=6.8.0\n",
      "   - ipyparallel=8.1.0\n",
      "-  - ipython=8.0.1\n",
      "+  - ipython\n",
      "   - ipython_genutils=0.2.0\n",
      "   - ipywidgets=7.6.5\n",
      "   - jbig=2.1\n",
      "@@ -72,8 +72,8 @@ dependencies:\n",
      "   - libcxx=12.0.1\n",
      "   - libdeflate=1.8\n",
      "   - libffi=3.4.2\n",
      "-  - libgfortran=5.0.0.dev0\n",
      "-  - libgfortran5=11.0.1.dev0\n",
      "+  - libgfortran\n",
      "+  - libgfortran5\n",
      "   - liblapack=3.9.0\n",
      "   - libopenblas=0.3.18\n",
      "   - libpng=1.6.37\n",
      "@@ -86,9 +86,9 @@ dependencies:\n",
      "   - llvm-openmp=12.0.1\n",
      "   - lz4-c=1.9.3\n",
      "   - markupsafe=2.0.1\n",
      "-  - matplotlib=3.5.1\n",
      "-  - matplotlib-base=3.5.1\n",
      "-  - matplotlib-inline=0.1.3\n",
      "+  - matplotlib\n",
      "+  - matplotlib-base\n",
      "+  - matplotlib-inline\n",
      "   - mistune=0.8.4\n",
      "   - mpc=1.2.1\n",
      "   - mpfr=4.1.0\n",
      "@@ -131,9 +131,8 @@ dependencies:\n",
      "   - pygmo=2.18.0\n",
      "   - pyparsing=3.0.7\n",
      "   - pyrsistent=0.18.1\n",
      "-  - python=3.9.10\n",
      "+  - python=3.9\n",
      "   - python-dateutil=2.8.2\n",
      "-  - python_abi=3.9\n",
      "   - pytz=2021.3\n",
      "   - pyzmq=22.3.0\n",
      "   - readline=8.1\n",
      "\n",
      "Mark Kittisopikul said: Aha, I think I figured it out. I was inadvertently using a mambaforge-pypy install\n",
      "Others reacted to the previous message with facepalm a total of 1 times, and with horse a total of 1 times.\n",
      "Mark Kittisopikul said: That doesn't completely resolve the issue though.\n",
      "Mark Kittisopikul said: The following packages are incompatible\n",
      "├─ appnope 0.1.2**  does not exist (perhaps a typo or a missing channel);\n",
      "├─ icu 69.1**  is requested and can be installed;\n",
      "├─ libffi 3.4.2**  is requested and can be installed;\n",
      "├─ libgfortran 5.0.0.dev0**  does not exist (perhaps a typo or a missing channel);\n",
      "├─ libgfortran5 11.0.1.dev0**  does not exist (perhaps a typo or a missing channel);\n",
      "├─ matplotlib 3.5.1**  is installable with the potential options\n",
      "│  ├─ matplotlib 3.5.1 would require\n",
      "│  │  └─ pyqt   with the potential options\n",
      "│  │     ├─ pyqt 4.11.4 would require\n",
      "│  │     │  └─ qt [4.8.* |>=4.8.6,<5.0 ], which requires\n",
      "│  │     │     └─ icu 58.* , which conflicts with any installable versions previously reported;\n",
      "│  │     ├─ pyqt 5.12.3 would require\n",
      "│  │     │  ├─ pyqt-impl [5.12.3 py310h1f8e252_8|5.12.3 py37hac37412_8|...|5.12.3 py39hde8b62d_8], which requires\n",
      "│  │     │  │  └─ qt >=5.12.9,<5.13.0a0  with the potential options\n",
      "│  │     │  │     ├─ qt [5.12.5|5.12.6|5.12.9] would require\n",
      "│  │     │  │     │  └─ icu >=67.1,<68.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │  │     ├─ qt 5.12.9 would require\n",
      "│  │     │  │     │  └─ sqlite >=3.37.1,<4.0a0 , which can be installed;\n",
      "│  │     │  │     ├─ qt 5.12.9 would require\n",
      "│  │     │  │     │  └─ icu >=68.1,<69.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │  │     └─ qt 5.12.9 would require\n",
      "│  │     │  │        └─ openssl <3 , which can be installed;\n",
      "│  │     │  └─ qt >=5.12.9,<5.13.0a0  with the potential options\n",
      "│  │     │     ├─ qt [5.12.5|5.12.6|5.12.9], which cannot be installed (as previously explained);\n",
      "│  │     │     ├─ qt 5.12.9, which can be installed (as previously explained);\n",
      "│  │     │     ├─ qt 5.12.9, which cannot be installed (as previously explained);\n",
      "│  │     │     └─ qt 5.12.9, which can be installed (as previously explained);\n",
      "│  │     ├─ pyqt 5.12.3 would require\n",
      "│  │     │  └─ python >=2.7,<2.8.0a0  with the potential options\n",
      "│  │     │     ├─ python [2.7.12|2.7.13|...|3.6.3] would require\n",
      "│  │     │     │  └─ sqlite 3.13.* , which can be installed;\n",
      "│  │     │     ├─ python [2.7.14|2.7.15|3.6.3|3.6.4|3.6.5] would require\n",
      "│  │     │     │  └─ sqlite 3.20.* , which can be installed;\n",
      "│  │     │     ├─ python [2.7.15|3.6.10|...|3.6.9] would require\n",
      "│  │     │     │  ├─ libffi >=3.2.1,<3.3.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     │  └─ pypy3.6 [7.3.0.* |7.3.1.* |7.3.2.* ], which requires\n",
      "│  │     │     │     └─ libffi >=3.2.1,<3.3.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     └─ python 2.7.15 would require\n",
      "│  │     │        └─ openssl >=1.0.2o,<1.0.3a , which can be installed;\n",
      "│  │     ├─ pyqt 5.12.3 would require\n",
      "│  │     │  └─ python >=3.6,<3.7.0a0  with the potential options\n",
      "│  │     │     ├─ python [2.7.12|2.7.13|...|3.6.3], which can be installed (as previously explained);\n",
      "│  │     │     ├─ python [2.7.14|2.7.15|3.6.3|3.6.4|3.6.5], which can be installed (as previously explained);\n",
      "│  │     │     ├─ python [2.7.15|3.6.10|...|3.6.9], which cannot be installed (as previously explained);\n",
      "│  │     │     ├─ python [3.6.11|3.6.12|3.6.13] would require\n",
      "│  │     │     │  ├─ libffi >=3.3,<3.4.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     │  └─ pypy3.6 7.3.3.* , which requires\n",
      "│  │     │     │     └─ libffi >=3.3,<3.4.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     └─ python [3.6.13|3.6.15|3.9.10] would require\n",
      "│  │     │        └─ openssl >=1.1.1l,<1.1.2a , which can be installed;\n",
      "│  │     ├─ pyqt 5.12.3 would require\n",
      "│  │     │  └─ qt >=5.12.5,<5.13.0a0  with the potential options\n",
      "│  │     │     ├─ qt [5.12.5|5.9.7] would require\n",
      "│  │     │     │  └─ icu >=64.2,<65.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     ├─ qt [5.12.5|5.12.6|5.12.9], which cannot be installed (as previously explained);\n",
      "│  │     │     ├─ qt 5.12.9, which can be installed (as previously explained);\n",
      "│  │     │     ├─ qt 5.12.9, which cannot be installed (as previously explained);\n",
      "│  │     │     └─ qt 5.12.9, which can be installed (as previously explained);\n",
      "│  │     ├─ pyqt [5.15.4|5.15.7] would require\n",
      "│  │     │  └─ qt-main [>=5.15.3,<5.16.0a0 |>=5.15.4,<5.16.0a0 |>=5.15.6,<5.16.0a0 ] but there are no viable options\n",
      "│  │     │     ├─ qt-main [5.15.3|5.15.4|5.15.6|5.15.8] would require\n",
      "│  │     │     │  └─ icu >=70.1,<71.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     └─ qt-main 5.15.8 would require\n",
      "│  │     │        └─ icu >=72.1,<73.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     ├─ pyqt 5.15.4 would require\n",
      "│  │     │  └─ qt-main 5.15.*  with the potential options\n",
      "│  │     │     ├─ qt-main 5.15.2 would require\n",
      "│  │     │     │  └─ openssl >=1.1.1l,<1.1.2a , which can be installed;\n",
      "│  │     │     ├─ qt-main 5.15.2 would require\n",
      "│  │     │     │  └─ icu >=68.2,<69.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     │     ├─ qt-main [5.15.3|5.15.4|5.15.6|5.15.8], which cannot be installed (as previously explained);\n",
      "│  │     │     └─ qt-main 5.15.8, which cannot be installed (as previously explained);\n",
      "│  │     ├─ pyqt 5.6.0 would require\n",
      "│  │     │  └─ qt [5.6.* |>=5.6.2,<5.7.0a0 ], which requires\n",
      "│  │     │     └─ icu >=58.2,<59.0a0 , which conflicts with any installable versions previously reported;\n",
      "│  │     └─ pyqt 5.9.2 would require\n",
      "│  │        └─ qt 5.9.* , which cannot be installed (as previously explained);\n",
      "│  └─ matplotlib 3.5.1 would require\n",
      "│     └─ pypy3.7 >=7.3.7  with the potential options\n",
      "│        ├─ pypy3.7 7.3.7 would require\n",
      "│        │  └─ python 3.7.* *_73_pypy, which can be installed;\n",
      "│        └─ pypy3.7 7.3.7 would require\n",
      "│           └─ openssl >=1.1.1l,<1.1.2a , which can be installed;\n",
      "├─ openssl 3.1.0**  is uninstallable because it conflicts with any installable versions previously reported;\n",
      "├─ python 3.9.10**  is installable with the potential options\n",
      "│  ├─ python [3.6.13|3.6.15|3.9.10], which can be installed (as previously explained);\n",
      "│  ├─ python 3.9.10 would require\n",
      "│  │  └─ pypy3.9 7.3.8.* , which requires\n",
      "│  │     └─ sqlite >=3.37.1,<4.0a0 , which can be installed;\n",
      "│  └─ python 3.9.10 conflicts with any installable versions previously reported;\n",
      "└─ sqlite 3.37.0**  is uninstallable because it conflicts with any installable versions previously reported.\n",
      "\n",
      "Mark Kittisopikul said: The `conda env export --from-history` option may be useful here. Without the history, it seems like conda could improve by tracking explicit and implicit dependencies separately.\n",
      "\n",
      "1683234106.235179\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683234106.235179]\n",
      "Davis Bennett said: continuing this big day for the python slack channel:\n",
      "https://modal.com/docs/guide\n",
      "Introduction to Modal\n",
      "Modal lets you run code in the cloud without having to think about infrastructure.\n",
      "Davis Bennett said: looks like a great platform for making expensive mistakes\n",
      "Mark Kittisopikul said: A brand new container runtime written in Rust, specifically designed for modern-day use cases.\n",
      "Davis Bennett said: the price / compute hour comes out to 0.191 $, so not really competitive with our in-house cluster on a price basis\n",
      "Ken Carlile said: Amazing how a for profit company costs more than internal chargebacks of imaginary money\n",
      "Davis Bennett said: i bet they are running at a loss, too!\n",
      "\n",
      "1683248443.064539\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683248443.064539]\n",
      "Ben Arthur said: mark-- what can PySR do (if anything) than https://github.com/MilesCranmer/SymbolicRegression.jl can't?  my understanding is that the former is just a thin wrapper around the latter.\n",
      "MilesCranmer/SymbolicRegression.jl\n",
      "Distributed High-Performance symbolic regression in Julia\n",
      "Mark Kittisopikul said: It's not very thin actually.\n",
      "Mark Kittisopikul said: It can also reach out to pytorch, jax, and tensorflow and use sympy.\n",
      "Mark Kittisopikul said: You can also install the whole thing via conda-forge\n",
      "Mark Kittisopikul said: \n",
      "Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl\n",
      "<#C011TMUB3UP|python> and <#C015MJGSM2S|julia>\n",
      "<https://arxiv.org/abs/2305.01582>\n",
      "\n",
      "1683360509.961199\n",
      "1683360559.677279\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683360509.961199]\n",
      "Mark Kittisopikul said: https://github.com/gventuri/pandas-ai\n",
      "gventuri/pandas-ai\n",
      "Pandas AI is a Python library that integrates generative artificial intelligence capabilities into Pandas, making dataframes conversational\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "William Katz said: Seems very useful given how much pandas can do, but I'd worry about hallucinations for more complex queries if it was dynamically generating the code underneath. In most cases you'd be tipped off by not getting what you asked and seeing it visually or in columns. I wonder how many defects will be picked up.\n",
      "OMG, it uses `exec` on a generated code string??\n",
      "https://github.com/gventuri/pandas-ai/issues/43\n",
      "#43 `exec` is risky\n",
      "<https://github.com/gventuri/pandas-ai/blob/95667b94361ec8101ab0ae08183e4d49930fce25/pandasai/__init__.py#L130-L165|pandas-ai/pandasai/__init__.py>\n",
      "Lines 130 to 165 in </gventuri/pandas-ai/commit/95667b94361ec8101ab0ae08183e4d49930fce25|95667b9>\n",
      "we take the code generated by LLM and run it via `exec`, this is extremely risky and can have data leakage or execution at the operating system level\n",
      "if llm returns this:\n",
      "```\n",
      "import os\n",
      "os.environ\n",
      "```\n",
      "all environment variables are exposed, even the LLM token\n",
      "*solution?*\n",
      "limit execution of modules\n",
      "Mark Kittisopikul said: I think this should probably be in a VM like Loic's thing\n",
      "William Katz said: A VM that can't call outside its box?  Connecting to internet or anything else would also be bad.\n",
      "Mark Kittisopikul said: I'm coming to the conclusion that perhaps the only safe space for this is in a web assembly sandbox.\n",
      "William Katz said: Aside from the security issue, there's clearly a hallucination issue:\n",
      "https://github.com/gventuri/pandas-ai/issues/31\n",
      "#31 Key error with Excel data\n",
      "When loading Excel spreadsheet, GPT hallucinates column name (eg., 'HQ Location' or 'Country' instead of 'Location'). If there's vector embedding behind this, column names should be included in the prompt.\n",
      "Mark Kittisopikul said: \n",
      "<image.png>\n",
      "Others reacted to the previous message with panda_face a total of 2 times.\n",
      "\n",
      "1683657219.332329\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683657219.332329]\n",
      "Mark Kittisopikul said: Maybe a wasm sandbox might be appropriate to contain ChatGPT? https://til.simonwillison.net/webassembly/python-in-a-wasm-sandbox\n",
      "Run Python code in a WebAssembly sandbox\n",
      "I've been trying to figure this out for ages. Tim Bart responded to [my call for help on Hacker News](<https://news.ycombinator.com/item?id=34598024>) with [this extremely useful code example](https://g\n",
      "\n",
      "1683659765.531549\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683659765.531549]\n",
      "Mary Lay said: Best way to set up mamba when creating environments on the cluster?  I initially did `conda install mamba -n base -c conda-forge` , which isn't recommended, but how likely am I to run into issues going this route?  And/or what's your solution to the long \"Solving Environment\" time for creating conda envs?  (I just had one take 15+ mins; looking for alternatives).\n",
      "Stuart Berg said: FWIW, that's the method I used to install `mamba`, and it's been working fine for me.  (For years now.)\n",
      "Ben Arthur said: mamba took 15min?  or conda??  i find mamba is much much faster.\n",
      "Mary Lay said: Conda took 15 min.  Mamba ended up taking maybe 2 min.\n",
      "also- thank you Mark Kittisopikul for fixing the slight mess I made with my environments! \n",
      "\n",
      "1683660462.141109\n",
      "1683660561.185249\n",
      "1683660655.398719\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683660462.141109]\n",
      "Davis Bennett said: i don't recall needing to set up mamba, I just installed it\n",
      "Davis Bennett said: did the environment take a long time with mamba? if so, you might want to talk to the person who created it\n",
      "Davis Bennett said: and if you are worried about problems from running `conda install mamba -n base -c conda-forge`, then you can just do the thing recommended on the mamba page and install mambaforge\n",
      "\n",
      "1683662077.430919\n",
      "1683662087.193519\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683662077.430919]\n",
      "Mark Kittisopikul said: Might as well just install mambaforge directly via https://github.com/conda-forge/miniforge#mambaforge\n",
      "Others reacted to the previous message with + a total of 1 times.\n",
      "Mark Kittisopikul said: (not the pypy one)\n",
      "\n",
      "1683662804.934019\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683662804.934019]\n",
      "Adam Taylor said: If micromamba (https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html) will work for you, it's available on dm11 here:\n",
      "/groups/scicompsoft/home/taylora/bin/micromamba\n",
      "micromamba is nice in that it's a standalone executable, and doesn't \"helpfully\" alter your `.bashrc` file for you.  There are apparently some small differences with conda/mamba syntax, but I've never had an issue.  Plus it won't tell you that it wants to update itself all the darn time...\n",
      "Konrad Rokicki said: Maybe we could have a copy in /misc/sc for everyone to use?\n",
      "Mary Lay said: Was about to ask that.\n",
      "Adam Taylor said: Done.\n",
      "Others reacted to the previous message with tada a total of 1 times.\n",
      "\n",
      "1683923632.707029\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683923632.707029]\n",
      "Mark Kittisopikul said: The reactive notebook / dashboard concept seems to be catching on in Python now:\n",
      "https://solara.dev/\n",
      "https://github.com/ipyflow/ipyflow\n",
      "Solara documentation\n",
      "Use ipywidgets with Solara to build powerful and scalable web apps for Jupyter and production in Python.\n",
      "ipyflow/ipyflow\n",
      "Next-generation IPython kernel with reactivity, execution suggestions, syntax extensions, and more.\n",
      "Davis Bennett said: messy execution state is one of my gripes about vanilla jupyter notebooks\n",
      "Mark Kittisopikul said: I think ipyflow has optional reactivity. That seems to be make it messier to me than just fully committing to reactivity.\n",
      "Mark Kittisopikul said: While I understand that some people do not like reactivity, it's a familiar concept to many coming from spreadsheets.\n",
      "Mark Kittisopikul said: e.g. You change a cell, and then the other cells in the spreadsheet change in response.\n",
      "\n",
      "1683990849.295049\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1683990849.295049]\n",
      "Mark Kittisopikul said: Numcodecs is in urgent need of reform or refactoring. Am I the only one who thinks a corruption bug in a compression library might need to be addressed with some urgency?\n",
      "https://github.com/zarr-developers/numcodecs/issues/429\n",
      "#429 Upgrade to ZSTD 1.5.5 due to potential corruption\n",
      "<https://github.com/Blosc/c-blosc/issues/364|Blosc/c-blosc#364>\n",
      "<https://github.com/facebook/zstd/releases/tag/v1.5.5|https://github.com/facebook/zstd/releases/tag/v1.5.5>\n",
      "Mark Kittisopikul said: Numcodecs is dependent on upstream c-blosc 1 upgrading. But upstream has basically abandoned c-blosc 1.\n",
      "https://github.com/Blosc/c-blosc/issues/364\n",
      "#364 Upgrade ZSTD to 1.5.5 due to potential corruption\n",
      "See <https://github.com/facebook/zstd/releases/tag/v1.5.5|https://github.com/facebook/zstd/releases/tag/v1.5.5>\n",
      "\n",
      "1684002801.489359\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684002801.489359]\n",
      "Mark Kittisopikul said: What is Anaconda selling at http://pyscript.com?\n",
      "https://pyscript.com/join\n",
      "Davis Bennett said: is this that \"python in the browser\" thing?\n",
      "\n",
      "1684085651.483869\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684085651.483869]\n",
      "Davis Bennett said: https://github.com/spencerkclark/xpartition\n",
      "spencerkclark/xpartition\n",
      "Tool for writing large xarray datasets to zarr stores with independent processes\n",
      "Davis Bennett said: the noteworthy thing about this repo is that it's a testament to a pretty glaring performance weakness of dask for these workloads\n",
      "\n",
      "1684101972.672329\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684101972.672329]\n",
      "Mark Kittisopikul said: Did `Py_UNICODE` leak into the Zarr v2 specification and is it still supported in Zarr v3?\n",
      "https://zarr.readthedocs.io/en/stable/spec/v2.html#data-type-encoding\n",
      "Mark Kittisopikul said: Does that mean implementations have to support UTF-16 and UTF-32 strings?\n",
      "\n",
      "1684184575.064749\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684184575.064749]\n",
      "Jody Clements said: Does Janelia sci comp have an organization on PyPI?\n",
      "Others reacted to the previous message with eyes a total of 1 times.\n",
      "Donald Olbris said: Not that I'm aware. I don't know that we as a group have released much Python software that was not released through/for a lab. I have helped labs set up their own conda channels, though.\n",
      "Donald Olbris said: And to the obvious follow up question, SciComp doesn't have a conda channel either, as far as I know.\n",
      "\n",
      "1684192069.140219\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684192069.140219]\n",
      "Mark Kittisopikul said: Someone got access to Mojo and made a YouTube video. strong language warning\n",
      "https://www.youtube.com/watch?v=6vznuGPC1Dk\n",
      "[Project] Let's test MOJO eheheheh 🤣 🥲 🥹\n",
      "William Katz said: Not worth a watch at all. I don't know, maybe it's the way that social media has tuned people who want hits to be very extreme in their opinion. \"Oh you don't let me run your early peek on MY hardware, so you are clearly a horrible company and I will delete every post that has your language name in it.\"  He can do whatever he wants, but seems childish. They probably didn't want hassle of Q&A of downloading their stack on variety of platforms as they work through the roadmap and push changes to their service.\n",
      "I got access to Mojo on the weekend, looked around a bit, was surprised they didn't already have timings for the non-vectorized vs vectorized Mandelbrot example. You can add the timings to see how it is on their hardware.  I wandered off to Taichi and other stuff that seemed more usable.\n",
      "William Katz said: It also shouldn't have been an amazing reveal what he was going to get with the playground. They say exactly what they are providing in their docs: https://docs.modular.com/mojo/get-started.html\n",
      "Modular Docs - Try Mojo:fire:\n",
      "An introduction to the Mojo Playground, where you can try Mojo in a virtual environment.\n",
      "Mark Kittisopikul said: I got access to Mojo on the weekend, looked around a bit, was surprised they didn't already have timings for the non-vectorized vs vectorized Mandelbrot example. You can add the timings to see how it is on their hardware. I wandered off to Taichi and other stuff that seemed more usable.\n",
      "It seems you came to the same basic conclusion at the moment, though?\n",
      "William Katz said: Yeah, although I have a particular system I want to implement and Mojo might eventually be a way to do it, but not now, particularly with the churn evident in their roadmap. If I had more time, I'd play around more with their approach.\n",
      "William Katz said: There are some aspects of Taichi that really impresses me. The way they handle sparse volume data structures and separate iteration from defining memory layout is pretty cool. Too bad Mojo doesn't buy out Taichi and integrate complementary parts.\n",
      "Mark Kittisopikul said: I was basically stepping through the frames to see if there was anything interesting to look at. I was trying to figure out how the SIMD works.\n",
      "Davis Bennett said: this is literally \"young man yells at cloud\"\n",
      "Others reacted to the previous message with 100 a total of 2 times.\n",
      "\n",
      "1684243204.111419\n",
      "1684243207.095379\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684243204.111419]\n",
      "Davis Bennett said: https://github.com/scipy/scipy/issues/18118\n",
      "#18118 ENH: The Fortran 77 implementation of COBYLA is buggy and challenging to maintain. Switch to the PRIMA implementation?\n",
      "*Is your feature request related to a problem? Please describe.*\n",
      "Dear SciPy maintainers,\n",
      "This is Dr. Zaikun Zhang from the Hong Kong Polytechnic University. Together with Professor <https://www.numerical.rl.ac.uk/people/nimg/|N.I.M. Gould>, I am responsible for maintaining the derivative-free optimization solvers of the late Professor <https://www.zhangzk.net/powell.html|M.J.D. Powell>.\n",
      "Thank you for making COBYLA available in SciPy. I note that the current version is based on the original Fortran 77 implementation, which is *not maintained anymore*.\n",
      "Although the Fortran 77 code is truly a masterpiece, it contains many bugs, most of which are due to the language itself. For example, see <https://arxiv.org/pdf/2302.13246.pdf|Section 4.4 of our recent paper> and the GitHub issues / requests listed below (*not all* of them concern SciPy).\n",
      "• The Fortran 77 solvers may get *stuck* in infinite loops.\n",
      "    \n",
      "    • <https://github.com/scipy/scipy/issues/8998|optimize: COBYLA hangs / infinite loop #8998>\n",
      "    • <https://github.com/scipy/scipy/issues/15527|BUG: Scipy.optimize / COBYLA hangs on some CPUs #15527>\n",
      "    • <https://github.com/stevengj/nlopt/issues/370|COBYLA freezes (though maxeval and maxtime are given) #370>\n",
      "    • <https://github.com/stevengj/nlopt/issues/118|COBYLA hangs #118>\n",
      "    • <https://github.com/stevengj/nlopt/issues/117|NEWUOA_BOUND stuck in infinite loop inside MMA #117>\n",
      "    • <https://github.com/openturns/openturns/issues/1651|Cobyla freezes in 0T1.16rc1 #1651>\n",
      "    • <https://github.com/astamm/nloptr/issues/25|Optimization freezes #25>\n",
      "    • <https://github.com/cureos/csnumerics/issues/7|BOBYQA gets stuck in infinite loop. #7>\n",
      "    • <https://github.com/xypron/jcobyla/issues/3|Algorithm turns into infinite loop and never finishes #3>\n",
      "• The Fortran 77 solvers may *crash* with <https://en.wikipedia.org/wiki/Segmentation_fault|segmentation faults>  \n",
      "    due to uninitialized variables that are used as indices.\n",
      "    \n",
      "    • <https://github.com/stevengj/nlopt/issues/134|Fix all uninitialized variable warnings #134>\n",
      "    • <https://github.com/stevengj/nlopt/issues/133|BOBYQA uninitialised variables in rare cases #133>\n",
      "    • <https://github.com/stevengj/nlopt/issues/36|Use of uninitialized variable in BOBYQA altmov #36>\n",
      "• Fortran 77 COBYLA may *not return the best point* that is evaluated; sometimes, the returned point can have a  \n",
      "    large constraint violation even though the starting point is feasible.\n",
      "    \n",
      "    • <https://github.com/stevengj/nlopt/issues/182|nlopt COBYLA optimizer gives unexpected output #182>\n",
      "    • <https://github.com/stevengj/nlopt/issues/110|Last Result Returned Not Optimized Result #110>\n",
      "    • <https://github.com/stevengj/nlopt/issues/57|COBYLA returns last evaluated function which might not be minimum #57>\n",
      "    • <https://github.com/cureos/jcobyla/issues/1|Successful termination when constraints violated #1>\n",
      "*Describe the solution you'd like.*\n",
      "To avoid the problems originating from the Fortran 77 code, I suggest you use the <HTTP://www.libprima.net|PRIMA implementation> of Powell's solvers. PRIMA provides the reference implementation for Powell's renowned derivative-free optimization methods, namely COBYLA, UOBYQA, NEWUOA, BOBYQA, and LINCOA. The \"P\" in the name stands for <https://www.zhangzk.net/powell.html|Powell>, and \"RIMA\" is an acronym for \"Reference Implementation with Modernization and Amelioration\".\n",
      "PRIMA provides the modern implementation of the solvers in Fortran 2008. It fixes bugs in the original Fortran 77 code. In addition, it introduces improvements that boost the performance in terms of the *number of function evaluations*, which is the standard measure of computational costs in derivative-free optimization.\n",
      "See <https://github.com/libprima/prima|the GitHub repo> of PRIMA for more information. I will be glad to provide assistance if help is needed.\n",
      "Thanks.\n",
      "*Describe alternatives you've considered.*\n",
      "_No response_\n",
      "*Additional context (e.g. screenshots, GIFs)*\n",
      "Davis Bennett said: this issue is a fun read\n",
      "Stuart Berg said: Interesting.  Somehow I doubt that I've yet written any code that will ever be cherished so sincerely as the package under discussion in that thread.\n",
      "Davis Bennett said: this professor powell guy seems to have been pretty cool\n",
      "\n",
      "1684244535.169289\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684244535.169289]\n",
      "Mark Kittisopikul said: Oh you mean this:\n",
      "https://fortran-lang.discourse.group/t/optimization-without-using-derivatives-the-prima-package-its-fortran-implementation-and-its-inclusion-in-scipy/5798\n",
      "Optimization Without Using Derivatives: the PRIMA Package, its Fortran Implementation, and Its Inclusion in SciPy\n",
      "As mentioned two weeks ago under the thread of LFortran, I have been developing a package named PRIMA for solving general nonlinear optimization problems without using derivatives. @certik suggested that I should write about PRIMA, especially its inclusion in SciPy. What is PRIMA? PRIMA is a package for solving general nonlinear optimization problems without using derivatives. It provides the reference implementation of Powell’s renowned derivative-free optimization methods, i.e., COBYLA, UOB...\n",
      "\n",
      "1684245082.173689\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684245082.173689]\n",
      "Davis Bennett said: not really, I mean the github issue \n",
      "\n",
      "1684247818.206809\n",
      "--------------------------------------------------\n",
      "Document[channel=C011TMUB3UP,ts=1684247818.206809]\n",
      "Davis Bennett said: another one of these, but this one is very rusty: https://erg-lang.org/\n",
      "Mark Kittisopikul said: I'm confused. What's our operational definition of statically typed again and how is that applied here?\n",
      "Davis Bennett said: i think the normal definition applies? i.e. types are known before the code runs?\n",
      "Mark Kittisopikul said: When do we check that?\n",
      "Mark Kittisopikul said: It doesn't look like there is a static analysis or compilation step here.\n",
      "Mark Kittisopikul said: So the static typing errors are reported at runtime?\n",
      "Davis Bennett said: no clue, i just found it on github \n",
      "Mark Kittisopikul said: Hehe, we can do the Python 2 no parentheses thing.\n",
      "print! \"Hello, World!\" # OK\n",
      "\n",
      "Davis Bennett said: yeah that struck me as odd\n",
      "Mark Kittisopikul said: This is more like Ruby than Python\n",
      "Davis Bennett said: it seems that generally parens are optional if it's unambiguous, which is a choice\n",
      "Mark Kittisopikul said: Oh yea, the Ruby influence is very strong here.\n",
      "for! 0..9, i =>\n",
      "    print! i; print! i\n",
      "\n",
      "Mark Kittisopikul said: The only thing that makes this very Python-like is the indentation I think\n",
      "Davis Bennett said: ...and the fact that it emits python bytecode\n",
      "Davis Bennett said: https://erg-lang.org/the-erg-book/33_integration_with_Python.html\n",
      "\n",
      "1681693804.160259\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1681693804.160259]\n",
      "Donald Olbris said: I had been using free fillable forms for both federal and Virginia for the past several years, until Virginia discontinued it 2 years ago. I've been filing Virginia on paper since then (still using fed free fillable forms without issue). Thanks for the pointer to Cash App; I'll look into it for next year.\n",
      "William Katz said: WAPO ran a story about how the commercial companies are making money off tax prep by mining the data, and I wonder if this “personalization” question gets asked by Cash App. (I had used credit karma before trying IRS free fillable forms last year.)\n",
      "https://www.washingtonpost.com/technology/2023/04/11/tax-prep-turbotax-privacy/\n",
      "How to file your taxes without selling your soul\n",
      "TurboTax and H&amp;R Block ask you to give up the ironclad secrecy of your tax return. Why? To help them make more money.\n",
      "Others reacted to the previous message with face_in_clouds a total of 1 times.\n",
      "William Katz said: We should all have the same Janelia Employer IDs on our W2 so that makes me question the source of the error they were reporting. At least last year, the IRS web app rejects a submission only after you submit it and they literally email (to general public)  \"The XML data has failed schema validation. cvc-complex-type.2.4.a\" and proceed with a dump of the XPath. I wonder if it could've been a browser issue or additional forms (Sch C) messing things up.\n",
      "\n",
      "1682015117.219289\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682015117.219289]\n",
      "Davis Bennett said: https://www.moderndescartes.com/essays/why_brain/\n",
      "Davis Bennett said: an essay from an ex-google-brainer\n",
      "Davis Bennett said: google brain is now folded into deepmind (which is now \"google deepmind\") https://www.deepmind.com/blog/announcing-google-deepmind\n",
      "Announcing Google DeepMind\n",
      "DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.\n",
      "\n",
      "1682025591.294079\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682025591.294079]\n",
      "Konrad Rokicki said: https://www.cat-gpt.com\n",
      "Others reacted to the previous message with smirk_cat a total of 4 times.\n",
      "Stuart Berg said: Honestly, I think it would be unethical to use that system until they release more details on how it was trained.\n",
      "Others reacted to the previous message with cattype a total of 1 times, and with catjam a total of 1 times.\n",
      "\n",
      "1682095848.134929\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682095848.134929]\n",
      "Davis Bennett said: https://www.fileside.app/blog/2023-03-17_windows-file-paths/\n",
      "The weird world of Windows file paths\n",
      "File system paths on Windows are stranger than you might think. On any Unix-derived system, a path is an admirably simple thing: if it starts with a /, it’s a path. Not so on Windows, which serves up a bewildering variety of schemes for composing a path.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Mary Lay said: That last part is fascinating and horrifying at the same time. Is there ever a “wipe the slate clean” point, or is Windows doomed to support all these options for the rest of eternity?\n",
      "Mark Kittisopikul said: On the other hand you can often now use forward slashes:\n",
      "PS C:\\> cd c:/users/kittisopikulm\n",
      "PS C:\\users\\kittisopikulm>\n",
      "\n",
      "\n",
      "1682128181.582019\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682128181.582019]\n",
      "Mark Kittisopikul said: \n",
      "This is an nice restatement of the \"Two Language Problem\" as the \"Two Culture Problem\"\n",
      "<https://scientificcoder.com/my-target-audience>\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "\n",
      "1682192894.662909\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682192894.662909]\n",
      "Mark Kittisopikul said: I talked about security keys at a recent . Here is a recent article on the topic.\n",
      "https://www.wired.com/story/passwords-passkey-transition-time/\n",
      "The War on Passwords Enters a Chaotic New Phase\n",
      "The transition from traditional logins to cryptographic passkeys is getting messy. But don’t worry—there’s a plan.\n",
      "Mark Kittisopikul said: Don is encouraging me to add context...  I did not find the article this way, but I found the discussion on Hacker News interesting:\n",
      "https://news.ycombinator.com/item?id=35675567#35677212\n",
      "\n",
      "1682241936.689379\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682241936.689379]\n",
      "Mark Kittisopikul said: Rusty Object Notation (RON) seems like it must have a future at Janelia if only just because of the name. Trailing commas are also nice.\n",
      "https://github.com/ron-rs/ron\n",
      "ron-rs/ron\n",
      "Rusty Object Notation\n",
      "Others reacted to the previous message with 90s_ron a total of 4 times, and with joy a total of 1 times.\n",
      "\n",
      "1682349095.764969\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682349095.764969]\n",
      "Davis Bennett said: does anyone else routinely find their network drives going haywire? e.g.:\n",
      "bennettd@bennettd-ws1 (base) ➜  ~ ls -lh  /\n",
      "ls: cannot access '/nearline': Permission denied\n",
      "ls: cannot access '/nrs': Permission denied\n",
      "ls: cannot access '/groups': Permission denied\n",
      "total 2.1G\n",
      "lrwxrwxrwx   1 root root    7 Jul 19  2022 bin -> usr/bin\n",
      "drwxr-xr-x   4 root root 4.0K Jul 19  2022 boot\n",
      "drwxr-xr-x   2 root root 4.0K Jul 19  2022 cdrom\n",
      "drwxr-xr-x  20 root root 5.0K Apr 18 07:24 dev\n",
      "drwxr-xr-x 138 root root  12K Oct 28 19:08 etc\n",
      "d?????????   ? ?    ?       ?            ? groups\n",
      "drwxr-xr-x   8 root root 4.0K Sep  8  2022 home\n",
      "lrwxrwxrwx   1 root root    7 Jul 19  2022 lib -> usr/lib\n",
      "lrwxrwxrwx   1 root root    9 Jul 19  2022 lib32 -> usr/lib32\n",
      "lrwxrwxrwx   1 root root    9 Jul 19  2022 lib64 -> usr/lib64\n",
      "lrwxrwxrwx   1 root root   10 Jul 19  2022 libx32 -> usr/libx32\n",
      "drwx------   2 root root  16K Jul 19  2022 lost+found\n",
      "drwxr-xr-x   4 root root 4.0K Sep  8  2022 media\n",
      "drwxr-xr-x   2 root root 4.0K Feb 23  2022 mnt\n",
      "d?????????   ? ?    ?       ?            ? nearline\n",
      "d?????????   ? ?    ?       ?            ? nrs\n",
      "drwxr-xr-x   3 root root 4.0K Oct 28 19:08 opt\n",
      "dr-xr-xr-x 996 root root    0 Mar 21 13:23 proc\n",
      "drwx------   7 root root 4.0K Nov 17 09:57 root\n",
      "drwxr-xr-x  38 root root 1.2K Apr 24 11:07 run\n",
      "lrwxrwxrwx   1 root root    8 Jul 19  2022 sbin -> usr/sbin\n",
      "drwxr-xr-x  14 root root 4.0K Jul 20  2022 snap\n",
      "drwxr-xr-x   2 root root 4.0K Feb 23  2022 srv\n",
      "-rw-------   1 root root 2.0G Jul 19  2022 swapfile\n",
      "dr-xr-xr-x  13 root root    0 Mar 21 13:23 sys\n",
      "drwxrwxrwt  38 root root 4.0K Apr 24 11:09 tmp\n",
      "drwxr-xr-x  15 root root 4.0K Aug 17  2022 usr\n",
      "drwxr-xr-x  15 root root 4.0K Aug 17  2022 var\n",
      "bennettd@bennettd-ws1 (base) ➜  ~ sudo umount /nearline\n",
      "[sudo] password for bennettd:\n",
      "bennettd@bennettd-ws1 (base) ➜  ~ sudo umount /nrs\n",
      "bennettd@bennettd-ws1 (base) ➜  ~ sudo umount /groups\n",
      "umount.nfs4: /groups: device is busy\n",
      "\n",
      "Davis Bennett said: and the `mount` command I used to use no longer works, which makes me think something changed about the drives?\n",
      "Davis Bennett said: `mount.nfs: an incorrect mount option was specified` very helpful error \n",
      "Davis Bennett said: (i opened a https://issues.hhmi.org/issues/servicedesk/customer/portal/10/IT-69863)\n",
      "Kristin Branson said: yes, i think sam and alice have had some issues with inescapable badness happening. i think for alice it is kerberos tickets expiring. sam had some issues with stale nfs. ken has been able to solve all problems afaik. if it's helpful, here is my mount script:\n",
      "#!/bin/bash\n",
      "sudo mkdir -p /groups || echo \"Failed to create directory /groups\";\n",
      "sudo mount -o rw,hard,bg,nolock,nfsvers=4.0,sec=krb5 dm11.hhmi.org:/ifs/groups/ /groups/ || echo \"Could not mount /groups\";\n",
      "sudo mkdir -p /nrs || echo \"Failed to create directory /nrs\";\n",
      "sudo mount -o rw,hard,bg,nolock,nfsvers=4.1,sec=krb5 nrs.hhmi.org:/nrs/ /nrs/ || echo \"Failed to mount /nrs\";\n",
      "sudo mkdir -p /nearline || echo \"Failed to create directory /nearline\";\n",
      "#sudo mount -t cifs -o cruid=$USER,sec=krb5,cifsacl,multiuser //nearline4.hhmi.org/branson /nearline/branson || echo \"Failed to mount /nearline/branson\";\n",
      "sudo mount -o rw,hard,bg,nolock,nfsvers=4.1,sec=krb5 nearline4.hhmi.org:/nearline/ /nearline/ || echo \"Failed to mount /nearline\";\n",
      "sudo mkdir -p /misc || echo \"Failed to create directory /misc\";\n",
      "#sudo mount -o rw,hard,bg,nolock,nfsvers=4.0,sec=krb5 nfs40g.isln-j.janelia.org:/ifs/misc  /misc || echo \"Failed to mount /misc\";\n",
      "sudo mount -o rw,hard,bg,nolock,nfsvers=4.0,sec=krb5 dm11.hhmi.org:/ifs/misc /misc || echo \"Failed to mount /misc\";\n",
      "\n",
      "Davis Bennett said: my problems went away after restarting. \n",
      "\n",
      "1682380800.251899\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682380800.251899]\n",
      "Philip Hubbard said: https://www.sri.com/press/press-release/the-palo-alto-research-center-parc-will-join-sri-international/\n",
      "The Palo Alto Research Center (PARC) will join SRI International\n",
      "Signaling a new chapter for Silicon Valley, two veteran R&amp;D leaders are combining minds to advance and accelerate world-changing science and technology. \n",
      "Mark Kittisopikul said: I understand SRI was formerly \"Stanford Research Institute\", and this \"parc\" was Xeros Parc of early computing fame. I do not have much other context than this. Is this good?\n",
      "Philip Hubbard said: On Mastodon, I have seen only positive opinions:\n",
      "Philip Hubbard said: https://sigmoid.social/@Riedl/110256252305834076\n",
      "Mark Riedl (@Riedl@sigmoid.social)\n",
      "Xerox “donates” PARC to SRI. \n",
      "<https://www.sri.com/press/press-release/the-palo-alto-research-center-parc-will-join-sri-international/>\n",
      "I feel like Xerox always regretted acquiring PARC. I think this will be good for PARC.\n",
      "Philip Hubbard said: https://sigmoid.social/@shiwali/110256122669780023\n",
      "Shiwali Mohan | शिवाली मोहन (@shiwali@sigmoid.social)\n",
      "#PARC joins forces with #StanfordResearchInstitute #SRI #SRIInc to form one of the largest #CS #AI #Tech research non-profits. \n",
      "Research at a non-profit, that is funded by the public, supports science that is conducted in good faith, is accessible, and is for the social, communal good. \n",
      "I am so very excited! \n",
      "I am also very grateful that market pressures don't pull my research in random directions.\n",
      "SRI: <https://www.sri.com>\n",
      "\n",
      "1682600404.127319\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682600404.127319]\n",
      "Mark Kittisopikul said: Yes.\n",
      "Mark Kittisopikul said: I randomly came across this while browsing PubMedCentral:\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1169563/\n",
      "Philip Hubbard said: Wasn't that about 21 years ago, if I'm understanding correctly?  I wonder if there has been any more recent study.\n",
      "\n",
      "1682697865.341739\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682697865.341739]\n",
      "Nathan Soules said: https://aws.amazon.com/codewhisperer/ AWS's response to co-pilot\n",
      "AI Code Generator - Amazon CodeWhisperer - AWS\n",
      "Amazon CodeWhisperer is an AI coding companion that generates whole line and full function code suggestions in your IDE to help you get more done faster.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "William Katz said: That's nice they can tell you the source and its license.\n",
      "\n",
      "1682715789.026809\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682715789.026809]\n",
      "Virginia Scarlett said: \n",
      "<Screenshot_20230423_184313_Instagram.jpg>\n",
      "Others reacted to the previous message with joy a total of 6 times, and with knife_fork_plate a total of 1 times.\n",
      "\n",
      "1682736164.944119\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682736164.944119]\n",
      "Mark Kittisopikul said: An evaluation of risks associated with relying on Fortran for mission critical codes for the next 15 years\n",
      "https://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/LA-UR-23-23992\n",
      "Mark Kittisopikul said: Discussion for context:\n",
      "https://fortran-lang.discourse.group/t/an-evaluation-of-risks-associated-with-relying-on-fortran-for-mission-critical-codes-for-the-next-15-years/5644\n",
      "An evaluation of risks associated with relying on Fortran for mission critical codes for the next 15 years\n",
      "<https://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/LA-UR-23-23992>\n",
      "Davis Bennett said: i wonder if we will see similar documents for c++ / python in 15 years\n",
      "\n",
      "1682806187.482319\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682806187.482319]\n",
      "Mark Kittisopikul said: Run and write MATLAB from VSCode:\n",
      "https://blogs.mathworks.com/matlab/2023/04/26/do-you-use-visual-studio-code-matlab-is-now-there-too/\n",
      "Do you use Visual Studio Code? MATLAB is now there too.\n",
      "Along with many other developers, it was love at first sight for me when I first experienced Visual Studio Code. Highly customisable, easy to use and available for all 3 major operating systems; it took hardly any time at all for me to switch from using a plethora of editors across all my machines to using just two – The\n",
      "Magdalena Schneider said: The current version does not seem to let you run the MATLAB code directly within VSCode, though: \"it doesn’t include things such as the ability to execute MATLAB code or debugging support\".\n",
      "\n",
      "1682806927.099709\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1682806927.099709]\n",
      "Philip Hubbard said: \"Findings  In this cross-sectional study of 195 randomly drawn patient questions from a social media forum, a team of licensed health care professionals compared physician’s and chatbot’s responses to patient’s questions asked publicly on a public social media forum. The chatbot responses were preferred over physician responses and rated significantly higher for both quality and empathy.\" https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309\n",
      "Philip Hubbard said: Some comments on these findings: https://mastodon.gamedev.place/@BartWronski/110278294531951964\n",
      "Bart Wronski :flag-ua: (@BartWronski@mastodon.gamedev.place)\n",
      "Attached: 1 image\n",
      "<https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309> Note: The quality was judged by other doctors.\n",
      "AI outperforms physicians.\n",
      "This shouldn't be surprising - every profession is full of incompetent people. AI of average competence can still be better than half of the professionals. ¯\\_(ツ)_/¯\n",
      "William Katz said: Q: What do you call the person who finishes last in their med school class?\n",
      "A: Doctor.\n",
      "Philip Hubbard said: My interpretation of that (old) joke had been that it was a pep talk, but perhaps it is a condemnation.\n",
      "William Katz said: The biggest hurdles to graduate aren't the classes, it's the boards and residency acceptance rate varies a lot across programs & disciplines. And there's also a large range of quality when you include all med schools including those overseas catering to Americans. But my biggest bias is working with some who have very very little empathy and/or have sub-optimal reasoning skills. (In general, everyone memorizes well and works relatively hard at least through residency).\n",
      "Mark Kittisopikul said: I never finished med school, but I don't recall empathy being a quality strongly selected for by the boards.\n",
      "William Katz said: Yeah, there's at least some ability for the least empathic to choose appropriately... like pathology \n",
      "Mark Kittisopikul said: I initially questioned the need for some of the ethics and empathy sessions we needed, but after seeing how some of my classmates responded, I definitely saw the need.\n",
      "Mark Kittisopikul said: It was still questionable if those sessions could actually have any influence though...\n",
      "William Katz said: Empathy also erodes when faced with frequent 100+ hr weeks (I think they put a ceiling around my time at 120 hrs), crappy call schedules, and a lot of the stuff that goes down in hospitals that erode your faith in humanity.\n",
      "William Katz said: Philip Hubbard After skimming that JAMA article, I think it has some problems. First, are physicians who respond for free on reddit (after taking time to get verified by mods) representative of the average physician? (I doubt it.) Second, there's no restriction for these online physicians to answer questions in any discipline. Pretty sure they don't check if they're licensed for the appropriate subject matter. Finally, the fact that it's online and unpaid, complete with pseudonyms, means they are likely to show even less empathy than they would in real-world settings. Chatbots don't behave differently in those settings. That said, I do think that AI advice makes sense if they can really address hallucination issues, even if it's quite rare.\n",
      "Others reacted to the previous message with eyes a total of 1 times.\n",
      "Philip Hubbard said: Good points.\n",
      "Ben Arthur said: i was recently referred to a doctor by another one, and explicitly warned about his poor bedside manner.  it was truly awful!  but he was great otherwise.  i would certainly prefer that to the other way around.\n",
      "Ben Arthur said: worth noting too that he went to one of those \"overseas med schools catering to americans\", but then did a residency and fellowship at mayo and penn.  had an american flag on display in his waiting room.\n",
      "William Katz said: Ben Arthur The overseas med schools (e.g., the Caribbean ones) have a reputation but there are certainly excellent physicians who come from them. One of the top med students in my class went through additional post-baccalaureate training at one of those schools before admission. She did say there was a wide variation in abilities and wealth. I also imagine the state of those schools way back when I was a student is different than now with admissions being much more competitive.\n",
      "William Katz said: Eric Topol wrote an https://erictopol.substack.com/p/when-patient-questions-are-answered. I didn't realize he wrote a book on \"Deep Medicine\" a few years ago arguing that AI could actually strengthen the doctor-patient relationship by giving doctors back the \"Gift of Time\".  For the JAMA paper, he points out that it doesn't discuss additive use of AI (so physician + AI) and the really bad empathy scores are somewhat grounded in the lack of time physicians have to respond and explore a patient's concerns.  There's a new book (https://erictopol.substack.com/p/the-gpt-x-revolution-in-medicine) that explores this topic.\n",
      "When Patient Questions Are Answered With Higher Quality and Empathy by ChatGPT than Physicians\n",
      "A new study with significant implications\n",
      "The GPT-x Revolution in Medicine\n",
      "Review of a new book with 6 months of assessing GPT-4 for medical applications\n",
      "\n",
      "1683216434.497699\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683216434.497699]\n",
      "Philip Hubbard said: A little analysis on \"the most interesting piece of writing I’ve seen about LLMs in a while\": https://simonwillison.net/2023/May/4/no-moat/\n",
      "Leaked Google document: “We Have No Moat, And Neither Does OpenAI”\n",
      "SemiAnalysis published something of a bombshell leaked document this morning: Google “We Have No Moat, And Neither Does OpenAI”. The source of the document is vague: The text below is …\n",
      "Philip Hubbard said: https://sigmoid.social/@Riedl/110311171092286651\n",
      "Mark Riedl (@Riedl@sigmoid.social)\n",
      "Purportedly leaked internal Google document\n",
      "\"We Have No Moat, And Neither Does OpenAI\"\n",
      "<https://www.semianalysis.com/p/google-we-have-no-moat-and-neither>\n",
      "Whether this is a real document or not, I believe the larger message that Google and OpenAI are running on borrowed time is legit. I've been saying as much.\n",
      "Others reacted to the previous message with +1 a total of 2 times, and with thinking_face a total of 1 times.\n",
      "Philip Hubbard said: The document argues that the leak of the Meta LLaMA model two months ago has caused an explosion of open-source activity.  Another key factor has been LoRA (low-rank adaptation).\n",
      "Philip Hubbard said: \"LoRA updates are very cheap to produce (~$100) for the most popular model sizes. This means that almost anyone with an idea can generate one and distribute it. Training times under a day are the norm. At that pace, it doesn’t take long before the cumulative effect of all of these fine-tunings overcomes starting off at a size disadvantage. Indeed, in terms of engineer-hours, the pace of improvement from these models vastly outstrips what we can do with our largest variants, and the best are already largely indistinguishable from ChatGPT. Focusing on maintaining some of the largest models on the planet actually puts us at a disadvantage.\"\n",
      "Philip Hubbard said: \"And in the end, OpenAI doesn’t matter. They are making the same mistakes we are in their posture relative to open source, and their ability to maintain an edge is necessarily in question. Open source alternatives can and will eventually eclipse them unless they change their stance. In this respect, at least, we can make the first move.\"\n",
      "\n",
      "1683300399.324659\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683300399.324659]\n",
      "Philip Hubbard said: Can anyone recommend a nice destination in the Blue Ridge Mountains for a family trip in June (with two 7-year olds)?  I had written down that JARS went to Wintergreen Resort a while back, and the area looks appealing but that particular resort may have more of a luxury focus than we need.  Thanks.\n",
      "Davis Bennett said: not sure how long you are willing to drive, but i think asheville is really nice\n",
      "Philip Hubbard said: Indeed.  I'm from NC (as you!) so I like Asheville, but we are interested in a closer destination if possible.\n",
      "Daniel Milkie said: https://www.dcr.virginia.gov/state-parks/sky-meadows\n",
      "The park amenities are always in good condition.  They have good looking events all the time.  Including a \"https://www.dcr.virginia.gov/state-parks/event?id=2023-03-28-10-09-22-835374-6gv\" .\n",
      "https://royalhorseshoe.com/\n",
      "Took my girls (10 and 7) on this horse ride on legit trails (aka not a multiuse path).\n",
      "Marys Rock is a good hike.\n",
      "Nathan Soules said: https://www.nps.gov/shen/index.htm Shenandoah National Park is close by, and there are a lot of great options.  Skyland is great if you want lodging: https://www.goshenandoah.com/lodging/skyland or Big Meadows for camping/RV  https://www.nps.gov/places/big-meadows.htm.\n",
      "Shenandoah National Park (U.S. National Park Service)\n",
      "Shenandoah National Park Home Page\n",
      "Skyland on Skyline Drive | Shenandoah National Park Lodging\n",
      "Located on Skyline Drive in the Blue Ridge Mountains there's no better Shenandoah hotel option than Skyland right in Shenandoah National Park. Check out our lodging options and book your getaway today!\n",
      "\n",
      "1683314914.754999\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683314914.754999]\n",
      "Philip Hubbard said: It looks like https://github.com/openlm-research/open_llama is not only interesting itself but also references a number of other interesting projects, like the RedPajama dataset, the Meerkat visualization tool, the Stanford Hazy Research group, etc.\n",
      "openlm-research/open_llama\n",
      "\n",
      "1683315311.416879\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683315311.416879]\n",
      "Philip Hubbard said: It's hard to keep up: \"There’s a lot to absorb about this one. Mosaic trained this model from scratch on 1 trillion tokens, at a cost of $200,000 taking 9.5 days. It’s Apache-2.0 licensed and the model weights are available today.\" https://simonwillison.net/2023/May/5/\n",
      "\n",
      "1683590873.340669\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683590873.340669]\n",
      "Philip Hubbard said: https://contentauthenticity.org\n",
      "Content Authenticity Initiative\n",
      "Creating the standard for digital content provenance.\n",
      "Others reacted to the previous message with clap a total of 1 times.\n",
      "Philip Hubbard said: Some big names are members: Adobe, AP, Canon, Leica, Microsoft, Nikon, NY Times, NVIDIA,\n",
      "Philip Hubbard said: Some big names are not: Alphabet, Apple, Meta, OpenAI.\n",
      "\n",
      "1683718881.887769\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683718881.887769]\n",
      "Kristin Branson said: https://www.aicrowd.com/challenges/hackaprompt-2023\n",
      "AIcrowd | HackAPrompt 2023 | Challenges\n",
      "Trick Large Language Models\n",
      "\n",
      "1683739961.468619\n",
      "1683740039.086309\n",
      "1683740072.102769\n",
      "1683740086.497359\n",
      "1683740206.988869\n",
      "1683740257.842139\n",
      "1683740288.358169\n",
      "1683740383.469739\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683739961.468619]\n",
      "Mark Kittisopikul said: They are offering shots (drinks) in the gallery.\n",
      "<IMG_20230510_133227.jpg>\n",
      "Others reacted to the previous message with raised_hands::skin-tone-4 a total of 1 times, and with zipper_mouth_face a total of 1 times.\n",
      "Ken Carlile said: sounds horrifying\n",
      "Roian Egnor said: i’m awake! :)\n",
      "Ken Carlile said: I'll be awake if I don't have to drink that?\n",
      "Michael Innerberger said: The shots are actually quite nice and they also have cookies / brownies \n",
      "Ken Carlile said: oh, well then. \n",
      "Ken Carlile said: if I wasn't buried in cables in the datacenter...\n",
      "Others reacted to the previous message with knot a total of 3 times.\n",
      "Roian Egnor said: you need one of those shots for the energy to escape \n",
      "\n",
      "1683761959.512199\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683761959.512199]\n",
      "Mary Lay said: Anyone know what’s up with the “Prehistoric village archaeological site”? (Also, I could swear this tag wasn’t here a month ago.)\n",
      "<IMG_5216.png>\n",
      "Kristin Branson said: https://www.jstor.org/stable/24531465\n",
      "A prehistoric Indian site on Selden Island, Montgomery County, Md on JSTOR\n",
      "Richard G. Slattery, A prehistoric Indian site on Selden Island, Montgomery County, Md, Journal of the Washington Academy of Sciences, Vol. 36, No. 8 (August 15, 1946), pp. 262-266\n",
      "Others reacted to the previous message with +1 a total of 2 times, and with exploding_head a total of 1 times.\n",
      "Mary Lay said: Where’s the Indians Jones emoji for this? Haha\n",
      "Others reacted to the previous message with laughing a total of 3 times, and with male-detective a total of 2 times.\n",
      "Brenda Arevalo said: https://en.wikipedia.org/wiki/Walker_Prehistoric_Village_Archeological_Site\n",
      "Walker Prehistoric Village Archeological Site\n",
      "Walker Prehistoric Village Archeological Site is an archeological site located near Poolesville, Montgomery County, Maryland.  The site is a large Late Woodland village located on Selden Island in the Potomac River. Excavations carried out in the 1930s and 1940s revealed a 40-foot section of a palisade, circular house patterns, shallow oval pits and cylindrical pits, and flexed burials interred in the floors of the houses.The island lends its name to a characteristic Early Woodland period ceramic ware known as Selden Island ware, dating from 1000 – 750 BCE and distributed from Virginia to Delaware and southeastern Pennsylvania including Maryland's piedmont and coastal plain.The archaeological site was listed on the National Register of Historic Places in 1975.\n",
      "\n",
      "1683826237.255019\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683826237.255019]\n",
      "Philip Hubbard said: https://sigmoid.social/@Riedl/110351126213957842\n",
      "Mark Riedl (@Riedl@sigmoid.social)\n",
      "Allen Institute is working on a new open LLM \n",
      "<https://blog.allenai.org/announcing-ai2-olmo-an-open-language-model-made-by-scientists-for-scientists-ab761e4e9b76>\n",
      "We are about to be awash with LLMs of varying degrees of openness.\n",
      "\n",
      "1683909261.450489\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683909261.450489]\n",
      "Brenda Arevalo said: JWST has selected the 2nd year study roster & published the winners. I managed Grants & Contracts at UNC some years ago & helped PIs create abstract catalog pages. When competing against 1000s+, creative titles stand out-like some did here: Crouching Galaxy, Hidden Stars…  Solving a Solar Neighborhood Crime Scene…Shaken and Stirred: Shocks and Turbulence… To be or not to be in equilibrium… Yale will be studying runaway SMBHs (super massive black hole). They think they found the markers of one. It’s been 50+ years since the theory was proposed & JWST is giving them a chance to prove it, should be interesting. https://www.stsci.edu/files/live/sites/www/files/home/jwst/science-execution/approved-programs/general-observers/cycle-2-go/_documents/jwst-cycle2-general-observer-abstracts.pdf\n",
      "Others reacted to the previous message with telescope a total of 2 times, and with rolling_on_the_floor_laughing a total of 1 times.\n",
      "Davis Bennett said: \"A Hot View of Cold Gas\" \n",
      "Others reacted to the previous message with joy a total of 2 times.\n",
      "\n",
      "1683919441.857919\n",
      "--------------------------------------------------\n",
      "Document[channel=C011W6YDV99,ts=1683919441.857919]\n",
      "Mark Kittisopikul said: On Wednesday, May 17, 2023, 10-12:30pm, the Academies of Loudoun (a local public magnet high school) is holding a research symposium where their students will present their work. If anyone would like to coordinate going there, let me know.\n",
      "<ACL Research Symposium Community Member Letter.pdf>\n",
      "Others reacted to the previous message with + a total of 2 times.\n",
      "Mark Kittisopikul said: Academies of Loudoun webpage:\n",
      "https://www.lcps.org/acl\n",
      "John Bogovic said: and they ask that you rsvp if you'd like to go (follow a link in that document).\n",
      "Mark Kittisopikul said: Direct link to the Google Form here:\n",
      "https://forms.gle/BtiUpBjxTy7JF2uz8\n",
      "Community Member Expert Registration May 2023\n",
      "The symposium will take place on Wednesday, May 17, 2023 10-12:30pm. We are inviting members of the STEM community to participate as guest experts to listen to student presentations and ask questions/engage in authentic conversation with student presenters; evaluate presentations based on scientific/engineering merit, project design, data collection and analysis, and presentation.\n",
      "Christopher Bleck said: I’m sorry, but I won’t be able to attend the student session as I already have a busy schedule. However, I appreciate the invitation. Please feel free to contact me earlier in the future for any other opportunities.\n",
      "Others reacted to the previous message with heavy_check_mark a total of 1 times.\n",
      "\n",
      "1681844751.750139\n",
      "1681844765.907739\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1681844751.750139]\n",
      "Mark Kittisopikul said: Is it just me or did the standard Github layout change?\n",
      "Mark Kittisopikul said: I kept trying to find the green button by muscle memory, and it wasn't there.\n",
      "Jody Clements said: Yes, Code and Overview buttons are new as well\n",
      "Jody Clements said: Green code button moved to top right, annoyingly\n",
      "Mark Kittisopikul said: and it's smaller\n",
      "Jody Clements said: Obviously developers want to get to the README before the code\n",
      "Mark Kittisopikul said: And the text is overflowing:\n",
      "<image.png>\n",
      "Jody Clements said: you should probably rebase that branch \n",
      "Mark Kittisopikul said: It's not mine\n",
      "Mark Kittisopikul said: That's from here:\n",
      "https://github.com/eschnett/Yggdrasil\n",
      "eschnett/Yggdrasil\n",
      "Collection of builder repositories for BinaryBuilder.jl\n",
      "William Katz said: Definitely highlights crappy READMEs \n",
      "Davis Bennett said: i noticed something similar\n",
      "Others reacted to the previous message with + a total of 1 times.\n",
      "\n",
      "1682018703.146099\n",
      "1682018931.061479\n",
      "1682019028.653599\n",
      "1682019030.486809\n",
      "1682019036.646849\n",
      "1682019132.668309\n",
      "1682019234.665839\n",
      "1682019412.048639\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682018703.146099]\n",
      "William Patton said: whats the best practice on large cluster jobs? I have a job that will take approximately 8 days using all 248 gpu_rtx nodes. Its a blockwise convolutional neural network prediction job so its embarrassingly parallel and scales almost linearly with number of gpus. Is it better to take 120 for 16 days ish or somewhere in between?\n",
      "Ben Arthur said: how long does each of your jobs take?\n",
      "William Patton said: It's just one job. I just need to start a bunch of workers to run prediction in parallel.\n",
      "Ben Arthur said: how long does each worker take to process one task?\n",
      "William Patton said: Oh only a few seconds per block\n",
      "Ken Carlile said: There is a limit on how many gpus you can use at one time. It's programatic, so just submit everything at once.\n",
      "William Patton said: Ah whats the limit?\n",
      "Ken Carlile said: the limit is 150\n",
      "Ken Carlile said: sorry, had to look that up\n",
      "William Patton said: Ok, perfect. I'll stick to 150 then\n",
      "Robert Lines said: That is the limit today.  It can be varied based on the mix of jobs running but generally it is at ~80% of the total gpus.  Also of note there are only 176 gpus available in the gpu_rtx queue.  22 nodes * 8 gpus.  1 of the rtx2080ti nodes is upgraded to OL9 and in the test queue.\n",
      "William Patton said: Oh I see. If I'm using ~80% and cluster usage goes up will it kill any of my workers?\n",
      "\n",
      "1682019947.863859\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682019947.863859]\n",
      "Robert Lines said: It won't kill your jobs we just may reduce the cap to relieve a backlog should it come up.  Right now it is a little higher at ~86% of the gpus because we had another node fail recent and the upgraded os node being out of the queue.  But there hasn't been high contention to encourage reducing it.  All of that is to say that the 150 number may change without notice but it won't kill your jobs if you end up exceeding the limit it will just not start new jobs until you are below the limit.\n",
      "I would encourage you to submit the jobs in the most logical work unit.  Though that may not fit if you are just running a bunch of workers who are waiting around to grab tasks off a work queue.\n",
      "\n",
      "1682020310.047969\n",
      "1682020390.210469\n",
      "1682020506.304319\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682020310.047969]\n",
      "William Patton said: Exactly, it's just a bunch of workers I'm starting that will be grabbing tasks, (in this case just spatial blocks) to read, predict, write.\n",
      "No point in trying to start more workers than the limit if they will just be sitting waiting for a gpu until the rest of the workers have finished the task queue.\n",
      "William Patton said: And it sounds like the only down side to requesting all 150 currently available would be that if one fails, it might not be able to restart\n",
      "Robert Lines said: yep.  If a worker dies does it have to start a new one or will it continue with n-1 workers?  And does it have to have all N workers running before it starts processing?\n",
      "\n",
      "1682602905.366859\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1682602905.366859]\n",
      "Virginia Scarlett said: Good morning all! Hope to see you in Photon at 1pm for the How To... interest group. Mark Kittisopikul will present \"How to... embed an Outlook calendar in a Confluence wiki page\". See Hughes Hub for Zoom info and more: https://hhmionline.sharepoint.com/Pages/Calendar/ScheduledEvent.aspx?EventId=104364\n",
      "Others reacted to the previous message with calendar a total of 1 times.\n",
      "\n",
      "1683113400.267969\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683113400.267969]\n",
      "Mark Kittisopikul said: What is Omega?\n",
      "Omega is a LLM-based and tool-armed autonomous agent that demonstrates the potential for Large Language Models (LLMs) to be applied to image processing, analysis and visualisation. Can LLM-based agents write image processing code and napari widgets, correct its coding mistakes, perform follow-up analysis, and control the napari viewer? The answer appears to be yes.https://twitter.com/loicaroyer/status/1653600252807757824\n",
      "Loïc A. Royer :computer::microscope::alembic: :flag-ua: on Twitter\n",
      ":rotating_light: #ChatGPT + @napari_imaging :rotating_light:\n",
      "Releasing my latest weekend project: Omega, an autonomous LLM agent that writes image processing and analysis code, fixes its mistakes, accesses the napari viewer, makes widgets, &amp; more!\n",
      "<https://t.co/5btZq05ro1>\n",
      "@LangChainAI @OpenAI \n",
      "#OmegaAgent\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Philip Hubbard said: \"Do not use this software lightly, it will download libaries by its own volition, write any code that it deems nescessary, it might actually do what you ask, even if it is a bad idea. Also, beware that it might misundertand what you ask and then do something bad. For example, it is unwise to use Omega to delete 'some' files from your system, it might end up deleteing more than that if you are unclear in your request. To be 100% safe, we recommend that you use this software from within a sandboxed virtual machine.\"\n",
      "Philip Hubbard said: It sounds like using it heavily might be risky, too!\n",
      "Mark Kittisopikul said: Perhaps we should screen WarGames (1983)\n",
      "https://m.imdb.com/title/tt0086567/\n",
      "WarGames (1983) - IMDb\n",
      "WarGames: Directed by John Badham. With Matthew Broderick, Dabney Coleman, John Wood, Ally Sheedy. A young man finds a back door into a military central computer in which reality is confused with game-playing, possibly starting World War III.\n",
      "Larissa Heinrich said: The Alamo is already on it https://drafthouse.com/northern-virginia/show/wargames\n",
      "WarGames | Alamo Drafthouse Cinema\n",
      "A potent and thoroughly ‘80s Cold War thriller\n",
      "Mark Kittisopikul said: Wonderful. Who wants to go see it?\n",
      "\n",
      "1683205862.707639\n",
      "1683205993.699479\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683205862.707639]\n",
      "Mark Kittisopikul said: On Tuesday May 9th at 10 am, the EMBL-Janelia Bioimaging Series will be hosting the following talk by Daniele Ancora.\n",
      " has been rescheduled for 1 pm that day for Loic Royer's ChatGPT Napari plugin.\n",
      "Title: Autocorrelation for image fusion of multiply aberrated detections\n",
      "Abstract: Cross-correlation represents a commonly used mathematical tool to estimate statistical properties in images, ranging from studying spatial distributions or temporal fluctuations of objects to estimation (and correction) of misalignments. In this seminar, we will briefly cover the special case of autocorrelation, which we employ to fuse multiple views of the same object subject to different point spread functions. Our idea stems from the fact that autocorrelation could be helpful for implicit realignment in optical computed tomography but can also enable access to reconstructions having a resolution higher than those obtained in real space. We discuss the logic behind the resolution improvement, validating the process experimentally using multi-view light-sheet imaging and SPAD array detection. With this work, we try to set a novel framework for image reconstruction that could be generically applied in any multi-view detection scheme, highlighting future research directions and current limitations of the method.\n",
      "\n",
      "Mark Kittisopikul said: Virtual Info:\n",
      "https://embl-org.zoom.us/j/96306653238?pwd=MEJ1V0laemtWMW5KMDkvcjJWWTJwZz09\n",
      "Meeting ID: 963 0665 3238\n",
      "Passcode: 008788\n",
      "Philip Hubbard said: Here is the Zoom link for Loic Royer's talk on Tuesday, May 9, 1 PM, on the ChatGPT Napari plugin: https://hhmi.zoom.us/j/6829491045?pwd=cEFEZkRQdzRieGIySWtrTWhqRGVTQT09\n",
      "\n",
      "1683212645.692399\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683212645.692399]\n",
      "John Bogovic said: sharepoint dead for anyone else?\n",
      "<Screenshot from 2023-05-04 10-54-38.png>\n",
      "Others reacted to the previous message with + a total of 2 times.\n",
      "Dominique Harajchi said: Yes. IT and Business Solutions are aware and working on it. \n",
      "Others reacted to the previous message with pray a total of 1 times, and with +1 a total of 1 times, and with pray::skin-tone-3 a total of 1 times.\n",
      "\n",
      "1683656357.556339\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683656357.556339]\n",
      "Mark Kittisopikul said: If it's useful at all, I know https://www.linkedin.com/in/logankilpatrick/ who works on developer relations for OpenAI. I asked him to get Loic GPT 4 access.\n",
      "\n",
      "1683728053.268489\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1683728053.268489]\n",
      "Stephan Preibisch said: Hi, I just uploaded the recording of yesterday's code-review session on Napari-ChatGPT by Loic Royer to YouTube: https://www.youtube.com/watch?v=JMo6Sn-L_j4\n",
      "Code review of Napari-ChatGPT by Loic Royer (CZI Biohub)\n",
      "Others reacted to the previous message with pray::skin-tone-3 a total of 1 times.\n",
      "\n",
      "1684244275.308909\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684244275.308909]\n",
      "Davis Bennett said: besides myself, how many people use https://en.wikipedia.org/wiki/Tmux?\n",
      "Tmux\n",
      "tmux is an open-source terminal multiplexer for Unix-like operating systems. It allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time. It can also be used to detach processes from their controlling terminals, allowing remote sessions to remain active without being visible.\n",
      "Others reacted to the previous message with + a total of 4 times.\n",
      "Jody Clements said: I still use screen, too much muscle memory to switch over\n",
      "Mark Kittisopikul said: I generally go to screen first\n",
      "John Bogovic said: I use screen for the same reason as Jody Clements\n",
      "Mark Kittisopikul said: The main reason I need a split terminal is usually to look at two editor buffers, and for that I usually use within vim.\n",
      "Others reacted to the previous message with vim a total of 3 times.\n",
      "Konrad Rokicki said: I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.\n",
      "Others reacted to the previous message with exploding_head a total of 1 times, and with octopus a total of 1 times.\n",
      "Kristin Branson said: thanks, seems like a nice alternative to screen, will try it out!\n",
      "Stuart Berg said: +1 for iTerm2, though it sounds like I'd need to up my game to match Konrad.\n",
      "I like it for its \"triggers\" capability, which lets you type something into your remote session that triggers a command on your local MacBook.\n",
      "Adam Taylor said: I have used screen/tmux in the past, but don't use it regularly.  I prefer to use X11 forwarding or NoMachine.\n",
      "Others reacted to the previous message with joy a total of 1 times.\n",
      "Ken Carlile said: I've used all 3. tmux occasionally when I remember, screen more often, and iTerm2 exclusively for when I'm working on nearline or the backup qumulo. i do batches of 20 nodes at once\n",
      "Ken Carlile said: Even at 4K, my screen isn't big enough to make it feasible to do more at a time\n",
      "Davis Bennett said:  I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.Konrad Rokicki can we get a demo of this?\n",
      "Jody Clements said: I used to use csshX to input commands into multiple terminals at the same time, but iTerm2 seems like a better solution now.\n",
      "Ken Carlile said: just make sure the response you get on all of the terminals is the same...\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1684254047.775209\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684254047.775209]\n",
      "Konrad Rokicki said: Just so everyone is aware, there was a hardware failure on NRS and it’s currently down. Systems is working with the vendor to bring it back up.\n",
      "Others reacted to the previous message with +1 a total of 5 times, and with +1::skin-tone-3 a total of 1 times.\n",
      "Konrad Rokicki said: NRS is back up. Thanks to Systems!\n",
      "Others reacted to the previous message with floppy_disk a total of 3 times.\n",
      "\n",
      "1684276589.246029\n",
      "--------------------------------------------------\n",
      "Document[channel=C0128K68NE5,ts=1684276589.246029]\n",
      "Boaz Mohar said: Anyone having issues with VPN? I get this weird issue from my Mac:\n",
      "SAML Transfer failed. Please contact your system administrator.\n",
      "Detail: FAILURE: No valid assertion found in SAML response\n",
      "\n",
      "1681830599.100389\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1681830599.100389]\n",
      "Mark Kittisopikul said: Michael Innerberger this may be of interest:\n",
      "https://forum.hdfgroup.org/t/anndata-python-package-and-hdf5-file-content-interoperability-aleksandar-jelenak-on-call-the-doctor-4-18-23/11041\n",
      "Also cc: Virginia Scarlett\n",
      "Anndata Python package and HDF5 file content interoperability - Aleksandar Jelenak on Call the Doctor 4/18/23\n",
      "Anndata Python package and HDF5 file content interoperability - Aleksandar Jelenak on Call the Doctor 4/18/23 Aleksandar Jelenak (@ajelenak) will host Call the Doctor on Tuesday 4/18. Aleksander will talk about the anndata Python package in the context of HDF5 file content interoperability. To join, just jump on the zoom: <https://us06web.zoom.us/s/98286880081> April 18, 2023 12:20 p.m. central time US/Canada\n",
      "Michael Innerberger said: Thanks, that does seem interesting.\n",
      "Mark Kittisopikul said: We really should invite Jelenak to Janelia. He lives in Virginia.\n",
      "Mark Kittisopikul said: 1:20 pm local time\n",
      "Michael Innerberger said: Hm, I might have another meeting at this time. We'll see..\n",
      "Mark Kittisopikul said: It's usually recorded\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Davis Bennett said: every time anndata comes up I think of their weird naming scheme\n",
      "Davis Bennett said: https://raw.githubusercontent.com/scverse/anndata/main/docs/_static/img/anndata_schema.svg\n",
      "Davis Bennett said: obs, obsp, obsm \n",
      "Davis Bennett said: uns \n",
      "Michael Innerberger said: If you accept their weird data-modeling, the names make at least some sense: m = multidimensional, p = pairwise, uns = unsorted. However, I don't think you need a fine-grained distinction of 1D annotations, nD annotations and pairwise relationships. All of them are just annotations (=dataset that enhances another one). \n",
      "Davis Bennett said: the need to designate a variable as \"pairwise\" represents a relational-database-shaped hole in this setup\n",
      "Others reacted to the previous message with laughing a total of 1 times.\n",
      "Davis Bennett said: but i guess that critique approaches \"not accepting their weird data modeling\"\n",
      "Michael Innerberger said: Yep. The best thing is, they even have a spec for String-scalars and numerical scalars: 2 metadata fields (spec name and version) and 1 dataset (with associated metadata such as compression, block size, ...) for storing 1 String / double. \n",
      "If you wanna see me ranting about AnnData (at least a bit), you should join next week's CompMeth group meeting. \n",
      "William Katz said: Michael Innerberger Is your talk about AnnData or a broader topic? Was just looking at their arxiv paper.\n",
      "Davis Bennett said:  Yep. The best thing is, they even have a spec for String-scalars and numerical scalars: 2 metadata fields (spec name and version) and 1 dataset (with associated metadata such as compression, block size, ...) for storing 1 String / double.\n",
      "Michael Innerberger said: William Katz I plan to give an overview over packages for spatial transcriptomics in Python, since we want to provide a java-based one. AnnData will probably make up a good portion of my talk since it is the base of most Python packages in that context.\n",
      "William Katz said: When is your talk? I think I have the old Zoom link when I attended Gary's talk, who I think is in your group.\n",
      "Michael Innerberger said: Should be Thursday 4/27 at 2pm.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1682023482.240679\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1682023482.240679]\n",
      "Davis Bennett said: a fresh idea for gpu-based chunk decompression in zarr: https://github.com/zarr-developers/zarr-python/issues/1398\n",
      "#1398 Allow batched/concurrent (de)compression support\n",
      "I have a proposal for an enhancement and would like to get feedback on this and potentially better ways of achieving the same goal.\n",
      "In many use cases that are read I/O bandwidth bound like in dataloaders for AI training, compression is typically turned off because that would end up being the bottleneck. We can get upwards of 10 GB/s of read throughput either from an object store or from a distributed filesystem like lustre. But CPU decompression typically maxes out at ~1 GB/s and that is usually with coarse grained multithreaded parallelism. A nice solution to this problem would be to decompress data on the GPU, which we can do at > 50 GB/s. This would make decompression no longer be the bottleneck while speeding up all other parts of the pipeline: lower use of storage, lower data volume fetched from storage, higher effective cache capacity if caching locally and faster CPU-GPU transfers. The issue with any kind of parallel decompression is that the API needs to support batched or concurrent decompression rather than calling decompression on chunks serially in a loop.\n",
      "Here is some data comparing throughput of some GPU decompression algorithms in nvcomp to multithreaded zstd on the CPU:  \n",
      "<https://user-images.githubusercontent.com/6964110/233476737-69eb0966-2031-49ca-9553-b2eb5dce95e3.png|decomp_throughput>\n",
      "This is conceptually similar to <https://github.com/zarr-developers/zarr-python/issues/547|#547> with concurrent chunk accesses, but for compression/decompression.\n",
      "The idea is to allow `Codec`s in numcodecs to implement a `batched_encode` and `batched_decode` in addition to the current `encode` and `decode` methods. When a codec has these methods available, zarr can dispatch a batch of chunks for encode/decode. The codec implementation can then either use a serial loop, multi-threaded parallelism or parallelize on the GPU using nvcomp. I'm envisioning this to be quite similar to `getitems` here:\n",
      "<https://github.com/zarr-developers/zarr-python/blob/2ff887548496855706c69a3c5983b00f17025af6/zarr/core.py#L2061|zarr-python/zarr/core.py>\n",
      "Line 2061 in </zarr-developers/zarr-python/commit/2ff887548496855706c69a3c5983b00f17025af6|2ff8875>\n",
      "From a GPU (de)compression standpoint, we have thought about using the sharding transformer format in ZEP0002 as the internal format in nvcomp. But after some consideration, this batched compression approach seems much more useful because:\n",
      "1. We can start using GPU decompression with existing zarr datasets that don't have the shard structure. The new sharding format would make GPU compression/decompression much more efficient but would not be required for functionality.\n",
      "2. We can support GPU decompression when accessing subset of chunks within a shard or from multiple shards, like when slicing along a dimension.\n",
      "3. We wouldn't have to add any new shard access api in zarr or multi-dimensional aware codec implementations.\n",
      "4. We don't need to have separate CPU and GPU codec implementations using the same underlying compression algorithm. For example, we can just have a single LZ4 codec that can use the GPU if available in batched mode or use the CPU multi-threaded path or just single threaded on the CPU. So data compressed with a GPU will be directly compatible with CPU decompression allowing users to not require a GPU to decompress a file.\n",
      "Would really appreciate any suggestions, concerns or other ideas that I might be missing. Also cc <https://github.com/jakirkham|@jakirkham> since he mentioned that there might be some intersection with Blosc.\n",
      "\n",
      "1682622122.169299\n",
      "1682622147.472779\n",
      "1682622161.390669\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1682622122.169299]\n",
      "Davis Bennett said: https://github.com/pgvector/pgvector\n",
      "pgvector/pgvector\n",
      "Open-source vector similarity search for Postgres\n",
      "Davis Bennett said: seems like this could be useful for doing spatial queries of point-based data (assuming the GIS stuff wasn't sufficient)\n",
      "Davis Bennett said: but it looks like ML people use this as well\n",
      "\n",
      "1683131120.266439\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1683131120.266439]\n",
      "Mark Kittisopikul said: I posted a comparison of the HDF5 fixed array data structures and the Zarr shard proposal here:\n",
      "https://github.com/zarr-developers/zarr-specs/pull/152#issuecomment-1533335795\n",
      "Comment on #152 Review of the ZEP2 spec - Sharding storage transformer\n",
      "It would be good to compare the chunk index to a HDF5 fixed array index, especially the data block.\n",
      "<https://docs.hdfgroup.org/hdf5/develop/_f_m_t3.html#FixedArray|https://docs.hdfgroup.org/hdf5/develop/_f_m_t3.html#FixedArray>\n",
      "<https://user-images.githubusercontent.com/8062771/235972578-e3a84d75-c9dc-4a65-b233-745675d38627.jpg|Screenshot_20230503_115726_Chrome.jpg>\n",
      "The fixed array data block consists of a 14 byte header, followed by data block elements, followed by a 4 byte checksum (Jenkin's lookup3).\n",
      "Each data block element consists of an address, chunk size, and filter mask. This is very similar to the chunk index here with an 8 byte address and an 8 nbytes field. There is an additional 4 byte filter mask field. The filter mask indicates whether filters (e.g. compression) have been applied or not.\n",
      "<https://user-images.githubusercontent.com/8062771/235973671-c0878699-2d81-4f8b-b6d5-66ed6ddd896d.jpg|Screenshot_20230503_115911_Chrome.jpg>\n",
      "The besides the header structures and the checksum, the main difference here is the additonal per chunk filter mask. Another potential difference is that the size field, nbytes here, can be smaller.\n",
      "The size of the size, nbytes, field is determined by a 28 header as the \"entry size\".\n",
      "<https://user-images.githubusercontent.com/8062771/235976850-fb9366b8-f6f8-4ea1-8778-f483846486bf.jpg|Screenshot_20230503_121412_Chrome.jpg>\n",
      "The Jenkin's lookup3 checksum is described here:  \n",
      "<http://www.burtleburtle.net/bob/hash/doobs.html|http://www.burtleburtle.net/bob/hash/doobs.html>\n",
      "In summary, the main difference between the proposed chunk index is a 4 byte filter mask per chunk. Another difference is that the size of the nbytes field can be smaller than 8 bytes. Additionally, there is the continuing theme of using checksums to ensure data integrity.\n",
      "\n",
      "1683914627.878489\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1683914627.878489]\n",
      "William Katz said: https://corecursive.com/data-compression-yann-collet/\n",
      "Comment on the interviewee from HN comment on parallel gzip:\n",
      "Yann was bored and working as a project manager. So he started working on a game for his old HP 48 graphing calculator.\n",
      "Eventually, this hobby led him to revolutionize the field of data compression, releasing LZ4, ZStandard, and Finite State Entropy coders.\n",
      "From Project Management to Data Compression Innovator - CoRecursive Podcast\n",
      "How do you accomplish something massive over time? I've had the chance to meet with a number of exceptional software developers and it's something I always wonder about. Today, I might have an answer with the incredible story of Yann Collet.Yann was a project manager who went from being burnt out on corporate life to becoming one of the most... […]\n",
      "\n",
      "1683920884.121699\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1683920884.121699]\n",
      "Virginia Scarlett said: Is the main challenge being able to interactively annotate ROIs in images? It seems like everything else could be handled by a spatial database\n",
      "Davis Bennett said: i think spatial databases are not in the toolbox yet for scientists in this domain\n",
      "Mark Kittisopikul said: Michael Innerberger, have you seen https://www.biorxiv.org/content/10.1101/2023.05.05.539647v1 ?\n",
      "SpatialData: an open and universal data framework for spatial omics\n",
      "Spatially resolved omics technologies are transforming our understanding of biological tissues. However, handling uni- and multi-modal spatial omics datasets remains a challenge owing to large volumes of data, heterogeneous data types and the lack of unified spatially-aware data structures. Here, we introduce SpatialData, a framework that establishes a unified and extensible multi-platform file-format, lazy representation of larger-than-memory data, transformations, and alignment to common coordinate systems. SpatialData facilitates spatial annotations and cross-modal aggregation and analysis, the utility of which is illustrated via multiple vignettes, including integrative analysis on a multi-modal Xenium and Visium breast cancer study.\n",
      "### Competing Interest Statement\n",
      "J.M. holds equity in Glencoe Software which builds products based on OME-NGFF. F.J.T. consults for Immunai Inc., Singularity Bio B.V., CytoReason Ltd, Cellarity, and Omniscope Ltd, and has ownership interest in Dermagnostix GmbH and Cellarity.\n",
      "Mark Kittisopikul said: Also see Twitter thread from Luca Marconato:\n",
      "https://twitter.com/LucaMarconato2/status/1656239450131660800\n",
      "Luca Marconato on Twitter\n",
      "1/9 Excited to share #SpatiaData, a framework for representing spatially-resolved omics technologies. Great colab w/ @g_palla1 @ky396 @ivirshup @HeidariElyas @TreisTim Marcella @rahulbshrestha @harald_voeh @wolfgangkhuber @MoritzGerstung @notjustmoore @fabian_theis @OliverStegle\n",
      "Michael Innerberger said: Yes, I already saw this, but thanks for sharing!\n",
      "Davis Bennett said: \"SpatialData\" is kind of an unfortunate name IMO\n",
      "Davis Bennett said: at least they are using parquet for storing points\n",
      "Davis Bennett said: ....but this undermines the argument for cramming tables in zarr arrays\n",
      "Michael Innerberger said: Maybe that's the reaction to the resistance they were facing with that proposal.\n",
      "Davis Bennett said: i bet it's simply that storing millions of points in a zarr array is borderline unworkable\n",
      "Davis Bennett said: unless you are happy doing all your indexing in memory, in which case zarr gives you nothing\n",
      "Davis Bennett said: but it's not really clear what this \"framework\" is, if it's conventions for multiple file formats\n",
      "Michael Innerberger said: Yeah, I don't quite get it yet, either. I hope I'll find out when reading the paper.\n",
      "Davis Bennett said: i guess the optimistic upshot is that this is clearly a growing area and there's opportunity for software developers to make rapid progress in the tooling\n",
      "\n",
      "1684240489.153149\n",
      "--------------------------------------------------\n",
      "Document[channel=C013E4ULBFU,ts=1684240489.153149]\n",
      "Mark Kittisopikul said: Aleksandar Jelenak (https://forum.hdfgroup.org/u/ajelenak) will host Call the Doctor on Tuesday, May 16 and will will present a proposal for HDF5 columnar table data convention. This is continuation of what Aleksandar talked about during his last Call the Doctor session.\n",
      "https://us06web.zoom.us/j/98286880081\n",
      "May 16, 2023 1:20 p.m. eastern time US/Canada\n",
      "Mark Kittisopikul said: https://forum.hdfgroup.org/t/a-proposal-for-hdf5-columnar-table-data-convention-aleksandar-jelenak-on-call-the-doctor-tuesday-may-16/11118\n",
      "A proposal for HDF5 columnar table data convention - Aleksandar Jelenak on Call the Doctor Tuesday May 16\n",
      "A proposal for HDF5 columnar table data convention - Aleksandar Jelenak on Call the Doctor Tuesday May 16 Aleksandar Jelenak (@ajelenak) will host Call the Doctor on Tuesday, May 16 and will will present a proposal for HDF5 columnar table data convention. This is continuation of what Aleksandar talked about during his last Call the Doctor session. Last sessions recording can be seen here: This was a really popular topic and a good discussion, so we look forward to having everyone join us aga...\n",
      "\n",
      "1683219334.279909\n",
      "--------------------------------------------------\n",
      "Document[channel=C013EB3CZM1,ts=1683219334.279909]\n",
      "Tobias Pietzsch said: https://cohost.org/mcc/post/1406157-i-want-to-talk-about-webgpu\n",
      "I want to talk about WebGPU\n",
      "Tobias Pietzsch said: I found that well-written on the topic (historical context, how it relates to Vulkan, etc)\n",
      "Philip Hubbard said: WebGPU is now available in the release version (113) of Chrome.\n",
      "Others reacted to the previous message with +1 a total of 4 times.\n",
      "William Katz said: I was surprised when https://github.com/AmesingFlank/taichi.js https://taichi-js.com/playground (requires WebGPU) worked for me when I opened it up recently.  Here's the article for the WebGPU release:\n",
      "https://developer.chrome.com/blog/webgpu-release/\n",
      "Playground | taichi.js\n",
      "PlayGround\n",
      "Chrome ships WebGPU - Chrome Developers\n",
      "After years of development, the Chrome team ships WebGPU which allows high-performance 3D graphics and data-parallel computation on the web.\n",
      "Philip Hubbard said: It is a big moment, but... https://mastodon.gamedev.place/@aras/110151390138920647\n",
      "Aras Pranckevičius (@aras@mastodon.gamedev.place)\n",
      "Oh cool, WebGPU is about to ship in Chrome (113) without any feature flags soon. <https://developer.chrome.com/blog/webgpu-release/>\n",
      "WebGL was getting really old by now. I do wonder whether WebGPU is a bit late too though (e.g. right now Vulkan decides that PSOs maybe are not a great idea lol)\n",
      "Philip Hubbard said: https://mastodon.gamedev.place/@aras/110151397066811200\n",
      "Aras Pranckevičius (@aras@mastodon.gamedev.place)\n",
      "As in, WebGPU is very much a \"modern graphics API design\" as it was 8 years ago by now. Better late than never, but... What's \"modern\" now seems to be moving towards like: bindless everything (like 3rd iteration of what \"bindless\" means), mesh shaders, raytracing, flexible pipeline state. All of which are not in WebGPU.\n",
      "\n",
      "1683415001.876329\n",
      "--------------------------------------------------\n",
      "Document[channel=C013EB3CZM1,ts=1683415001.876329]\n",
      "Philip Hubbard said: https://mastodon.gamedev.place/@BartWronski/110323818939649986\n",
      "Bart Wronski :flag-ua: (@BartWronski@mastodon.gamedev.place)\n",
      "This is great work, beats NeRFs on all axes <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>\n",
      "IMO global representations, such as full NeRF coordinate networks without local or hash grids are somewhat silly and will never be the destination (but are a cool local research detour!) for performance and cache reasons.\n",
      "Similarly, blind ray marching. \n",
      "Local 2/2.5/3D representations were the past, are the present, and will be the future.\n",
      "\n",
      "1682609067.999479\n",
      "--------------------------------------------------\n",
      "Document[channel=C01430CRBHT,ts=1682609067.999479]\n",
      "Donald Olbris said: Hello everyone...\n",
      "As you may be aware, GitHub will be requiring two-factor authentication (2FA) on all accounts by the end of 2023. Each account will be given a deadline at some point during the year. Mine is next week. I procrastinated a long time before finally setting it up. I figured other people might be in a similar position, so I wrote up my experience as a case study on the wiki. It's not a comprehensive guide to all the ways you can set up 2FA on GitHub or all the details thereof, but it's the way I chose, and it serves as a general guide on what to expect. This is a first draft; please let me know if things aren't clear or you have questions.\n",
      "Setting up 2FA on GitHub: https://wikis.janelia.org/display/SCSW/setting+up+2FA+on+GitHub%3A+a+case+study\n",
      "Others reacted to the previous message with clap a total of 3 times.\n",
      "Michael Innerberger said: While you claim it's not comprehensive, it is still very detailed. I didn't receive any notice so far, but I'm sure I will also procrastinate this to the latest possible point. So, this will be really helpful, thanks!\n",
      "I also feel like this would be an ideal topic to present in a How To meeting. \n",
      "William Katz said: I added a note to your \"which app\" section that I've found Google Authenticator to be very painless and https://security.googleblog.com/2023/04/google-authenticator-now-supports.html.\n",
      "Google Authenticator now supports Google Account synchronization\n",
      "Christiaan Brand, Group Product Manager We are excited to announce an update to Google Authenticator , across both iOS and Android, which a...\n",
      "William Katz said: Also something I found out the hard way -- if your GitHub Mobile is turned off for notifications, it will not work for the 2FA code check. So make sure notifications are on for that app if you plan on using it for 2FA. Or at least that was the situation a couple months ago.\n",
      "Adam Taylor said: This is a super-helpful article, Don.  Personally, I'm a big fan of 1Password (https://1password.com/) for all things password-related.  I'm honestly not sure how I would manage without it.  And it supports TOTPs.\n",
      "1Password - Password Manager for Families, Businesses, Teams | 1Password\n",
      "A password manager, digital vault, form filler and secure digital wallet. 1Password remembers all your passwords for you to help keep account information safe.\n",
      "\n",
      "1681835837.105069\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1681835837.105069]\n",
      "William Katz said: I was wondering the ways Microsoft would use Github acquisition. Haven't explored Codespaces and wonder if this might be a topic to investigate. In general, though, I'm not planning on learning C# since it's down in my language TODOs.\n",
      "https://devblogs.microsoft.com/dotnet/codespaces-template-blazor-portfolio-website\n",
      "William Katz said: Seems like Codespaces is Microsoft's answer to http://repl.it. The Blazor part is C#/.NET to build full stack web apps with components running in WebAssembly, server-side, or native client apps.\n",
      "\n",
      "1682367176.852519\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1682367176.852519]\n",
      "Jody Clements said: I am going to cancel this weeks meeting so that people can attend the science for everyone talk, which has been scheduled for the same time.\n",
      "Others reacted to the previous message with microscope a total of 1 times.\n",
      "\n",
      "1682619920.633469\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1682619920.633469]\n",
      "Jody Clements said: https://twitter.com/JasperRLZ/status/1648046875675856897?ck_subscriber_id=921192603\n",
      "Jasper (cohost.org/jasper) on Twitter\n",
      "There is a game called Mario vs. Donkey Kong: Tipping Stars for the Wii U. It is, inexplicably, written in HTML and JavaScript, with some custom APIs designed for the Wii U. But that means, with about 1400 lines of shim code, I can do this:\n",
      "Others reacted to the previous message with star a total of 1 times.\n",
      "\n",
      "1682970803.429379\n",
      "1682970820.677219\n",
      "1682970835.914889\n",
      "1682970974.201399\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1682970803.429379]\n",
      "Davis Bennett said: https://vercel.com/blog/vercel-storage#vercel-postgres-complex-data-made-easy\n",
      "Introducing storage on Vercel – Vercel\n",
      "New to the Vercel dashboard: Vercel KV, Vercel Postgres, and Vercel Blob\n",
      "Davis Bennett said: this code snippet is interesting --\n",
      "  const { rows } = await sql`\n",
      "    INSERT INTO products (name)\n",
      "    VALUES (${formData.get('name')})\n",
      "  `;\n",
      "\n",
      "Davis Bennett said: i've never seen prefixed-formatted strings like this before in js\n",
      "Davis Bennett said: looks like this uses \"tagged templates\":\n",
      "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates\n",
      "Template literals (Template strings) - JavaScript | MDN\n",
      "Template literals are literals delimited with backtick (`) characters, allowing for multi-line strings, string interpolation with embedded expressions, and special constructs called tagged templates.\n",
      "Jody Clements said: looks neat, but does add some indirection, although in this case (SQL statement) it seems useful\n",
      "Davis Bennett said: one of my big complaints about code that works with raw strings is the lack of type safety / IDE introspection available, but I think typescript mitigates a lot of this with template literal types\n",
      "Davis Bennett said: i wish we had stuff like this in python\n",
      "Jody Clements said: I see a wacky pull request in your future \n",
      "Davis Bennett said: haha\n",
      "\n",
      "1682977195.091289\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1682977195.091289]\n",
      "Jody Clements said: That would work and is not too hard to do. \n",
      "Stuart Berg said: If I understand correctly, my problem is that http://clio-ng.int.janelia.org refuses to load content from `emdata6.int.janelia.org:9000` .  I guess the `:9000` part makes it non-CORS-compatible?\n",
      "Stuart Berg said: Multiple sites seem to say that the way to disable it in Chrome is to launch Chrome with `--disable-web-security --user-data-dir`\n",
      "That doesn't sound great...\n",
      "Jody Clements said: Those are the flags and i would not recommend. \n",
      "Jody Clements said: Is there any reason that server on port 9000 is not fronted by the nginx server on emdata6?\n",
      "Stuart Berg said: Heh, my goal was specifically to compare behavior with and without Nginx. \n",
      "Stuart Berg said: Port 9000 is indeed fronted by Nginx. It’s our main production DVID server. \n",
      "Stuart Berg said: I want to understand how many requests neuroglancer sends in parallel using http2 vs http1 (or how many of those requests get passed on by nginx in parallel).\n",
      "Jody Clements said: In that case, i would suggest using postman or apachebench, which don’t care about CORS headers and will provide stats on the request times, etc.  \n",
      "Stuart Berg said: Attempting to script this use-case would be quite painful, sadly. \n",
      "Jody Clements said: Are you not just changing the x,y coordinates, maybe the layer or zoom level for a single dataset?\n",
      "Stuart Berg said: One question is whether neuroglancer issues 100 concurrent requests to DVID in both the http1 and http2 scenarios.  A related question is whether and how the requests are cancelled in each scenario when the user navigates to a new area before the original data chunks have been served.  I was hoping to just play around with two different neuroglancer windows and watch kibana in each case.\n",
      "It would be nice if DVID could just provide its own CORS headers, but unfortunately those would then override the CORS headers from nginx.  I don't recall exactly why that's a problem, but I remember that it has been a problem in the past.\n",
      "Jody Clements said: You need to have either nginx or dvid provide the headers, because nginx wont replace existing headers, but will add to them, which will result in duplicate headers. If the browser receives duplicate headers it will fail the request.\n",
      "Jody Clements said: As for the behavior of neuroglancer, I doubt it is doing anything different based on the HTTP protocol. I would think that is all handled by the browsers dispatch mechanisms.\n",
      "Stuart Berg said: I'm not so sure about that.   Http1 doesn't support concurrent requests over a single connection, nor does it support cancellation.  So my theory is that the number of concurrent requests is constrained by the number of web worker threads in neuroglancer.  I'm not sure, though... Maybe neuroglancer just opens 100 connections spread across 10 worker threads?\n",
      "And then there's the question of what Nginx actually DOES with the requests.  How does multiplexing work in the http1 case vs. the http2 case?  I know nginx is incapable of using http2 on the backend side... So when it's using http2 on the front-end side and http1 on the backend side, then I have the same question: Does it open 100 independent connections to DVID, or does it queue the requests internally?\n",
      "The answer to that question may have ramifications for which requests are subject to cancelation when neuroglancer decides it no longer needs a chunk that it had just requested moments ago.  Since DVID can't actually handle cancelation properly, then there's a chance that we might have shot ourselves in the foot by switching to http2.  With http2, neuroglancer has the ability to send a ton of requests... Does it now do so with new-found impunity, since it believes all of those requests can be safely cancelled if necessary?  Is sending 100 concurrent requests something it didn't do previously (whether it was by design or due to practical limitations)?\n",
      "Stuart Berg said: Anyway, I think the easiest way to get a handle on these questions would be to fool around with it for a while and see what I can observe.  I don't know precisely which tests to run, which is why scripting them in advance would be a bummer.\n",
      "Stuart Berg said: Or, a related question: How does one control the multiplexing behavior in nginx?  Diagrams https://www.nginx.com/blog/http2-module-nginx/#multiplexing show that nginx will open multiple http1 connections to the backend... But how many?  My (very brief) search didn't yield an explanation of the config settings to control that.\n",
      "Jody Clements said: I have always used the defaults and never looked into changing how nginx is sending requests to the proxied backend servers.\n",
      "Stuart Berg said: Is there ANY way to get Chrome (or Firefox) to temporarily ignore CORS violations?  Using a setting in the developer tools window, perhaps?\n",
      "Stuart Berg said: Or maybe is there something I could add in my `/etc/hosts` file as a workaround?  Maybe that's the answer...\n",
      "Jody Clements said: There is a flag you can set, but it will disable all CORS for all sites and it is not easy to turn off or on\n",
      "Jody Clements said: Modifying the hosts file will probably not work, I presume you are trying to load data from a different domain.\n",
      "Mark Kittisopikul said: Would setting up some kind of http proxy work?\n",
      "\n",
      "1683252131.667979\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1683252131.667979]\n",
      "Mark Kittisopikul said: This visualization of web browser market share is nice:\n",
      "https://www.visualcapitalist.com/cp/the-rise-and-fall-of-popular-web-browsers-since-1994/\n",
      "Others reacted to the previous message with +1 a total of 3 times.\n",
      "\n",
      "1683837180.870209\n",
      "--------------------------------------------------\n",
      "Document[channel=C0146BJ38PQ,ts=1683837180.870209]\n",
      "Jody Clements said: Here is a the python notebook from Wednesday, that we used to connect to a python API endpoint and plot the response times.\n",
      "<sam_service_enpoint_benchmark.ipynb>\n",
      "\n",
      "1681860351.400109\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1681860351.400109]\n",
      "Mark Kittisopikul said: https://bssw.io/blog_posts/julia-s-value-proposition-for-better-scientific-software\n",
      "Julia's Value Proposition for Better Scientific Software\n",
      "Julia provides a mathematical front end to LLVM to provide easy and performant CPU and GPU access and lightweight interoperability with existing C, Fortran, R, and Python codes, coupled with a rich unified ecosystem for packaging, data science, and interactive computing. Hence, Julia fills a gap at the intersection of high performance and high productivity for scientific software.\n",
      "\n",
      "1682078543.237569\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1682078543.237569]\n",
      "Mark Kittisopikul said: This is a fun wasm demo via Julia:\n",
      "https://alexander-barth.github.io/FluidSimDemo-WebAssembly/\n",
      "Alexander-Barth/FluidSimDemo-WebAssembly\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "\n",
      "1682127577.292019\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1682127577.292019]\n",
      "Mark Kittisopikul said: This is an nice restatement of the \"Two Language Problem\" as the \"Two Culture Problem\"\n",
      "https://scientificcoder.com/my-target-audience\n",
      "My Target Audience\n",
      "What kind of people do I have in mind while writing this blog? People who share my professional mission of course! What is that mission you ask? Let me elaborate.\n",
      "From Research to Engineering\n",
      "I am a scientist who danced with startups and moved into i...\n",
      "<image.png>\n",
      "<image.png>\n",
      "<image.png>\n",
      "\n",
      "1682445517.801179\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1682445517.801179]\n",
      "Mark Kittisopikul said: Some Julia syntax weirdness:\n",
      "julia> t = (3,4,5)\n",
      "(3, 4, 5)\n",
      "julia> t.1\n",
      "ERROR: syntax: extra token \"0.1\" after end of expression\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ none:1\n",
      "julia> t[1]\n",
      "3\n",
      "julia> t. 1\n",
      "3\n",
      "julia> t.:1\n",
      "3\n",
      "\n",
      "Davis Bennett said: that's pretty regrettable\n",
      "Mark Kittisopikul said: There's some consistency to it\n",
      "julia> named_tuple = (a=1, b=2, c=3)\n",
      "(a = 1, b = 2, c = 3)\n",
      "julia> named_tuple.a\n",
      "1\n",
      "julia> named_tuple.:a\n",
      "1\n",
      "julia> named_tuple. a\n",
      "1\n",
      "\n",
      "Mark Kittisopikul said: I think `t.1` is not valid due to some syntax disambiguation issue, possibly involving floating point. I'm not sure though.\n",
      "\n",
      "1682514004.745119\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1682514004.745119]\n",
      "Mark Kittisopikul said: Michael Innerberger this compressed SVD implementation may be of interest:\n",
      "https://discourse.julialang.org/t/compressed-svd-algorithm-implementation/97837\n",
      "Compressed svd algorithm implementation\n",
      "I am trying to implement the compressed SVD in this [<https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w25/Erichson_Compressed_Singular_Value_ICCV_2017_paper.pdf>]. Attached is the algorithm. I have problems implementing the algorithm as when I apply it to an image, all I get is just black image. It seems I am missing something any help. I’d appreciate any help. Here is the code I implemented in Julia. function compressed_SVD(X,k,p=10) m,n=size(X) l=k+p Phi=rand(l,...\n",
      "Michael Innerberger said: Thanks, that looks very interesting!\n",
      "\n",
      "1682623838.267509\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1682623838.267509]\n",
      "Ben Arthur said: how does that compare to other languages?\n",
      "Mark Kittisopikul said: Comparable to Python's asyncio performance when Python is  using a single thread and also using OpenSSL.\n",
      "Mark Kittisopikul said: https://discourse.julialang.org/t/http-jl-async-is-slow-compared-to-python-aiohttp/96736?u=mkitti\n",
      "HTTP.jl async is slow compared to python+aiohttp\n",
      "So @fabiangans and I have been having discussions about improving the performance of Zarr.jl. The goal is to improve the time it takes to load data from a remote zarr store (eg on HTTP, GCS, or S3). The current implementation for this is very fast in zarr-python because it uses async, so we have been trying to also start loading chunks from remote storage using async. However we have found that retrieving the chunks async via HTTP.jl is several times slower than through python which is using aio...\n",
      "Mark Kittisopikul said: There have been performance upgrades to Julia's HTTP.jl package, especially for running multiple concurrent connections via OpenSSL (as opposed to the default MbedTLS):\n",
      "https://github.com/JuliaWeb/HTTP.jl/pull/1034#issuecomment-1525624843\n",
      "This also impacts Julia's Zarr implementation: Zarr.jl\n",
      "Comment on #1034 Summary of changes in this PR:\n",
      "Ok, I've done a _bunch_ of additional benchmarking, comparing lots of different variables, but at a high-level summary, the net effect of this (+ other webstack PRs (OpenSSL, CloudBase, CloudStore, ConcurrentUtilities)) is:  \n",
      "<https://user-images.githubusercontent.com/2896623/234863945-c2c0e7ca-93ae-4594-84f6-199a7a272ebb.png|image>\n",
      "with more details broken out by specific cloud and file sizes:  \n",
      "<https://user-images.githubusercontent.com/2896623/234864073-70071179-badf-4332-9d77-09b3ade4e9cf.png|image>\n",
      "So all in all, I'm feeling confident in how all these changes have settled.\n",
      "<image.png>\n",
      "\n",
      "1683248433.102339\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1683248433.102339]\n",
      "Mark Kittisopikul said: Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl\n",
      " and \n",
      "https://arxiv.org/abs/2305.01582\n",
      "Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl\n",
      "PySR is an open-source library for practical symbolic regression, a type of\n",
      "machine learning which aims to discover human-interpretable symbolic models.\n",
      "PySR was developed to democratize and popularize symbolic regression for the\n",
      "sciences, and is built on a high-performance distributed back-end, a flexible\n",
      "search algorithm, and interfaces with several deep learning packages. PySR's\n",
      "internal search algorithm is a multi-population evolutionary algorithm, which\n",
      "consists of a unique evolve-simplify-optimize loop, designed for optimization\n",
      "of unknown scalar constants in newly-discovered empirical expressions. PySR's\n",
      "backend is the extremely optimized Julia library SymbolicRegression.jl, which\n",
      "can be used directly from Julia. It is capable of fusing user-defined operators\n",
      "into SIMD kernels at runtime, performing automatic differentiation, and\n",
      "distributing populations of expressions to thousands of cores across a cluster.\n",
      "In describing this software, we also introduce a new benchmark,\n",
      "\"EmpiricalBench,\" to quantify the applicability of symbolic regression\n",
      "algorithms in science. This benchmark measures recovery of historical empirical\n",
      "equations from original and synthetic datasets.\n",
      "Mark Kittisopikul said: Tweetorial: https://twitter.com/MilesCranmer/status/1654169022852894721\n",
      "Miles Cranmer on Twitter\n",
      "Three years ago, I started working on an easy-to-use tool for interpretable machine learning in science. I wanted it to do for symbolic regression what Theano did for deep learning.\n",
      "Today, I am beyond excited to share with you the paper describing it!\n",
      "<https://t.co/be2cVdukPy>\n",
      "1.\n",
      "\n",
      "1683491648.811839\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1683491648.811839]\n",
      "Mark Kittisopikul said: This is an example app to demonstrate how Julia code for DiffEq-type simulations can be compiled for use on the web. This app is built with the following:\n",
      "https://tshort.github.io/Lorenz-WebAssembly-Model.jl/\n",
      "\n",
      "1683707065.241899\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1683707065.241899]\n",
      "Mark Kittisopikul said: Julia 1.9 is out. Native code is now cached at the package level in shared libraries (.dll, .so, .dylib) files.\n",
      "An interactive thread pool is introduced for smoother UIs.\n",
      "See the blog post for more details.\n",
      "https://julialang.org/blog/2023/04/julia-1.9-highlights/\n",
      "Julia 1.9 Highlights\n",
      "Highlights of the Julia 1.9 release.\n",
      "Others reacted to the previous message with tada a total of 2 times.\n",
      "\n",
      "1683707928.048779\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1683707928.048779]\n",
      "Mark Kittisopikul said: One effect is a significantly reduced time to first execution (TTFX).\n",
      "<image.png>\n",
      "Michael Innerberger said: What is the ODE package doing to have 150MB cache?  Solving ODEs is usually a rather easy task and most solvers can be implemented in ~10lines if you have a suitable LinearAlgebra package.\n",
      "Davis Bennett said: maybe lots of different optimizers * lots of different numerical types * lots of different compute backends?\n",
      "Michael Innerberger said: Could be. Also, I don't really know what the scope of this package is. ODE is a very broad term, so this package might just be huge. But given its realtively quick startup time, I'm curious what they cache (seems to be the case since 1.7).\n",
      "Mark Kittisopikul said: https://github.com/SciML/OrdinaryDiffEq.jl\n",
      "SciML/OrdinaryDiffEq.jl\n",
      "High performance ordinary differential equation (ODE) and differential-algebraic equation (DAE) solvers, including neural ordinary differential equations (neural ODEs) and scientific machine learning (SciML)\n",
      "\n",
      "1683920297.417709\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1683920297.417709]\n",
      "Mark Kittisopikul said: \n",
      "Extreme Multi-Threading: C++ and Julia 1.9 Integration [and some Java]\n",
      "<https://scientificcoder.com/extreme-multi-threading-c-and-julia-19-integration>\n",
      "> In this tutorial we demonstrate how to call Julia libraries with multiple threads from C++. With the introduction of Julia 1.9 in May 2023, the runtime can dynamically \"adopt\" external threads, enabling the integration of Julia libraries into multi-threaded codebases written in other languages, such as C++. This article is written in collaboration with <https://www.linkedin.com/in/evangelos-paradas/|Evangelos Paradas>, the maestro of algorithm deployment at ASML.\n",
      "...\n",
      "> We encountered a pitfall with Java, when embedding our library into Spark. In this article, we will not go into the details of passing Java threads (via C++) to Julia, but we noticed some issues with the Java signal handler. Make sure that your library is explicitly aware of the Java signal handling library, for example via `export LD_PRELOAD=/path/to/libjsig.so` . Otherwise Julia will produce a segmentation fault and your application will crash. This is some kind of language interoperability issue that we had to circumvent.\n",
      "\n",
      "1684239698.857859\n",
      "1684239736.872789\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1684239698.857859]\n",
      "Mark Kittisopikul said: https://www.youtube.com/watch?v=S-uLTsE2wZg\n",
      "Miner.jl Weekend Updates(09-04-2023)\n",
      "Mark Kittisopikul said: Someone is building a minecraft like game in Julia via https://docs.makie.org/stable/documentation/backends/glmakie/.\n",
      "https://github.com/ashwani-rathee/Miner.jl\n",
      "ashwani-rathee/Miner.jl\n",
      "Written in Julia, Makie\n",
      "\n",
      "1684294381.129019\n",
      "--------------------------------------------------\n",
      "Document[channel=C015MJGSM2S,ts=1684294381.129019]\n",
      "Mark Kittisopikul said: Reduction in Julia package load times. Time to load (TTL) and time to first execution (TTFX) of Julia 1.7 (left) versus Julia 1.9 (right).\n",
      "TTL (orange, below zero) is the time to `import` a package. This includes loading inferred or native code and initialization.\n",
      "TTFX (blue, above zero) is the time to execute the first function, such as `plot`.\n",
      "The speed up here comes from compiling more code to native code ahead of time and storing it to disk. This may slightly increase TTL because there is a larger shared library to load and validate. However it should significantly reduce TTFX since no compilation needs to be done then.\n",
      "<image-2.png>\n",
      "\n",
      "1681932730.115649\n",
      "1681932766.498529\n",
      "--------------------------------------------------\n",
      "Document[channel=C01H5PYR4TW,ts=1681932730.115649]\n",
      "Davis Bennett said: dumb ssh question: I'm getting the following error when I try to connect to login2:\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\n",
      "Someone could be eavesdropping on you right now (man-in-the-middle attack)!\n",
      "It is also possible that a host key has just been changed.\n",
      "The fingerprint for the RSA key sent by the remote host is\n",
      "SHA256:mrmgHYhCMxvj3VQRTNxxw7Jf0UYY8vx++YkBaSK4308.\n",
      "Please contact your system administrator.\n",
      "Add correct host key in /Users/bennettd/.ssh/known_hosts to get rid of this message.\n",
      "Offending RSA key in /Users/bennettd/.ssh/known_hosts:16\n",
      "Host key for login2.int.janelia.org has changed and you have requested strict checking.\n",
      "Host key verification failed.\n",
      "the error message recommends that I add the correct host key to `known_hosts`, but what is the correct host key? is it that SHA256 fingerprint?\n",
      "Davis Bennett said: or should i just delete the old references to login2 from my ssh config\n",
      "Donald Olbris said: Delete the offending line and let the system recreate it is the easiest, when you trust the host.\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "Davis Bennett said: thanks!\n",
      "Konrad Rokicki said: This is so annoying, especially with all the changes to the machines here and at GitHub recently. Does anyone know a way to get it done faster, without opening the file and entering the line number manually?\n",
      "Adam Taylor said: If you can live without host checking, you can add\n",
      "Host *\n",
      "   StrictHostKeyChecking no\n",
      "   UserKnownHostsFile=/dev/null\n",
      "to your `~/.ssh/config` file...\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Gary Huang said: Konrad Rokicki I believe `ssh-keygen -R [hostname]` will remove the corresponding hostname entry from the known_hosts file\n",
      "Others reacted to the previous message with +1 a total of 3 times, and with +1::skin-tone-2 a total of 1 times.\n",
      "\n",
      "1682002061.476779\n",
      "--------------------------------------------------\n",
      "Document[channel=C01J3KE45LG,ts=1682002061.476779]\n",
      "Mark Kittisopikul said: https://blog.rust-lang.org/2023/04/20/Rust-1.69.0.html\n",
      "Announcing Rust 1.69.0 | Rust Blog\n",
      "Empowering everyone to build reliable and efficient software.\n",
      "Davis Bennett said: is there anything of note in this release?\n",
      "\n",
      "1683306133.827609\n",
      "--------------------------------------------------\n",
      "Document[channel=C01J3KE45LG,ts=1683306133.827609]\n",
      "Mark Kittisopikul said: Vua HN\n",
      "https://github.com/juspay/hyperswitch/wiki/Ask-not-what-the-compiler-can-do-for-you\n",
      "\n",
      "1683315304.232669\n",
      "--------------------------------------------------\n",
      "Document[channel=C01J3KE45LG,ts=1683315304.232669]\n",
      "William Katz said: tl;dr: Rust is good at some use cases but not everything, particularly when taking into account app requirements and context (fast dev vs. as-safe-as-possible code result):\n",
      "https://mdwdotla.medium.com/using-rust-at-a-startup-a-cautionary-tale-42ab823d9454\n",
      "Using Rust at a startup: A cautionary tale\n",
      "Rust is awesome, for certain things. But think twice before picking it up for a startup that needs to move fast.\n",
      "William Katz said: I think some of the comments were interesting but doesn't override one of the messages: if your app needs are simple so safety / memory usage is brain-dead simple, maybe Rust isn't the best choice.\n",
      "Ben Arthur said: i used rust for a https://github.com/JaneliaSciComp/aitch and the borrow checker is a PITA.   experienced rust developers can code quicker i bet, but boy is it painful for beginners (at least it was for me).\n",
      "JaneliaSciComp/aitch\n",
      "local scheduler for heterogenous jobs\n",
      "\n",
      "1683920316.405589\n",
      "--------------------------------------------------\n",
      "Document[channel=C02CTFPCTDM,ts=1683920316.405589]\n",
      "Mark Kittisopikul said: \n",
      "Extreme Multi-Threading: C++ and Julia 1.9 Integration [and some Java]\n",
      "<https://scientificcoder.com/extreme-multi-threading-c-and-julia-19-integration>\n",
      "> In this tutorial we demonstrate how to call Julia libraries with multiple threads from C++. With the introduction of Julia 1.9 in May 2023, the runtime can dynamically \"adopt\" external threads, enabling the integration of Julia libraries into multi-threaded codebases written in other languages, such as C++. This article is written in collaboration with <https://www.linkedin.com/in/evangelos-paradas/|Evangelos Paradas>, the maestro of algorithm deployment at ASML.\n",
      "...\n",
      "> We encountered a pitfall with Java, when embedding our library into Spark. In this article, we will not go into the details of passing Java threads (via C++) to Julia, but we noticed some issues with the Java signal handler. Make sure that your library is explicitly aware of the Java signal handling library, for example via `export LD_PRELOAD=/path/to/libjsig.so` . Otherwise Julia will produce a segmentation fault and your application will crash. This is some kind of language interoperability issue that we had to circumvent.\n",
      "\n",
      "1684183401.246879\n",
      "--------------------------------------------------\n",
      "Document[channel=C02CTFPCTDM,ts=1684183401.246879]\n",
      "Mark Kittisopikul said: Does anyone know of a platform where this fails to compile?\n",
      "#include<assert.h>\n",
      "#include<float.h>\n",
      "int main(void) {\n",
      "    static_assert(sizeof(long double) ==                         16, \"\");\n",
      "    static_assert(LDBL_MANT_DIG   ==                             64, \"\");\n",
      "    static_assert(LDBL_DIG        ==                             18, \"\");\n",
      "    static_assert(LDBL_MIN_EXP    ==                         -16381, \"\");\n",
      "    static_assert(LDBL_MIN_10_EXP ==                          -4931, \"\");\n",
      "    static_assert(LDBL_MAX_EXP    ==                          16384, \"\");\n",
      "    static_assert(LDBL_MAX_10_EXP ==                           4932, \"\");\n",
      "    static_assert(LDBL_MAX        ==  1.18973149535723176502E+4932L, \"\");\n",
      "    static_assert(LDBL_MIN        ==  3.36210314311209350626E-4932L, \"\");\n",
      "    static_assert(LDBL_EPSILON    ==    1.08420217248550443401E-19L, \"\");\n",
      "}\n",
      "\n",
      "Michael Innerberger said: A quick google search tells me the MVC++ compiler doesn't really do `long double` .\n",
      "https://learn.microsoft.com/en-us/cpp/cpp/fundamental-types-cpp?view=msvc-170#floating-point-types\n",
      "William Katz said: From Google's Bard:\n",
      "<image.png>\n",
      "William Katz said: I then modified the prompt to ask about \"modern 64-bit platforms\":\n",
      "<image.png>\n",
      "William Katz said: However Bing now informs me \"https://stackoverflow.com/questions/7120710/why-did-microsoft-abandon-long-double-data-type`long double`https://stackoverflow.com/questions/7120710/why-did-microsoft-abandon-long-double-data-type. However, its support varies depending on the compiler and platform. https://en.wikipedia.org/wiki/Long_double`long double`https://en.wikipedia.org/wiki/Long_double`double` .\"  This is the end of my attempt to be a proxy for LLMs to answer your question :)\n",
      "Why did Microsoft abandon long double data type?\n",
      "A while ago I wrote a program which used some factorial functions. I used the long double data type to support \"relative\" big numbers. \n",
      "Now, I changed from codeblocks to Visualstudio 2010, I was\n",
      "Mark Kittisopikul said: We can test this via https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIApACYAQuYukl9ZATwDKjdAGFUtAK4sGIAGwA7KSuADJ4DJgAcj4ARpjEEgDMpAAOqAqETgwe3r4BwemZjgLhkTEs8YlcKXaYDtlCBEzEBLk%2BfkG2mPYlDE0tBGXRcQnJts2t7fldCpNDESOVYzUAlLaoXsTI7BzmSRHI3lgmSW5MCkqtAHQIp9gmGgCC%2B4fHmKduVLSoTAS390eTwiBAA1CwmBEIAA3VB4dCrUEmQJWZ6g9GguZ/PDIAD6FyuBAgmQAXphUFRqD8/oikUkACKnekYlmstls6RIsxmcxmVanVFPFlYxx4gkJIkAMVCABVcQAlJ70gCSAA02UymeztTqzKQuTzufykoLhc1RfjLhKINK5QBZJ5ROUqgDiGM1DJ1XpZZk5vN5xtNGJFOMthJtstxru1HuZ3vj/n1/qNAqBZuxYqtrQj9uVUVx2FVyhZsfjOoAtFwzABWJPcgOptHB82h8XZ224u153FcDQFot0xmesva8tJYIGhsmtPNjNh60dh2q/vF90MrUj9lVgAcdcNfMbQtnFrbUsjS57fcLxdLm41liSu8nKenTfRIcz4cXeZj6%2BH2q4a4uECatJAATkkJJq2wMdt0lPcpyDd8W0/BdzyedV2VvbUkmuSQNDMbczCSSRAgeB84IQl8kMxFD53bSNsGUIRlVCAB5KISz/OMAKAsCzA0fiwI0GCNECeDnwPV8XjfWi51PYk8DJCkIHQTZYnoRFsLvVkn2TKSaI/eiiXpCxQk7R1nWVN10W0nT0WrFJJMDGdkPkrMTLMqNrN/IcePs9EuFrZzD3TE8PIgUzzK7fNry4vyAtBSsCK4KiDNcuTwvDKLO27XsV0HDcArHMS0pc2SjIUnKLzitcEsS3tfTK0Lj1bCLqowy8CrsnSkg0PT62ojLKvaryl18jdAMCMDAn8MCkhqSRt38YigrI6w%2BoG/dyqPNysutaqfyw7j0TMa5uWrMTH2rbdLsCASpBgzbmuksK2uyrymJY9jOLqjczu5DRJEkfwgf4y6FoW0SuETELpORRlng4dZaE4ateD8DgtFIVBODcaxrExTZtkwLkFt4AhNGR9YAGsQEu/ROEkDGqZxzheAUEANFISmseR0g4FgJA0BYVI6ASchKBFsX6ESI5DGAXEAHd4SqYBSCwaEcUwAA1PBMCVtjUkYTgeD4OgCASTmIFiVnYgiFoAE9Td4e3mGIR22NibR6l5s2RbYQQ2IYWhnb5jXMAhIxxHD/BiF9vBoUwTnw8wVR6i8S2XfIQQelZ2g8FiYgnY8LBWYIYg8BYF31m%2BJhgAUPWDaNk3uF4fhBBEMR2CkGRBEUFR1HD3RUoMIwUAJyx9ELznIHWVBUj6FOOZ6BPnAgVxpj8VKwkWCoqj0IosgELfD4yY%2BGGGfeVm6XpGnmU/UrqBoBAGVor9GaoJkGR/v/fvfP4SHWAoYmOwgGMw4OjUgmNsa4w4KCVQy1yz%2BEkKCeWRhQQQBVugNWiIIC4EICQMmXBVgUypqsWm9Nuaow4MzaBrM4Ecy5jzchKNOBmBZuHRhLC%2BYUNIEnYgmRnCSCAA%3D.\n",
      "Compiler Explorer\n",
      "Compiler Explorer is an interactive online compiler which shows the assembly output of compiled C++, Rust, Go (and many more) code.\n",
      "\n",
      "1682352289.294579\n",
      "--------------------------------------------------\n",
      "Document[channel=C02HDABKNAE,ts=1682352289.294579]\n",
      "William Katz said: Michael Innerberger Was there an already existing repo from which you're presenting for Code Review tomorrow?\n",
      "Konrad Rokicki said: Here it is: https://github.com/PreibischLab/STIM/tree/feature/anndata-io\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Michael Innerberger said: Exactly, thanks Konrad! I'll be talking mainly about the IO module, since this is where I spent most of my time and what I would like some feedback on.\n",
      "\n",
      "1682449371.972299\n",
      "--------------------------------------------------\n",
      "Document[channel=C02HDABKNAE,ts=1682449371.972299]\n",
      "William Katz said: Was checking Bard for code analysis and used one of their recommended prompts \"Tell me about the code within the <name> repo\". Surprised it thought DVID was \"written in Python\" given Github's own language tracker shows it's mostly Go. I'm willing to forgive it's misstep, though, since it gave the repo a strong recommendation at the end and encouraged me to check it out.  \n",
      "<image.png>\n",
      "William Katz said: Note that a lot of the DVID description was hallucinated.\n",
      "Not sure how \"easy\" it is, and you can't \"add new data types and features to DVID by writing code in Python.\"\n",
      "Konrad Rokicki said: And yet people are using it to generate entire books: https://news.ycombinator.com/item?id=35687868\n",
      "\n",
      "1683321781.406009\n",
      "--------------------------------------------------\n",
      "Document[channel=C02HDABKNAE,ts=1683321781.406009]\n",
      "Konrad Rokicki said: \n",
      "Special <https://hhmionline.sharepoint.com/Pages/Calendar/ScheduledEvent.aspx?EventId=104873|\"Code Review\" talk> next Tuesday at 1 on napari-chatGPT:\n",
      "<https://github.com/royerlab/napari-chatgpt>\n",
      "Davis Bennett said: the github page for this repo makes my m2 mac lock up, i think due to some video content\n",
      "Davis Bennett said: curious if other m1 / m2 users experience the same thing\n",
      "Donald Olbris said: I think embedded fifty videos on your front page is probably not a good idea.\n",
      "\n",
      "1683248797.347759\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1683248797.347759]\n",
      "Donald Olbris said: I am interested.\n",
      "Cristian Goina said: Yes I am\n",
      "Tobias Pietzsch said: \n",
      "Eric Trautman said: \n",
      "Mark Kittisopikul said: didn't you get an invite already Eric?\n",
      "Eric Trautman said: yes - just being complete - sorry for any confusion\n",
      "Mark Kittisopikul said: I am attempting to schedule several Java @ Janelia meetings on a couple of topics:\n",
      "Conda & Java (May 15th, 1 pm)Java launcher OSSI Project (date TBD)Java 11+ migration (date TBD)\n",
      "If you would like a calendar invite, let me know.\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "Konrad Rokicki said: Cristian Goina might be interested in the Java 11 migration discussion\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1683654085.094949\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1683654085.094949]\n",
      "Michael Innerberger said: Here is a  problem I discussed earlier with John Bogovic: If there are multiple recursive bounds on template parameters, `javac` (version 11) throws an `incompatible types: inferred type does not conform to upper bound(s)` error on the second line of `main()` in the example below.  It seems that the order of template parameters matter..  Does anyone have a good explanation for that behavior and / or a workaround?\n",
      "import java.util.List;\n",
      "interface TypeA<T> {}\n",
      "interface TypeB<T> {}\n",
      "public class Mwe {\n",
      "    public static void main(String[] args) {\n",
      "        List<? extends TypeA<?>> erroneousListA = getGenericList(); // works\n",
      "        List<? extends TypeB<?>> erroneousListB = getGenericList(); // doesn't work\n",
      "    }\n",
      "    protected static <T extends TypeA<T> & TypeB<T>> List<T> getGenericList() {\n",
      "        return null;\n",
      "    }\n",
      "}\n",
      "\n",
      "Others reacted to the previous message with pray a total of 1 times.\n",
      "Michael Innerberger said: Note: if the type bounds are not recursive, this compiles just fine.\n",
      "Mark Kittisopikul said: This seems to be running on Zulu-11 for me...\n",
      "import java.util.List;\n",
      "interface TypeA<T> {}\n",
      "interface TypeB<T> {}\n",
      "public class MichaelTest {\n",
      "    public static void main(String[] args) {\n",
      "        List<? extends TypeA<?>> erroneousListA = getGenericList(); // works\n",
      "        List<? extends TypeB<?>> erroneousListB = getGenericList(); // doesn't work\n",
      "        System.out.println(\"erroneousListB: \" + erroneousListB);\n",
      "        System.out.println(System.getProperty(\"java.version\"));\n",
      "    }\n",
      "    protected static <T extends TypeA<T> & TypeB<T>> List<T> getGenericList() {\n",
      "        return null;\n",
      "    }\n",
      "}\n",
      "output:\n",
      "Mark Kittisopikul said: erroneousListB: null\n",
      "11.0.17\n",
      "\n",
      "Michael Innerberger said: Hm, interesting. For me, none of the installed JDKs would compile that file:\n",
      "openjdk-11 (from Ubuntu)semeru-11.0.17 (from IBM)azul-11.0.18\n",
      "Mark Kittisopikul said: Your error is a compile time error, right?\n",
      "Michael Innerberger said: Yep\n",
      "John Bogovic said: weirdly, i get that compilation error using maven and my systems jdk8, but not in eclipse \n",
      "Mark Kittisopikul said: There are some options that can be passed to the compiler.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Michael Innerberger said: I also get this error in IntelliJ (also tried with the zulu-17 jdk). Can you find out which compiler flags are passed in eclipse?\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Mark Kittisopikul said: I'm starting to suspect this is a bug.\n",
      "https://mail.openjdk.org/pipermail/compiler-dev/2019-December/014114.html\n",
      "Others reacted to the previous message with eyes a total of 2 times.\n",
      "Mark Kittisopikul said: import java.util.List;\n",
      "interface TypeA<T> {}\n",
      "interface TypeB<T> {}\n",
      "public class MichaelTest {\n",
      "    public static void main(String[] args) {\n",
      "        List<? extends TypeA<?>> erroneousListA = getGenericList(); // works\n",
      "        List<? extends TypeA<? extends TypeB<?>>> erroneousListB = getGenericList(); // works now\n",
      "        System.out.println(\"erroneousListB: \" + erroneousListB);\n",
      "        System.out.println(System.getProperty(\"java.version\"));\n",
      "    }\n",
      "    protected static <T extends TypeA<T> & TypeB<T>> List<T> getGenericList() {\n",
      "        return null;\n",
      "    }\n",
      "}\n",
      "This compiles.\n",
      "\n",
      "1683920338.730439\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1683920338.730439]\n",
      "Mark Kittisopikul said: \n",
      "Extreme Multi-Threading: C++ and Julia 1.9 Integration [and some Java]\n",
      "<https://scientificcoder.com/extreme-multi-threading-c-and-julia-19-integration>\n",
      "> In this tutorial we demonstrate how to call Julia libraries with multiple threads from C++. With the introduction of Julia 1.9 in May 2023, the runtime can dynamically \"adopt\" external threads, enabling the integration of Julia libraries into multi-threaded codebases written in other languages, such as C++. This article is written in collaboration with <https://www.linkedin.com/in/evangelos-paradas/|Evangelos Paradas>, the maestro of algorithm deployment at ASML.\n",
      "...\n",
      "> We encountered a pitfall with Java, when embedding our library into Spark. In this article, we will not go into the details of passing Java threads (via C++) to Julia, but we noticed some issues with the Java signal handler. Make sure that your library is explicitly aware of the Java signal handling library, for example via `export LD_PRELOAD=/path/to/libjsig.so` . Otherwise Julia will produce a segmentation fault and your application will crash. This is some kind of language interoperability issue that we had to circumvent.\n",
      "\n",
      "1684169049.161289\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684169049.161289]\n",
      "Mark Kittisopikul said: Slides for Java meeting at 1 pm:\n",
      "https://docs.google.com/presentation/d/1kM5rQMU6O-KlIcTMOqqGzv_RPYC3a_oj99BZq5xxx9o/edit?usp=sharing\n",
      "\n",
      "1684169798.834689\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684169798.834689]\n",
      "Mark Kittisopikul said: @here I'm in Photon, sorry foe the late notice about the location\n",
      "\n",
      "1684193791.194479\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684193791.194479]\n",
      "Mark Kittisopikul said: I reminded myself how to manipulate maven settings in a conda environment. Basically `maven.home` is set to `$CONDA_PREFIX/opt/maven`. The settings.xml can be manipulated at\n",
      "$CONDA_PREFIX/opt/maven/conf/settings.xml\n",
      "\n",
      "\n",
      "1684194167.093509\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684194167.093509]\n",
      "Mark Kittisopikul said: However, it does seem ones `~/.m2/settings.xml` is also investigated.\n",
      "\n",
      "1684240026.397689\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684240026.397689]\n",
      "Stephan Preibisch said: The registration for the ImgLib2 & BDV hackathon is open https://www.ceitec.eu/imglib2-and-bigdataviewer-hackathon-brno/a4534\n",
      "ImgLib2 and BigDataViewer Hackathon Brno\n",
      "The hackathon is intended for both experienced developers and&amp;nbsp;newcomers, and will include a learnathon part during the first days.&amp;nbsp;Participants are free to work on any topic related to ImgLib2 and/or&amp;nbsp;BigDataViewer, be it improving the core&amp;nbsp;libraries, or using them in other&amp;nbsp…\n",
      "Mark Kittisopikul said: Note that there is an overlap with US RSE\n",
      "\n",
      "1684242704.970379\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684242704.970379]\n",
      "Mark Kittisopikul said: This is my mamba base environment's package cache. The `pkgs` folder has multiple versions of zstd packages.\n",
      "$ ls mambaforge/pkgs | grep zstd\n",
      "zstd-1.4.9-haebb681_0\n",
      "zstd-1.4.9-haebb681_0.conda\n",
      "zstd-1.5.0-ha95c52a_0\n",
      "zstd-1.5.0-ha95c52a_0.tar.bz2\n",
      "zstd-1.5.2-h6239696_4\n",
      "zstd-1.5.2-h6239696_4.tar.bz2\n",
      "zstd-1.5.2-h8a70e8d_1\n",
      "zstd-1.5.2-h8a70e8d_1.tar.bz2\n",
      "I think this is more or less or equivalent to maven's cache.\n",
      "$ ls ~/.m2/repository/org/janelia/saalfeldlab/n5-hdf5/\n",
      "1.3.0  1.4.1  1.4.2  1.4.2-SNAPSHOT\n",
      "I'm still trying to understand the multiple version thing, Stephan Saalfeld. Do we need more than one version of something on the classpath at any one point in time?\n",
      "\n",
      "1684249317.620289\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684249317.620289]\n",
      "Stephan Saalfeld said: No, but in one environment. For different tools.\n",
      "Mark Kittisopikul said: I'm not quite sure I'm following still. The mapping here is Maven pom.xml => Conda environment. What is the functional role of the environment? Do the different tools need to live in the same JDK or are they different processes?\n",
      "Mark Kittisopikul said: I think what you want to do is just launch multiple tools from the same terminal, but each tool is effectively operating within it's own project and it's own practical set of dependencies.\n",
      "\n",
      "1684249787.391069\n",
      "1684249816.476759\n",
      "1684249863.560929\n",
      "1684249891.418109\n",
      "1684250022.320309\n",
      "1684250062.881859\n",
      "1684250087.108009\n",
      "1684250101.155329\n",
      "1684250114.018549\n",
      "1684250363.729279\n",
      "1684250410.597479\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684249787.391069]\n",
      "Stephan Saalfeld said: I do not think that the mapping pom-xml -> conda environment is practical\n",
      "Stephan Saalfeld said: I want to install a variety of command line tools that I can call without constantly switching conda environments\n",
      "Stephan Saalfeld said: with java tools, each tool has its own class-path, so this is absolutely no problem at all\n",
      "Stephan Saalfeld said: if I throw that away, I am reimplementing the inferior 'system-wide classpath' (include path without versions) of all of the entire OS which does not seem a good idea\n",
      "Stephan Saalfeld said: i.e. java tools do not need the entire concept of conda-environments except for their native bindings, we should not introduce this\n",
      "Mark Kittisopikul said: Generalizing that though each tool may not only need it's own `classpath`, but also it's own `LD_LIBRARY_PATH` , or `PATH `, `DYLD_LIBRARY_PATH` .\n",
      "Davis Bennett said: doesn't `conda run` solve the problem of needing to switch environments?\n",
      "Davis Bennett said: https://docs.conda.io/projects/conda/en/stable/commands/run.html\n",
      "Stephan Saalfeld said: confirmed, but turning the tool-local classpath into an environment global library path is not a good idea\n",
      "Mark Kittisopikul said: I think the main practical difference is interface. If you could just execute `./tool1 | ./tool2 | tool3`  in a shell, is that sufficient? Each tool maybe operating with it's own library path.\n",
      "Stephan Saalfeld said: `tool1 | tool2 | tool3` please\n",
      "\n",
      "1684250786.648799\n",
      "1684250862.557039\n",
      "1684251007.466129\n",
      "1684251091.662209\n",
      "1684251119.232959\n",
      "1684251224.267719\n",
      "1684251285.243469\n",
      "1684251306.483919\n",
      "1684251551.170169\n",
      "1684251588.453719\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684250786.648799]\n",
      "Mark Kittisopikul said: Right, so tool1 may be a short script that calls `conda run -n env1 tool1` . tool2 maybe a short script that is `mvn exec:java -Dexec.mainClass=\"org.example.Tool2\"`, and tool3 is a script that calls `conda run -n env3 mvn exec:java -Dexec.mainClass=\"org.example.Tool3WithNativeDeps\"`  . The scripts all live in your `~/bin` which is on your `PATH`\n",
      "Mark Kittisopikul said: Most people `conda activate some_env` but you don't actually need to do that to just run one of the tools\n",
      "Stephan Saalfeld said: that would be fine for most things I guess, there are also snap, flatpak, and appimage?\n",
      "Mark Kittisopikul said: https://guix.gnu.org/ ?\n",
      "GNU Guix transactional package manager and distribution — GNU Guix\n",
      "Guix is a distribution of the GNU operating system.\n",
      " Guix is technology that respects the freedom of computer users.\n",
      " You are free to run the system for any purpose, study how it\n",
      " works, improve it, and share it with the whole world.\n",
      "Stephan Saalfeld said: dunno, starts with a lot of propaganda talk, probably cool?\n",
      "Stephan Saalfeld said: it could become confusing, because people typically expect that their conda environment is what they made it, if we now have tools that secretly change the environment, that could be confusing, what does conda best practices say about this?\n",
      "Mark Kittisopikul said: I typically do one tool (a process) in it's own environment\n",
      "Mark Kittisopikul said: If they need to live within the same process, then they have to coexist in the same conda environment.\n",
      "Mark Kittisopikul said: If not, then they can be in separate environments\n",
      "Mark Kittisopikul said: I know some folks nest their environments as well. `env{1,2,3}` may be directories within `top_env`\n",
      "Davis Bennett said:  because people typically expect that their conda environment is what they made it,not exactly true, I expect conda to install what I ask it to install, but if a new package needs to upgrade / downgrade other packages to satisfy compatibility constraints, I'm OK with that, as long as conda tells me what it's doing to my environment beforehand\n",
      "\n",
      "1684252114.233509\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684252114.233509]\n",
      "Mark Kittisopikul said: The snap, flatpak, guix stuff is more on the deployment side, I think. That's next meeting.\n",
      "\n",
      "1684262866.855779\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K818Q3B6,ts=1684262866.855779]\n",
      "Mark Kittisopikul said: How does one build maven from source? It sounds like one uses... maven?\n",
      "https://maven.apache.org/guides/development/guide-building-maven.html\n",
      "Others reacted to the previous message with upside_down_face a total of 1 times.\n",
      "Michael Innerberger said: Until Maven 3.3, Maven core build could be boostrapped with an Ant build. This bootstrap has been removed in Maven 3.5: you need a pre-built Maven to build Maven from source.Do I get this right: if, for some reason, no existing maven install is available, one needs to build an older version of maven using Ant to build the latest version of maven? That seems a bit awkward. \n",
      "John Bogovic said: how else would it work? every compiler has to be compiled using another compiler.\n",
      "Michael Innerberger said: I get why bootstrapping makes sense (and is to some extent necessary) for lower level compilers. But what's the point here? There are usually plenty of other tools available on a system where I would want to install maven. This is a genuine question, I am not really familiar with how compiler / build toolchains work.\n",
      "John Bogovic said: ant was (to my knowledge) the most ubiquitous build tool for java before maven, so a sensible choice to build early versions of maven. at least one could build a new version of maven with an old one\n",
      "\n",
      "1682444361.767659\n",
      "--------------------------------------------------\n",
      "Document[channel=C031U6KUMNU,ts=1682444361.767659]\n",
      "Mark Kittisopikul said: Am I presenting this week?\n",
      "Michael Innerberger said: Yes, you are! Sorry, I've been a bit neglectful of my reminder-email duties this week.\n",
      "Mark Kittisopikul said: ok, I've been trying to stay off campus this week since I have sick family members\n",
      "Mark Kittisopikul said: I haven't figured out if I'm going to get sick yet or not. My sense is that I went through something mild last week, but family seems to have a much worse case. There's a chance I may end up doing this remotely.\n",
      "Michael Innerberger said: Oh no, I hope it's not too bad! If you're not feeling well we can also postpone the talk.\n",
      "Others reacted to the previous message with 100 a total of 1 times.\n",
      "Mark Kittisopikul said: I'm feeling fine at the moment. Everyone else around me is coughing.\n",
      "\n",
      "1682615865.422899\n",
      "--------------------------------------------------\n",
      "Document[channel=C031U6KUMNU,ts=1682615865.422899]\n",
      "Mark Kittisopikul said: https://outlook.office365.com/owa/calendar/c3e68d8adc75469eadd0b2def45f4120@janelia.hhmi.org/6107922bcf8f40dab905e855e70cfa8914597112461841561206/calendar.html\n",
      "\n",
      "1683831487.110499\n",
      "--------------------------------------------------\n",
      "Document[channel=C031U6KUMNU,ts=1683831487.110499]\n",
      "John Bogovic said: Some notes and follow up on today's how-to:\n",
      "Others reacted to the previous message with tada a total of 2 times.\n",
      "John Bogovic said: https://stedolan.github.io/jq/https://github.com/saalfeldlab/n5-ij/wiki/jq-examples-and-tutorialhttps://github.com/saalfeldlab/n5-ij/wiki/TranslateMetadata#tutorialhttps://github.com/saalfeldlab/n5-ij/wiki/N5-Container-Treehttps://github.com/doloopwhile/pyjqhttps://github.com/makenowjust/bf.jq\n",
      "John Bogovic said: Another json processing script with different tradeoffs to jq (re Mark Kittisopikul’s question): https://github.com/pkoppstein/jm\n",
      "relative to jq, it appears to use much less memory at the cost of speed\n",
      "\n",
      "1683139511.672019\n",
      "--------------------------------------------------\n",
      "Document[channel=C032XSC2CJC,ts=1683139511.672019]\n",
      "Mark Kittisopikul said: archived the channel\n",
      "\n",
      "1681737243.245529\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1681737243.245529]\n",
      "Davis Bennett said: I used the typescript library https://zod.dev/ for the first time recently. it provides runtime type checking, a bit like pydantic. thanks to the `satisfies` operator in typescript, I could get the runtime type checking hooked up to the my static types: https://github.com/colinhacks/zod/issues/372#issuecomment-1432110698\n",
      "Comment on #372 Typecheck schemas against existing types\n",
      "> As for the original question, I recommend using a utility function like this:\n",
      "> \n",
      "> ```\n",
      "> const schemaForType = <T>() => <S extends z.ZodType<T, any, any>>(arg: S) => {\n",
      ">   return arg;\n",
      "> };\n",
      "> \n",
      "> // use like this:\n",
      "> const dog = schemaForType<Dog>()(\n",
      ">   z.object({\n",
      ">     name: z.string(),\n",
      ">     neutered: z.boolean(),\n",
      ">   })\n",
      "> );\n",
      "> ```\n",
      "for anyone else finding this, it can now be done super cleanly in ts 4.9 with satisfies\n",
      "```\n",
      " // use like this:\n",
      " const dog = z.object({\n",
      "     name: z.string(),\n",
      "     neutered: z.boolean(),\n",
      "   }) satisifes z.ZodType<Dog, any, any>;\n",
      "```\n",
      "Davis Bennett said: the basic use case is this: I'm parsing a JSON blob (type: `unknown`) into  a row in a statically typed database table, so I want validation at runtime to ensure that contents of the blob are compatible with the database schema (I could always handle the error from the DB, but it's not typically very verbose).\n",
      "Mark Kittisopikul said: What happens when it does not satisfy the type?\n",
      "Mark Kittisopikul said: I'm assuming `dog` gets assigned the object normally?\n",
      "Davis Bennett said: if the object does not satisfy the type, then the type check fails and your code will not compile to js, unless you force it\n",
      "Mark Kittisopikul said: I thought you said runtime\n",
      "Davis Bennett said: `satisfies` is a typescript thing\n",
      "Davis Bennett said: the zod object does the runtime stuff. zod objects have a `.parse` method that takes some data and either returns an object or a validation error\n",
      "Mark Kittisopikul said: const dog = schemaForType<Dog>()(\n",
      "  z.object({\n",
      "    name: z.string(),\n",
      "    neutered: z.boolean(),\n",
      "  })\n",
      ");\n",
      "So this does a runtime check then?\n",
      "Davis Bennett said: yeah, the resulting `dog` object will have a `.parse` method that attempts to parse its input as a `dog`\n",
      "Mark Kittisopikul said: does it throw if it fails?\n",
      "Davis Bennett said: it either throws or returns an error object, let me check\n",
      "Davis Bennett said: import { z } from \"zod\";\n",
      "// creating a schema for strings\n",
      "const mySchema = z.string();\n",
      "// parsing\n",
      "mySchema.parse(\"tuna\"); // => \"tuna\"\n",
      "mySchema.parse(12); // => throws ZodError\n",
      "// \"safe\" parsing (doesn't throw error if validation fails)\n",
      "mySchema.safeParse(\"tuna\"); // => { success: true; data: \"tuna\" }\n",
      "mySchema.safeParse(12); // => { success: false; error: ZodError }\n",
      "\n",
      "Davis Bennett said: `parse` throws, `safeParse` returns a result object\n",
      "\n",
      "1681787445.079289\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1681787445.079289]\n",
      "Donald Olbris said: OOTL ELI5 WTF...is this a joke, or is it in reference to something? The page itself gives no indication.\n",
      "More broadly: when posting links, don't assume everyone reads the same news/blogs/twitter feeds you do. Context helps.\n",
      "Davis Bennett said: here's some context: https://www.theregister.com/2023/04/17/rust_foundation_apologizes_trademark_policy/\n",
      "Rust Foundation apologizes for trademark policy confusion\n",
      "Should have wrapped proposed rules on name and logo use in unsafe {} ?\n",
      "Others reacted to the previous message with thankyou a total of 2 times.\n",
      "Mark Kittisopikul said: Donald Olbris sorry, but I didn't have any further context. I found the project on Github and followed it to the website.\n",
      "Mark Kittisopikul said: A community fork of a language named after a plant fungus. All of the memory-safe features you love, now with 100% less bureaucracy!\n",
      "https://crablang.org/\n",
      "CrabLang\n",
      "A community fork of a language named after a plant fungus.\n",
      "\n",
      "1681995938.600049\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1681995938.600049]\n",
      "Davis Bennett said: something interesting about the  https://zod.dev/ is that you can create validators for composite types in two ways:\n",
      "const foo = z.optional(z.array(z.string()));\n",
      "const same_as_foo = z.string().array().optional();\n",
      "i'm not sure if I this is good or bad \n",
      "Davis Bennett said: i'm leaning towards \"mildly bad\"\n",
      "\n",
      "1682532537.653699\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1682532537.653699]\n",
      "Davis Bennett said: I have a python API for identifying arrays on disk / cloud storage by url, and I am thinking about ways of making the same API work for in-memory arrays as well. One idea would be to widen types from `string` to `string | array` , and if a `string` is supplied, use that string to load the data, but for in-memory arrays users would just pass in an instance of `array` without ever thinking about strings.\n",
      "But another idea would be to use strings like `memory://$VARNAME`, then dispatch on the protocol part of the url (`memory` ) in this case, and then look up `$VARNAME` from `locals()` . I kind of like that this avoid a union type, but maybe there's something terribly wrong with this approach?\n",
      "Davis Bennett said: one problem with the second approach is that if data is mutable, it's hard to keep track of which places in the code might be mutating it\n",
      "Michael Innerberger said: The only disadvantage I can think of right now would be the overhead incurred by the dispatch itself and the subsequent lookup in `locals()`. This might be a problem if you're querying a lot of small arrays, but I guess that's not your main usecase, anyways.\n",
      "Davis Bennett said: right, this would be called a few times on relatively large arrays\n",
      "Michael Innerberger said: What is the motivation behind thinking of such an API for in-memory arrays? I guess uniformity.\n",
      "Davis Bennett said: the main use case would be for generating specifications of collections of arrays, where url for the array is part of the specification, e.g.\n",
      "{\n",
      "arrays: [{url: 'memory://var_name', shape: [10, 10], dtype='int8'},\n",
      "         {url: 's3://bucket/path', shape: [5, 5], dtype='int8'}]\n",
      "}\n",
      "\n",
      "Davis Bennett said: and the goal for the specification is to enable validation of properties of collections of arrays\n",
      "Davis Bennett said: i'm still not convinced that I need this though \n",
      "Michael Innerberger said: You can definitely do without this. But syntactic sugar can also help to focus on the main problem by creating an abstraction-layer. Anyway, your solution looks good to me.\n",
      "Others reacted to the previous message with +1::skin-tone-2 a total of 1 times.\n",
      "Davis Bennett said: i'm starting to suspect that if i'm in a situation where I need a reference to an object AND copies of some of its properties, I can just use the object itself (which I can access, because I have the reference). But if I just need a few of its properties, then I don't need a reference to the object...\n",
      "Mark Kittisopikul said: The only disadvantage I can think of right now would be the overhead incurred by the dispatch itself and the subsequent lookup in `locals()`. This might be a problem if you're querying a lot of small arrays, but I guess that's not your main usecase, anyways.If only there were a way to elide dispatch when the types are known ...\n",
      "Mark Kittisopikul said: I think you are starting to rehash some of Plan 9's concepts:\n",
      "https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs\n",
      "Under Plan 9, UNIX's https://en.wikipedia.org/wiki/Everything_is_a_file metaphor is extended via a pervasive network-centric https://en.wikipedia.org/wiki/File_system\n",
      "Plan 9 from Bell Labs\n",
      "Plan 9 from Bell Labs is a distributed operating system which originated from the Computing Science Research Center (CSRC) at Bell Labs in the mid-1980s and built on UNIX concepts first developed there in the late 1960s. Since 2000, Plan 9 has been free and open-source. The final official release was in early 2015.\n",
      "Under Plan 9, UNIX's everything is a file metaphor is extended via a pervasive network-centric filesystem, and the cursor-addressed, terminal-based I/O at the heart of UNIX-like operating systems is replaced by a windowing system and graphical user interface without cursor addressing, although rc, the Plan 9 shell, is text-based.\n",
      "The name Plan 9 from Bell Labs is a reference to the Ed Wood 1957 cult science fiction Z-movie Plan 9 from Outer Space. The system continues to be used and developed by operating system researchers and hobbyists.\n",
      "Everything is a file\n",
      "Everything is a file is an idea that Unix, and its derivatives, handle input/output to and from resources such as documents, hard-drives, modems, keyboards, printers and even some inter-process and network communications as simple streams of bytes exposed through the filesystem name space. Exceptions include shared memory, semaphores, datagram sockets, symbolic links, directories (which are read-only, and are not accessed as a byte-stream), processes and threads.\n",
      "The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources and a number of file types. When a file is opened, a file descriptor is created, using the file path as an addressing system. The file descriptor is then a byte stream I/O interface on which file operations are performed. Additionally, file descriptors are also created for objects such as anonymous pipes and network sockets - and therefore a more accurate description of this feature is Everything is a file descriptor.Additionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy.\n",
      "An example of this purely virtual filesystem is under /proc that exposes many system properties as files.  All of these files, in the broader sense of the word, have standard Unix file attributes such as an owner and access permissions, and can be queried by the same classic Unix tools and filters.  However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns.  It is, though, used heavily by both the widely installed BusyBox on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.\n",
      "Mark Kittisopikul said: In a way, you could think of your named memory segments as being similar to POSIX shared memory. That is memory with an associated path. Really your `string` is a path, I believe.\n",
      "\n",
      "1683163787.122779\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1683163787.122779]\n",
      "Mark Kittisopikul said: More on Mojo, this time from Jeremy Howard:\n",
      "https://www.fast.ai/posts/2023-05-03-mojo-launch.html\n",
      "fast.ai - Mojo may be the biggest programming advance in decades\n",
      "Mojo is a new programming language, based on Python, which fixes Python’s performance and deployment problems.\n",
      "\n",
      "1683834081.953989\n",
      "1683834097.028369\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1683834081.953989]\n",
      "Davis Bennett said: i love and fear the typescript type system: here I'm defining a recursive type that maps strings to Date objects iff the key for the string is in a specific set.\n",
      "export type DateFields = \"createdAt\" | \"startDate\"\n",
      "type ToDate<T> = { \n",
      "    [P in keyof T]: [P, string] extends [DateFields, T[P]] \n",
      "    ? Date \n",
      "    : ToDate<T[P]> }\n",
      "type fooType = {\n",
      "    createdAt: string\n",
      "    foo: string\n",
      "    bar: {createdAt: string,\n",
      "          startDate: string,\n",
      "        foo: number,\n",
      "        baz: string[]\n",
      "    }\n",
      "}\n",
      "\n",
      "Davis Bennett said: \n",
      "<image.png>\n",
      "\n",
      "1683920271.714819\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1683920271.714819]\n",
      "Mark Kittisopikul said: Extreme Multi-Threading: C++ and Julia 1.9 Integration [and some Java]\n",
      "https://scientificcoder.com/extreme-multi-threading-c-and-julia-19-integration\n",
      "In this tutorial we demonstrate how to call Julia libraries with multiple threads from C++. With the introduction of Julia 1.9 in May 2023, the runtime can dynamically \"adopt\" external threads, enabling the integration of Julia libraries into multi-threaded codebases written in other languages, such as C++. This article is written in collaboration with https://www.linkedin.com/in/evangelos-paradas/, the maestro of algorithm deployment at ASML....\n",
      "We encountered a pitfall with Java, when embedding our library into Spark. In this article, we will not go into the details of passing Java threads (via C++) to Julia, but we noticed some issues with the Java signal handler. Make sure that your library is explicitly aware of the Java signal handling library, for example via `export LD_PRELOAD=/path/to/libjsig.so` . Otherwise Julia will produce a segmentation fault and your application will crash. This is some kind of language interoperability issue that we had to circumvent.\n",
      "Extreme Multi-Threading: C++ and Julia 1.9 Integration\n",
      "A guide to weaving C++ threads into Julia with the latest thread adoption feature in Julia version 1.9\n",
      "\n",
      "1684165081.391769\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1684165081.391769]\n",
      "Mark Kittisopikul said: This is an interesting map of relationships between Github projects:\n",
      "https://anvaka.github.io/map-of-github/#2/0/0\n",
      "Map of GitHub\n",
      "This website shows a map of GitHub. Each dot is a project. Two dots within the same cluster are usually close to each other if multiple users frequently gave starts to both projects\n",
      "Others reacted to the previous message with astonished a total of 5 times, and with star-struck a total of 1 times.\n",
      "\n",
      "1684262257.811839\n",
      "--------------------------------------------------\n",
      "Document[channel=C03DJGPC69K,ts=1684262257.811839]\n",
      "Mark Kittisopikul said: The discussion of Erg in  is challenging my definition of static typing.\n",
      "A statically-typed language is a language (such as Java, C, or C++) where variable types are known at compile time. In most of these languages, types must be expressly indicated by the programmer; in other cases (such as OCaml), type inference allows the programmer to not indicate their variable types.-From https://developer.mozilla.org/en-US/docs/Glossary/Static_typing#\n",
      "It's not clear to me what compile time means in Erg.\n",
      "By that definition, Julia could also be statically typed because type inference can determine the types at \"compile time\" even though I have not explicitly declared any types. In this case, we can determine that `f` is going to return an `Int64` if the arguments are `Int64` .\n",
      "julia> function f(x, y)\n",
      "           z = x + y^2\n",
      "       end\n",
      "f (generic function with 1 method)\n",
      "julia> code_warntype(f, (Int, Int))\n",
      "MethodInstance for f(::Int64, ::Int64)\n",
      "  from f(x, y) in Main at REPL[3]:1\n",
      "Arguments\n",
      "  #self#::Core.Const(f)\n",
      "  x::Int64\n",
      "  y::Int64\n",
      "Locals\n",
      "  z::Int64\n",
      "Body::Int64\n",
      "1 ─ %1 = Core.apply_type(Base.Val, 2)::Core.Const(Val{2})\n",
      "│   %2 = (%1)()::Core.Const(Val{2}())\n",
      "│   %3 = Base.literal_pow(Main.:^, y, %2)::Int64\n",
      "│   %4 = (x + %3)::Int64\n",
      "│        (z = %4)\n",
      "└──      return %4\n",
      "\n",
      "Static typing - MDN Web Docs Glossary: Definitions of Web-related terms | MDN\n",
      "A statically-typed language is a language (such as Java, C, or C++) where variable types are known at compile time. In most of these languages, types must be expressly indicated by the programmer; in other cases (such as OCaml), type inference allows the programmer to not indicate their variable types.\n",
      "Michael Innerberger said: Yes, Julia tries to infer as many types at compile time as possible, but sometimes this is not possible:\n",
      "julia> function unstable(flag::Bool)\n",
      "           if flag\n",
      "               return 1\n",
      "           else\n",
      "               return 1.0\n",
      "           end\n",
      "       end\n",
      "unstable (generic function with 1 method)\n",
      "See also this https://stackoverflow.com/a/28086285 on stackoverflow:\n",
      "Julia is dynamically typed, but in well-written julia code the types can usually be inferred. You often get a major performance enhancement when that is possible.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Mark Kittisopikul said: Right... so is stably typed code \"statically\" typed since it all can be inferred?\n",
      "Michael Innerberger said: Hm. In principle yes, I guess. But if it doesn't actually do the type inference or use that information, I would probably not call it statically typed (think of Python with type annotations). The boundary seems a bit fuzzy\n",
      "Mark Kittisopikul said: I can improve the definition in this way:\n",
      "julia> function f(x::T, y::T)::T where T\n",
      "           z = x + y^2\n",
      "       end\n",
      "julia> @code_warntype f(3, 4)\n",
      "MethodInstance for f(::Int64, ::Int64)\n",
      "  from f(x::T, y::T) where T @ Main REPL[11]:1\n",
      "Static Parameters\n",
      "  T = Int64\n",
      "Arguments\n",
      "  #self#::Core.Const(f)\n",
      "  x::Int64\n",
      "  y::Int64\n",
      "Locals\n",
      "  z::Int64\n",
      "Body::Int64\n",
      "1 ─ %1 = $(Expr(:static_parameter, 1))::Core.Const(Int64)\n",
      "│   %2 = Main.:^::Core.Const(^)\n",
      "│   %3 = Core.apply_type(Base.Val, 2)::Core.Const(Val{2})\n",
      "│   %4 = (%3)()::Core.Const(Val{2}())\n",
      "│   %5 = Base.literal_pow(%2, y, %4)::Int64\n",
      "│   %6 = (x + %5)::Int64\n",
      "│        (z = %6)\n",
      "│   %8 = Base.convert(%1, %6)::Int64\n",
      "│   %9 = Core.typeassert(%8, %1)::Int64\n",
      "└──      return %9\n",
      "\n",
      "Mark Kittisopikul said: Even better if assert this internally, then the compiler knows that this function will never return.\n",
      "julia> function f(x::T, y::T)::T where T\n",
      "           z = x + y^2 + 0.5\n",
      "           z::T\n",
      "       end\n",
      "f (generic function with 2 methods)\n",
      "julia> @code_llvm f(3, 4)\n",
      ";  @ REPL[21]:1 within `f`\n",
      "; Function Attrs: noreturn\n",
      "define void @julia_f_502(i64 signext %0, i64 signext %1) #0 {\n",
      "top:\n",
      "  %2 = call {}*** inttoptr (i64 7054503440 to {}*** (i64)*)(i64 261) #4\n",
      ";  @ REPL[21]:2 within `f`\n",
      "; ┌ @ intfuncs.jl:319 within `literal_pow`\n",
      "; │┌ @ int.jl:88 within `*`\n",
      "    %3 = mul i64 %1, %1\n",
      "; └└\n",
      "; ┌ @ operators.jl:578 within `+` @ int.jl:87\n",
      "   %4 = add i64 %3, %0\n",
      "; │ @ operators.jl:578 within `+` @ promotion.jl:410\n",
      "; │┌ @ promotion.jl:381 within `promote`\n",
      "; ││┌ @ promotion.jl:358 within `_promote`\n",
      "; │││┌ @ number.jl:7 within `convert`\n",
      "; ││││┌ @ float.jl:159 within `Float64`\n",
      "       %5 = sitofp i64 %4 to double\n",
      "; │└└└└\n",
      "; │ @ operators.jl:578 within `+` @ promotion.jl:410 @ float.jl:408\n",
      "   %6 = fadd double %5, 5.000000e-01\n",
      "; └\n",
      ";  @ REPL[21]:3 within `f`\n",
      "  %ptls_field4 = getelementptr inbounds {}**, {}*** %2, i64 2\n",
      "  %7 = bitcast {}*** %ptls_field4 to i8**\n",
      "  %ptls_load56 = load i8*, i8** %7, align 8\n",
      "  %8 = call noalias nonnull {}* @ijl_gc_pool_alloc(i8* %ptls_load56, i32 1392, i32 16) #5\n",
      "  %9 = bitcast {}* %8 to i64*\n",
      "  %10 = getelementptr inbounds i64, i64* %9, i64 -1\n",
      "  store atomic i64 4865755760, i64* %10 unordered, align 8\n",
      "  %11 = bitcast {}* %8 to double*\n",
      "  store double %6, double* %11, align 8\n",
      "  call void @ijl_type_error(i8* getelementptr inbounds ([11 x i8], [11 x i8]* @_j_str1, i64 0, i64 0), {}* inttoptr (i64 4865757936 to {}*), {}* %8)\n",
      "  unreachable\n",
      "}\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Document[channel=C03S782CCMD,ts=None]\n",
      "\n",
      "1681758480.607959\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1681758480.607959]\n",
      "Philip Hubbard said: I now have personal experience with the problems discussed in the data ethics lesson: any time I try to build an executable on my (managed) Windows laptop, CrowdStrike immediately deletes it.  IT has told me I am supposed to be able to do it, but sometimes CrowdStrike will block it as suspicious activity \"based on a machine learning confidence threshold\".  Yay!\n",
      "Others reacted to the previous message with scream_cat a total of 1 times.\n",
      "Philip Hubbard said: Better yet, we have no control over the \"whitelist\" ourselves.\n",
      "Adam Taylor said: Ach.  Please make sure you report this to Stephan!\n",
      "Philip Hubbard said: I did.  Supposedly, a new \"development group\" is being created for CrowdStrike, and the people in that group will be treated with \"sensor policies that aren't so strict on Windows\".  I am suggesting that everyone in SCSW should be in that group, but I have no idea how long it will take to go into effect (i.e., how soon I will be able to start doing work on one of my projects again).\n",
      "Adam Taylor said: Pretty ridiculous...\n",
      "William Katz said: Luckily I haven't been hit with this yet on my managed Windows PC. Mostly doing WSL2/Ubuntu builds but infrequently Win builds via VS2019. What kind of build process created the issue? Or is it flagging networking code?\n",
      "William Katz said: Or perhaps the ML intruder detector felt you operated far more like a sophisticated hacker than me \n",
      "Philip Hubbard said: I tried the simplest \"Hello world\" application that Visual Studio creates by default!\n",
      "William Katz said: That's weird. I compiled the open source Instant Neural Graphics Primitive system through VS2019 without issue on my new PC. On my old managed Windows laptop, I compiled a few deep learning and PBRT executables without issue either.\n",
      "William Katz said: Is this VS Code or VS 2019?\n",
      "Philip Hubbard said: VS 2019.  https://learn.microsoft.com/en-us/cpp/build/vscpp-step-1-create?view=msvc-170\n",
      "Create a C++ console app project\n",
      "Create a Hello World console app using Microsoft C++ in Visual Studio.\n",
      "Philip Hubbard said: Both Konrad and I have built a number of applications on Windows, too, but not within the last year or so.  I believe the rules must have been tightened.\n",
      "William Katz said: I built a new version of InstantNGP executable on Apr 9 and did full rebuild just now from VS2019. Execution is ok.  It's possible my PC (which I took in to get \"managed\" software installed in last month or two) is configured differently. I had built apps on my fully managed laptop within the last 3 months.\n",
      "I'm not sure I've ever built an app using that Console App method. Usually I'm configuring with CMake, display is ImGUI-based, but I'm typically starting it from powershell. Just tried the Console App from scratch and it built both Debug and Release.\n",
      "<image.png>\n",
      "William Katz said: I do seem to have CrowdStrike on my Win 11 PC since it shows up in my Windows Security and is \"turned on\".\n",
      "\n",
      "1682016202.519839\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1682016202.519839]\n",
      "Philip Hubbard said: Update: while a basic \"hello world\" console application made with Visual Studio Community 2019 still gets quarantined by CrowdStrike, one built with Visual Studio Community 2022 works.  So I guess 2019 must be considered unsafe.  But it is just a guess: several of us had a meeting this morning with folks from the Security team and from CrowdStrike, and CrowdStrike refuses to reveal why any particular executable is quarantined, saying that it is \"proprietary ML decisions\".  Yay.\n",
      "Mark Kittisopikul said: Maybe hackers (crackers?) use old tools? Honestly, it sounds to me like CrowdStrike might not even know why it was marked unsafe. The world is getting kind of scary.\n",
      "\n",
      "1682277240.622459\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1682277240.622459]\n",
      "William Katz said: Bark + D-ID\n",
      "<fastai-video-demo.mp4>\n",
      "Others reacted to the previous message with exploding_head a total of 1 times.\n",
      "William Katz said: Using https://github.com/suno-ai/bark:\n",
      "text_prompt = \"\"\"\n",
      "     Hello, I'm the Fast AI interest group's AI. And, uh — and I'll help you \n",
      "     with deep learning. [laughs] \n",
      "     I hope. Well, at least we'll be messing around with stuff.\n",
      "\"\"\"\n",
      "audio_array = generate_audio(text_prompt, history_prompt=\"en_speaker_1\")\n",
      "Audio(audio_array, rate=SAMPLE_RATE)\n",
      "\n",
      "William Katz said: Then run through https://studio.d-id.com/\n",
      "William Katz said: A public announcement.\n",
      "<fastai-IG-demo-warning-robot.mp4>\n",
      "David Ackerman said: ha maybe we should all make a bunch and have em battle it out. like pokemon\n",
      "Others reacted to the previous message with grinning a total of 1 times.\n",
      "\n",
      "1682964617.894219\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1682964617.894219]\n",
      "William Katz said: Reminder that we have FastAI office hours tomorrow at 2 pm with Srini Turaga. This would be to show demos, get feeback, and also discuss the last topic, Data / AI Ethics.  Here's the https://course.fast.ai/Lessons/lesson8a.html on it again, and feel free to bring up anything that you might have run across on this topic.\n",
      "Others reacted to the previous message with +1::skin-tone-4 a total of 1 times.\n",
      "\n",
      "1683026438.465429\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683026438.465429]\n",
      "Donald Olbris said: Different kind of AI ethics question: https://arstechnica.com/information-technology/2023/05/stone-hearted-researchers-gleefully-push-over-adorable-soccer-playing-robots/\n",
      "Stone-hearted researchers gleefully push over adorable soccer-playing robots\n",
      "DeepMind tests \"robustness to pushing\" in football robot breakthrough.\n",
      "\n",
      "1683054663.749729\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683054663.749729]\n",
      "Polly said: \n",
      "William Katz said: Can't tell how to edit the close date. As soon as we get a quorum for the preference, I'll modify the name, slack channel, and the wiki page.\n",
      "\n",
      "1683163794.847089\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683163794.847089]\n",
      "Mark Kittisopikul said: \n",
      "More on Mojo, this time from Jeremy Howard:\n",
      "<https://www.fast.ai/posts/2023-05-03-mojo-launch.html>\n",
      "\n",
      "1683315062.312659\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683315062.312659]\n",
      "William Katz said: Special https://hhmionline.sharepoint.com/Pages/Calendar/ScheduledEvent.aspx?EventId=104873 next Tuesday at 1 on napari-chatGPT:\n",
      "https://github.com/royerlab/napari-chatgpt\n",
      "royerlab/napari-chatgpt\n",
      "A napari plugin to process and analyse images with chatGPT!\n",
      "\n",
      "1683346154.854499\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683346154.854499]\n",
      "William Katz said: Habib released his notes on his approach to getting 2nd in the IceCube Kaggle competition that David Ackerman presented before:\n",
      "https://twitter.com/dr_hb_ai/status/1654636928958971905?s=20\n",
      "https://github.com/DrHB/icecube-journal\n",
      "DrHB on Twitter\n",
      "Curious about the dedication needed to achieve victory in a .@kaggle competition? In this repository, <https://t.co/oe0LYiCL1k>, I've documented all the experiments that culminated in my 2nd place win in the Neutrino Competition. \n",
      "This repo includes over 50+ distinct trials, such…\n",
      "DrHB/icecube-journal\n",
      "David Ackerman said: Intense!\n",
      "\n",
      "1683347789.136099\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683347789.136099]\n",
      "William Katz said: scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI\n",
      "By leveraging the exponentially growing single-cell sequencing data, we present the first attempt to construct a single-cell foundation model through generative pre-training on over 10 million cells. We demonstrate that the generative pre-trained transformer, scGPT, effectively captures meaningful biological insights into genes and cells.https://www.biorxiv.org/content/10.1101/2023.04.30.538439v1\n",
      "Others reacted to the previous message with eyes a total of 1 times.\n",
      "\n",
      "1683506561.078919\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683506561.078919]\n",
      "William Katz said: Donald Olbris I started modifying the FastAI IG wiki page to become the Applied Deep Learning IG wiki page. (Not sure if it would've been better to start new one and archive the old.) I see that changing the name has also changed the actual link to the page. Can we put in a redirection from the old page?\n",
      "\n",
      "1683508519.129329\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683508519.129329]\n",
      "William Katz said: I'm also thinking of renaming this Slack channel to #applied-deep-learning. I assume links will be broken but everyone who's subscribed will remain subscribed.\n",
      "\n",
      "1683545574.524639\n",
      "1683545601.201459\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683545574.524639]\n",
      "Donald Olbris said: Links within the wiki will not break. I think the wiki might even insert a redirect automatically, though I haven't tested that. If not, we could just recreate the old page with a link. There used to be a redirect macro, not sure if it's active right now.\n",
      "Donald Olbris said: Post after you rename and we can test and/or set that up.\n",
      "\n",
      "1683548245.757719\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1683548245.757719]\n",
      "Donald Olbris said: Ah, didn't see that you did the rename already. Confluence did create a nice \"maybe it's this page, which was recently renamed\" page. It's a manual click.\n",
      "\n",
      "1684157226.281579\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684157226.281579]\n",
      "Konrad Rokicki said: When is our next meeting? Calendar says tomorrow. I've been experimenting with building GPT-powered search engines using LlamaIndex/Weaviate and I could show a prototype. Would love to get some feedback at this early stage.\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "David Ackerman said: sounds cool!\n",
      "David Ackerman said: how hard is it to get started with gpt models?\n",
      "Konrad Rokicki said: You can just create a OpenAI key and immediately use their API to do prompt/response or get embeddings very easily. But I think the overall difficulty really depends on what you want to accomplish. I found that a lot of the open source libraries are still very provisional, and there are many limits to the APIs. Tomorrow I will talk about what I learned about existing frameworks, and some different strategies for search.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1684160490.533509\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684160490.533509]\n",
      "William Katz said: Good news: Habib has just confirmed he'll be presenting on May 23, 2 pm. He'll go through how he approached the IceCube Kaggle competition (holding 1st for a while and finishing 2nd) and the various tools he used. I'll see if we can bring in external speakers occasionally to show how they approached and implemented solutions to various problems. If you know someone who would be a good presenter, feel free to reach out to them directly and/or let me know so I can do the scheduling.\n",
      "Others reacted to the previous message with +1 a total of 3 times.\n",
      "\n",
      "1684249091.309579\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684249091.309579]\n",
      "William Katz said:  With the IG name conversion, it seems like some of the notifications are broken. We are indeed meeting today at 2 pm. Next week we'll be hosting Habib who has a great presentation planned.  Here's a synopsis of next week's meeting:\n",
      "I will take 30-40 minutes to talk about how ML projects are organized and how people in industry do experiments. This is a different topic from what we usually see when polished repositories are published.\n",
      "Here is a brief overview of the topics I will cover:\n",
      "How ML projects are organized\n",
      "The different stages of an ML project\n",
      "The tools and resources that are used\n",
      "How people in industry do experiments\n",
      "The different types of experiments that are conducted\n",
      "The methods that are used to evaluate experiments\n",
      "The challenges of conducting experiments\n",
      "Popular tools for ML projects\n",
      "Nbdev\n",
      "Hugging face transformer\n",
      "Hugging face accelerate\n",
      "Pytorch lightning\n",
      "Fastai\n",
      "Configuration managers\n",
      "Experiment trackers\n",
      "Advantages and when to use different tools\n",
      "The advantages of each tool\n",
      "When to use each tool\n",
      "Examples of how tools can be applied in real life\n",
      "Examples of how tools have been used to improve ML projects\n",
      "I will also mention briefly how it was applied to IceCube or to other challenges. \n",
      "\n",
      "David Ackerman said: this sounds like it could be of interest to a much wider audience than just us\n",
      "William Katz said: I'll post on CVML channel. Feel free to advertise though we will likely record and post to YouTube for this session.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1684260178.932429\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684260178.932429]\n",
      "Srini Turaga said: Can someone share the zoom link? Sorry\n",
      "Donald Olbris said: https://hhmi.zoom.us/j/95401304031?pwd=eUVkNFdodmRGVzViNHNRRzVaL1Y3dz09; it's also bookmarked in this channel.\n",
      "William Katz said: The Zoom link hasn't changed AFAIK.\n",
      "\n",
      "1684261116.451199\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684261116.451199]\n",
      "Stephan Preibisch said: https://arxiv.org/abs/2106.09685\n",
      "LoRA: Low-Rank Adaptation of Large Language Models\n",
      "An important paradigm of natural language processing consists of large-scale\n",
      "pre-training on general domain data and adaptation to particular tasks or\n",
      "domains. As we pre-train larger models, full fine-tuning, which retrains all\n",
      "model parameters, becomes less feasible. Using GPT-3 175B as an example --\n",
      "deploying independent instances of fine-tuned models, each with 175B\n",
      "parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\n",
      "LoRA, which freezes the pre-trained model weights and injects trainable rank\n",
      "decomposition matrices into each layer of the Transformer architecture, greatly\n",
      "reducing the number of trainable parameters for downstream tasks. Compared to\n",
      "GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\n",
      "parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\n",
      "performs on-par or better than fine-tuning in model quality on RoBERTa,\n",
      "DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\n",
      "training throughput, and, unlike adapters, no additional inference latency. We\n",
      "also provide an empirical investigation into rank-deficiency in language model\n",
      "adaptation, which sheds light on the efficacy of LoRA. We release a package\n",
      "that facilitates the integration of LoRA with PyTorch models and provide our\n",
      "implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\n",
      "<https://github.com/microsoft/LoRA>.\n",
      "\n",
      "1684292000.268419\n",
      "1684292042.829359\n",
      "--------------------------------------------------\n",
      "Document[channel=C041XB9U8BX,ts=1684292000.268419]\n",
      "Konrad Rokicki said: Here is my presentation from today about semantic search:\n",
      "<LLM Powered Search.pptx>\n",
      "Konrad Rokicki said: And initial proof of concept: https://github.com/JaneliaSciComp/gpt-semantic-search\n",
      "\n",
      "1683557183.323759\n",
      "--------------------------------------------------\n",
      "Document[channel=C045UGQB4LX,ts=1683557183.323759]\n",
      "Daniel Milkie said: First time globus user.  Made my first transfer (1.4 TB from nearline to UNC campus @ 22MB/s, 15 hours). Pretty good.\n",
      "Is there any way I can probe what limits the transfer rate? (network bandwidth on this end? their end? middle man?  storage speeds?)\n",
      "\n",
      "1683561645.039589\n",
      "1683561673.114399\n",
      "1683561684.247729\n",
      "1683561806.202129\n",
      "1683561848.623049\n",
      "1683561878.148639\n",
      "1683561936.668049\n",
      "1683561983.372669\n",
      "1683562041.011399\n",
      "1683562114.495669\n",
      "1683562160.882939\n",
      "1683562177.794369\n",
      "1683562277.416399\n",
      "1683562322.979339\n",
      "1683562362.778269\n",
      "1683562656.725579\n",
      "1683562703.132749\n",
      "--------------------------------------------------\n",
      "Document[channel=C045UGQB4LX,ts=1683561645.039589]\n",
      "Ken Carlile said: Hi Daniel, that's actually pretty good from what I've seen, assuming I'm understanding the parameters correctly. First off, was this a transfer to a Globus Connect Personal endpoint? From the logs, it looks like it was. I can see this in the Event log, on the last page: https://app.globus.org/activity/ab12b762-eb6b-11ed-ba3d-09d6a6f08166/events?page=188\n",
      "Ken Carlile said: If I look under the first Started, entry, I see this:\n",
      "Ken Carlile said: `{`\n",
      "`  \"type\": \"GridFTP Transfer\",`\n",
      "`  \"concurrency\": 4,`\n",
      "`  \"protocol\": \"Mode S\"`\n",
      "`}`\n",
      "Ken Carlile said: So this means that there are 4 streams going at one time from our Globus Connect Server cluster to a single GCP node, which is just fine and normal for this setup. However, if this was a GCServer->GCServer transfer, it would probably say something like concurrency: 8, parallelism: 8. Mode S is the slowest way Globus can send data.\n",
      "Ken Carlile said: Unfortunately, Mode S is the only thing available when the node receiving the data is running GCPersonal. (GCServer is very involved setup, so typically that would only be done on an institutional level)\n",
      "Ken Carlile said: If a GCPersonal node is sending data to a GCServer, the concurrency and threading would look different, potentially, and it would probably be running faster.\n",
      "Ken Carlile said: It is possible, but I haven't confirmed this, that GCPersonal->GCPersonal would be faster than GCServer->GCPersonal, and regardless, that would be slower than server->server\n",
      "Ken Carlile said: So the TL;DR of that chunk of info is that if at all possible, both sides should use their Institution's Globus installs rather than transferring directly to a laptop or desktop running GCPersonal for the best speed.\n",
      "Ken Carlile said: Another thing that I see from the transfer is that it was set up with the option `transfer new or changed files where the checksum is different (sync level 3)`\n",
      "Ken Carlile said: This option is fairly computationally intensive as both sides have to run checksums on the files on both ends. If this is not necessary (ie, if this is the first transfer and not a re-sync) I recommend against using this option. However, it is fairly important to use it if it is a re-sync.\n",
      "Ken Carlile said: `Verify file integrity after transfer` is a similar case, and again, a very necessary and highly recommended option! But it does add some extra time to the transfer.\n",
      "Ken Carlile said: I point these out because the reported speed in the Globus console is \"Effective Speed\"\n",
      "Ken Carlile said: This means taking the entire size of the transfer and dividing it by the time taken for the entire operation. If you look in the Event log under the Progress entries, you'll see the actual transfer rate for each chunk of files (or chunk of a file), e.g.:\n",
      "<image.png>\n",
      "Ken Carlile said: In this case, the on-the-wire transfer rate is 225MB/s, which is pretty good for commodity internet.\n",
      "Ken Carlile said: But because the system is using Mode S, it needs to do checksums, and it needs to do the verification, the total time elapsed makes the effective speed much lower.\n",
      "Ken Carlile said: I checked your logs to make sure there were no interruptions/faults, and it looks like this one was just fine. Globus is designed to be resilient to these things, but I have seen instances where the person receiving the data has closed their laptop for the night and then opened it in the morning--the transfer then resumes just fine, but the effective speed is drastically reduced, because as far as Globus is concerned, the transfer's just taking that long. Other instances have included using an external USB drive on the receiving end that is, perhaps, a little hinky, and Globus will have to retransmit a chunk of data or an entire file (or more), which will again increase the overall time. The only way I found out about that was Globus engineers looking at the internal logging on the transfer, so I don't think it's something you would be able to see.\n",
      "Ken Carlile said: Please let me know if you have any questions or would like me to go more into depth on any of this.\n",
      "\n",
      "1683565255.038359\n",
      "--------------------------------------------------\n",
      "Document[channel=C045UGQB4LX,ts=1683565255.038359]\n",
      "Daniel Milkie said: got it.  This is helpful.\n",
      "226 Mbs in the photo is 28 MB/s which is fairly close to 22MB/s, so I agree there is some computation overhead but it isn't a severe penalty.\n",
      "\n",
      "1683566524.665709\n",
      "--------------------------------------------------\n",
      "Document[channel=C045UGQB4LX,ts=1683566524.665709]\n",
      "Ken Carlile said: I think it's actually MBps, rather than Mbps (for some reason it's all lower case, but this corresponds to what I've seen before)\n",
      "\n",
      "1683638933.944649\n",
      "--------------------------------------------------\n",
      "Document[channel=C045UGQB4LX,ts=1683638933.944649]\n",
      "Daniel Milkie said: Transfer took 1.5 TB in 15 hours, so that's an aggregate of 100 GB/hour = 28 MB/s. But I'm still happy.\n",
      "\n",
      "1683405117.302599\n",
      "--------------------------------------------------\n",
      "Document[channel=C049U3BDYPL,ts=1683405117.302599]\n",
      "Geoffrey Meissner said: Can anyone explain the appeal of it over Mastodon?\n",
      "Philip Hubbard said: Maybe it sounds cool because it is associated with Jack Dorsey, a founder of Twitter?\n",
      "Geoffrey Meissner said: For me that's a reason to avoid it.\n",
      "William Katz said: The advantage is primarily a better protocol for decentralized social networks. In particular, you shouldn’t have the problems of Mastodon where moving between providers disrupts your social network and also makes you dependent on the provider’s moderation choices.\n",
      "https://blueskyweb.xyz/blog/10-18-2022-the-at-protocol\n",
      "The AT Protocol\n",
      "In the spring, we released “ADX,” the very first iteration of the protocol. Over the summer we improved ADX’s design, and today we’re sharing a preview of what’s to come.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "William Katz said: There's also this discussion on the AT protocol vs ActivityPub (what Mastadon is based on), including one of the main engineers on BlueSky.\n",
      "https://news.ycombinator.com/item?id=35881170\n",
      "If you really want to get into the guts of the various decentralized social ecosystems, there's this summary (from earlier in BlueSky development) of what's out there now and how all the parts come together:\n",
      "https://gitlab.com/bluesky-community1/decentralized-ecosystem/-/blob/master/README.md\n",
      "README.md · master · bluesky community / decentralized-ecosystem · GitLab\n",
      "<http://GitLab.com|GitLab.com>\n",
      "Geoffrey Meissner said: It's hard to imagine this sort of thing being the reason for their popularity, but I'll look into it more.\n",
      "William Katz said: Yeah, the technical stuff gets trumped by ease of use by the general public, which is impacted by the technical underpinnings but also coherent/effective management and $$ backing it. Example is the highly fractured onboarding of new Mastodon users where people signed up to various servers, all run by different people with different objectives and $$ for infrastructure. Moving from one server to another was not simple nor painless, so it gave a bad initial impression of the Mastodon system. In contrast, the bulk of the new BlueSky users are going through a single registration chokepoint with expectation of easy migration if necessary to other providers.\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "Philip Hubbard said: Did anyone join Bluesky, ahead of the current waiting list of 1.2 million people?\n",
      "\n",
      "1681494793.312299\n",
      "--------------------------------------------------\n",
      "Document[channel=C04UUTQVB61,ts=1681494793.312299]\n",
      "William Katz said: I like the idea of a \"first contact point\" because of the (overwhelming) number of communication channels that exist. It would be very useful to have some kind of decision tree to guide people to the appropriate resource.  I made an attempt to sketch out current systems at use in Janelia and was thinking how the wiki fit in. This is my list from https://wikis.janelia.org/pages/viewpage.action?pageId=70778916:\n",
      "Possible avenues of communication:\n",
      "This wiki ((based on Confluence)http://my.hhmi.org/(based on Microsoft Sharepoint)http://janelia.org websiteSlack (the https://hhmi.enterprise.slack.com/ as well as self-owned workspaces)https://github.com/janeliascicomp(allows repositories to be created with discussion forums, issue tracking, source code for software, proj management, etc).https://issues.hhmi.org/, used heavily at HQ for both tech support and proj management.Microsoft equivalents used by HQ (pointed out by Don):Slack > TeamsDropbox > One Drive.https://www.dropbox.com/work including https://paper.dropbox.com/ for collaborative documents.http://docs.google.com/ for collaborative documents.https://hhmi.zoom.us/ rooms for video conferencinghttps://hhmi.hosted.panopto.com/ for video meeting archivesEmail mailing lists.WhatsApp for ad-hoc Janelia groups like frisbee, table tennis, concerts in DMV area, etc.\n",
      "William Katz said: My feeling is that some ChatGPT mechanism of search would be ideal if we could point it toward the mass of Janelia information on sharepoint, wiki, and slack. At least the sharepoint part could be achievable since Microsoft is integrating OpenAI tech with their platforms. How nice would it be to say \"is there a frisbee club at Janelia?\" and receive the WhatsApp link and some info on last meeting? Or \"I'm interested in learning about AI\" and get a synopsis of CVML group, any Sci Comp Software IG, etc. Which makes me wonder about where to store new info, and if we shouldn't assume something like that is coming and put it in a Microsoft-associated system... like Github.\n",
      "\n",
      "1682087072.502779\n",
      "--------------------------------------------------\n",
      "Document[channel=C04UUTQVB61,ts=1682087072.502779]\n",
      "Virginia Scarlett said: Don, Ricky, Kelsey, Katy and I had a productive chat this morning. Katy and Kelsey seem pretty open to a wiki facelift, provided it doesn't take too much of their time. Ricky brought up some infrastructure concerns, foreboding a troubling trend in which Janelia's aggressive security infrastructure gets in the way of accessibility.\n",
      "I propose our next meeting be some form of a https://www.holacracy.org/governance-meetings. We don't have to do every single round described in that link; we could just do presentation/objection/integration, or even simply presentation/feedback.\n",
      "Since participation on the pinned Google Doc has been low, I would like to use that Google Doc to instead craft a proposal we can discuss together as a group. Currently, I'm envisioning three aspects to this proposal:\n",
      "(1) a statement of purpose,\n",
      "(2) a \"tipping point\" that marks the point when we decide to abandon Confluence and \"go rogue\" with some other platform such as Notion or Github Pages, and\n",
      "(3) a few experiments that are safe to try, in which we could begin improving the wiki without investing too much energy in a potentially sinking ship.\n",
      "Anyone is welcome to collaborate, or give feedback, on the Google Doc proposal as I'm working on it. As always, if you have feedback on this idea, you can DM me with your thoughts.\n",
      "Konrad Rokicki said: Re: Janelia's aggressive security infrastructure\n",
      "We had a very productive conversation with the IT Security team yesterday and set some things in motion to increase communication between our teams. I think this is trending slightly more positively now, but there's a lot of work left to do. If anyone has specific IT concerns please bring them to Stephan or myself.\n",
      "Konrad Rokicki said: Re: \"tipping point\"\n",
      "I don't think everything needs to be in the wiki -- for instance, we also use GitHub pages and wikis for project documentation. But there is a lot of critical information in the wiki that would be very difficult to move (for example Server tracking information), so abandoning the wiki entirely would be quite challenging.\n",
      "\n",
      "1683574934.855629\n",
      "--------------------------------------------------\n",
      "Document[channel=C04UUTQVB61,ts=1683574934.855629]\n",
      "Virginia Scarlett said: Hi folks! Thanks for your contributions to the proposed statement of purpose. (Pinned to this channel, and linked in the survey.) Let's meet and talk about this proposal, or some parts of it. https://forms.gle/UiDr99X4gf7r8yJ68 you can fill out to help plan our next meeting. The survey contains a link to a Doodle. Thanks!!\n",
      "Planning next wiki meeting\n",
      "This anonymous survey will help us plan our next meeting, in which we'll discuss the proposal we've drafted HERE.\n",
      "\n",
      "1683579124.741659\n",
      "--------------------------------------------------\n",
      "Document[channel=C04UUTQVB61,ts=1683579124.741659]\n",
      "Mark Kittisopikul said: There's still a lot of options. I got halfway through the options before I had to redirect my attention elsewhere. Could we consider using the Outlook scheduling assistant?\n",
      "https://support.microsoft.com/en-us/office/use-the-scheduling-assistant-and-room-finder-for-meetings-in-outlook-2e00ac07-cef1-47c8-9b99-77372434d3fa\n",
      "Virginia Scarlett said: The Doodle has been updated to offer a more manageable number of options.\n",
      "Others reacted to the previous message with smile a total of 1 times, and with +1 a total of 1 times.\n",
      "Virginia Scarlett said: Maybe we can talk about this IRL. I used outlook to find times that should work for everyone, and then I manually put those times into Doodle. I wanted a poll where people can confirm their availability.\n",
      "Mark Kittisopikul said: I remembered how to cross reference my Outlook calendar with Doodle and filled it out.\n",
      "Mark Kittisopikul said: \n",
      "<image.png>\n",
      "\n",
      "1683747942.936269\n",
      "--------------------------------------------------\n",
      "Document[channel=C04UUTQVB61,ts=1683747942.936269]\n",
      "Virginia Scarlett said: I've booked a room for Tue May 23 at 11am! We'll do a hybrid meeting. React to this message with an emoji to receive an outlook calendar invitation.\n",
      "Others reacted to the previous message with + a total of 3 times.\n",
      "\n",
      "1684259866.667699\n",
      "1684259923.429049\n",
      "--------------------------------------------------\n",
      "Document[channel=C057Z7J7F29,ts=1684259866.667699]\n",
      "Tiago Ferreira said: Cristian Goina, Konrad Rokicki: just to let you know that the workflow succeeded with the new home folder!\n",
      "Others reacted to the previous message with tada a total of 2 times.\n",
      "Konrad Rokicki said: multfish? \n",
      "Tiago Ferreira said: renamed \n",
      "\n",
      "1684260369.113449\n",
      "--------------------------------------------------\n",
      "Document[channel=C057Z7J7F29,ts=1684260369.113449]\n",
      "Tiago Ferreira said: Yuhan Wang, question on pipeline parameters:\n",
      "What exactly is \"bleed channel\" [Channel (other than DAPI) that needs bleedthrough correction] . what does the correction do?Which acquisition do you guys typically set for \"fixed reference\"? Round 1 or middle-round?\n",
      "\n",
      "1684262955.552019\n",
      "--------------------------------------------------\n",
      "Document[channel=C057Z7J7F29,ts=1684262955.552019]\n",
      "Yuhan Wang said: Hi Tiago. Nice!! Congratulations.\n",
      "What exactly is “bleed channel” [Channel (other than DAPI) that needs bleedthrough correction] . what does the correction do? We used to do two track imaging and DAPI is imaged together with the AF546 channel. In this case, we observed bleed through from DAPI channel into AF546 channel. So we corrected for the bleed through. The heuristic cross-talk correction has the following steps:\n",
      "1) estimate background intensity in both DAPI channel (BDAPI) and Red channel (BRed) (heuristic estimation: fluorescence intensity at 1st percentile) \n",
      "2) estimate and identify pixels with very bright DAPI signals in the DAPI channel (heuristic estimation: pixels with intensity over 99.5th percentile) \n",
      "3) determine the fluorescence intensity of the pixels identified in step 2) in DAPI channel (IDAPI) and red channel (Ired) pixel by pixel \n",
      "4) the cross-talks are calculated as following (pixel by pixel): \n",
      "cross-talks=Ired -Bred /IDAPI-BDAPI \n",
      "5) the cross-talk factor (C) is estimated as the median of the measurement in step 4) and applied to the red channel image pixel by pixel: \n",
      "Corrected image =Ired -Bred -C*(IDAPI-BDAPI) \n",
      " \n",
      "Please note that this method might not generalize very well. If there’s a large fraction of high intensity autofluorescence signals in the DAPI channel, that might affect the estimation.  \n",
      "\n",
      "1684263357.767089\n",
      "--------------------------------------------------\n",
      "Document[channel=C057Z7J7F29,ts=1684263357.767089]\n",
      "Yuhan Wang said: Which acquisition do you guys typically set for “fixed reference”? Round 1 or middle-round?It did not seem to matter for us. I have chosen R3, R9, R11, etc. I usually choose the one with the highest imaging quality (for segmentation purposes) and relatively small but desired image volume (less dead image volumes) to minimize data size for downstream steps.\n",
      "Others reacted to the previous message with +1::skin-tone-3 a total of 1 times.\n",
      "\n",
      "ERROR: no such user U029DL61X5L\n",
      "ERROR: no such user U019XPGTGRL\n",
      "ERROR: no such user UGF6L2YUS\n",
      "ERROR: no such user U029DL61X5L\n",
      "ERROR: no such user UGF6L2YUS\n",
      "ERROR: no such user U04PR67HG3B\n",
      "1681051252.792059\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1681051252.792059]\n",
      "Hoss said: have you tried it? Any issues with chromatix?\n",
      "I'm having a lot of issues on my new 4090 and the older cuda version so ugrading to 12 might help, but I don't wanna break chromatix\n",
      "diptodip said: Not yet, I'll let you know when I do\n",
      "\n",
      "1681502936.729849\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1681502936.729849]\n",
      "Gert-Jan Both said: Alas,\n",
      "Many JAX library functions are not yet supported.\n",
      "\n",
      "1681823970.606459\n",
      "1681824021.400479\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1681823970.606459]\n",
      "Gert-Jan Both said: UGF6L2YUS I don't think we can support Stokes parameters and Mueller calculus on the short term unfortunately. If I understand correctly, Mueller calculus deals with intensities (incoherent light) instead of fields so supporting that would require a pretty major reworking\n",
      "Xi said: I see, but maybe we can sum up a brunch of random fields with specific amplitude and phase to simulate the incoherent situation? I'm not using the Stokers vector and Muller matrix now in my simulation. I think it is interesting to implement it in the future.\n",
      " said: What is our policy going to be for incoherent light anyway? Do you think we would incorporate that at some point?\n",
      "Gert-Jan Both said: It's tough... we haven't considered it. We support it more or less - you can model it as a batch with random phase and average over the batch, but that's not ideal in many cases and doesn't support things like mueller calculus\n",
      "Gert-Jan Both said: It hasn't really been requested by anyone and I'd like to read up on the math, so I think parking it for now and reconsidering it in a few months would be best (unless someone drastically needs it ofcourse)\n",
      " said: Yes, even without polarization though, I would want to use incoherent illumination. I guess it's possible to use the autocorrelation but if there's a way to somehow incorporate it would be so nice..\n",
      " said: Oh okay I am requesting it :)\n",
      "Others reacted to the previous message with smiling_face_with_tear a total of 1 times.\n",
      " said: But I would be happy to help out in integrating it\n",
      "Gert-Jan Both said: Do you have a source for what you're trying to do?\n",
      " said: Yes it's in Goodman, I don't have it open rn, but I can get back with the section numbers. The idea is that we take autocorrelation of the Fourier Transform of the PSF.\n",
      "Gert-Jan Both said: That's supported no?\n",
      " said: I am not 100% sure how that will go. I have not tried to do incoherent imaging yet, but I have been thinking about it. I will send you those sections and I will actually try doing it and let you know how it goes.\n",
      "Gert-Jan Both said: Right, so to put it clearly: we support incoherent imaging as the average over coherent fields. We do not support the propagation / modelling / ... of incoherent fields\n",
      " said: Yep\n",
      "\n",
      "1681838752.260339\n",
      "1681838789.601889\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1681838752.260339]\n",
      "Gert-Jan Both said: Hi , I've been working on the documentation today. As promised, there's now some documentation for the polarisation, please find it here https://chromatix.readthedocs.io/en/latest/polarisation/\n",
      "Others reacted to the previous message with tada a total of 3 times.\n",
      "Gert-Jan Both said: There also more documentation on the training, including examples using Optax (adam and cousins) and Jaxopt (LBFGS, Conjugate gradient, etc): https://chromatix.readthedocs.io/en/latest/training/\n",
      "Others reacted to the previous message with tada a total of 3 times.\n",
      "\n",
      "1681840832.187269\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1681840832.187269]\n",
      "Gert-Jan Both said: Hi , we've discussed having a monthly meeting and here's the final poll: does the first Tuesday of the month at 13h east-coast time (10h for west coast, 19h for Europe) work for you? Please add a  if you're available. If okay, the first meeting would be Tuesday 2nd of May.\n",
      "Others reacted to the previous message with +1 a total of 3 times, and with +1::skin-tone-3 a total of 1 times.\n",
      "schneiderm2 said: In principle, the first Tuesday of the month at 1pm EST should work for me. However, I am already fully booked the whole afternoon of May 2 :/\n",
      "\n",
      "1682427001.522539\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1682427001.522539]\n",
      "Srini said: I  the new contributors list on the chromatix README! I wonder if we could also add github handles to everyone so that their contributions listed by github handle in https://github.com/TuragaLab/chromatix/graphs/contributors can be connected to their real names?\n",
      "Others reacted to the previous message with +1 a total of 1 times, and with heart a total of 1 times.\n",
      "diptodip said: Yes I was just being lazy  But also I think not everybody that contributed shows up on that list for various reasons\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1682446202.170999\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1682446202.170999]\n",
      "Gert-Jan Both said: U04PR67HG3B For the Zernike and Seidel Aberrations you take as input the shape, spacing, wavelength, focal length etc... We've made some changes so we can change all these parameters to just a radius R and a `Field` - that's what you originally wanted no? So okay with you if we replace it by that?\n",
      "diptodip said: I think we still want to keep `n`, `f`, and `NA` instead of replacing them with `R`. What we'll be removing --- from all the phase mask functional initializers --- are the `shape`, `spacing`, and `wavelength` arguments. These can be replaced by `Field`.\n",
      "Gert-Jan Both said: I think we still want to keep `n`, `f`, and `NA` instead of replacing them with `R`.why?\n",
      "diptodip said: Because we're usually defining phase masks in terms of pupil coordinates, and you'll likely have these numbers on hand rather than the actual size of the pupil. We can omit these arguments where we don't need them but for most of the phase masks that we've defined so far it seems a more natural description. That way I won't have to calculate the radius of the pupil myself first.\n",
      "Gert-Jan Both said: mmmmmm... I'm thinking - what if we save the pupil as a property of the field? That makes sense no, and we could get the normalised coordinates straight away as it's a very natural set of coordinates\n",
      "diptodip said: That doesn't really make sense, because most `Field`s are not at the pupil. We only need this information in specific situations, like getting the coordinates for these phase masks.\n",
      "diptodip said: It could be something we add to the grid utilities, though.\n",
      "diptodip said: A function that takes a `k_grid`, `n` and `NA` and returns the pupil normalized coordinates.\n",
      "Gert-Jan Both said: Sure but your (f, n, NA) are related to a lens no? It would be essentially keeping track of the non-zero size of the field\n",
      "Gert-Jan Both said: A function that takes a `k_grid`, `n` and `NA` and returns the pupil normalized coordinates.This is already implemented...\n",
      "diptodip said: Where?\n",
      "diptodip said: Ah right grid_spatial_to_pupil\n",
      "diptodip said: Maybe we should move that to the grid utilities.\n",
      "Gert-Jan Both said: I think we need to reorganize the utilities in general, and have a look at ops too\n",
      "diptodip said: Yeah I think we shouldn't have both `ops` and `utils`\n",
      "Gert-Jan Both said: It's not super clear what goes where\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "schneiderm2 said: Yes exactly, in principle the radius and shape of the field is enough information to calculate the Zernike aberrations. Ideally, the calculation of those parameters from n, f and NA would occur outside of the Zernike functions. This should be the case for the other phase masks, too. I created a function (now called grid_spatial_to_pupil) that avoids duplicated code for that calculation inside the phase mask functions.\n",
      "In principle, I would be fine if that part is changed, but it seems to cause problems somewhere else? Dip, you mentioned that then we would have to calculate the radius of the pupil ourselves. Can that calculation not be put somewhere else?\n",
      "diptodip said: I think the question is do we want to have users input a pupil radius or input some attributes of their system? The calculation could go anywhere but to calculate it, I would have to write down my f, n, and NA anyway. So to me it seems somehow more obvious to just type in these values. The shape and spectrum can be gotten from the `Field` of course.\n",
      "diptodip said: I think what we could do is to get rid of `f` from these calculations, since we have `k_grid` on the `Field` now. Never mind, don't think that can work.\n",
      "\n",
      "1682447427.547149\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1682447427.547149]\n",
      "Gert-Jan Both said: Hi  you should've just received the invitation to our monthly round-table - starting next week with the first one! Let me know if you didn't get and I'll add you to the list!\n",
      "Others reacted to the previous message with +1 a total of 3 times.\n",
      "\n",
      "1683028243.542309\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1683028243.542309]\n",
      "Gert-Jan Both said: Hi , short reminder that we're meeting today at at 13h east-coast time for our first Chromatix round-table! This is our agenda:\n",
      "Developer update:Integration of hackathon contributionsPolarisationSmall changes and new featuresFeedback - what features to prioritise next? What works well (and what doesn't)?Experiences - what have you been building?Questions - how do I ...?For those of you who can't make it we'll record the meeting. Hope to see many of you!\n",
      "Others reacted to the previous message with +1 a total of 2 times.\n",
      "\n",
      "1683041497.720939\n",
      "1683041504.894859\n",
      "1683041510.146389\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1683041497.720939]\n",
      " said: Unfortunately, I cannot make the round table.\n",
      " said: but Hoss and Changjia will be there.\n",
      "Others reacted to the previous message with +1 a total of 3 times.\n",
      " said: I will get an update form them.\n",
      "\n",
      "1683144735.784499\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1683144735.784499]\n",
      "schneiderm2 said: Do you have a link to yesterday's recording? :)\n",
      "Gert-Jan Both said: And here's the full recording if you prefer that over Dips pretty concise notes \n",
      "https://hhmi.zoom.us/rec/share/nZxWCt5ocb-wPG91P6u0DOOhse_qA1HNkb3SVufG9DqJD7U5QPTElqmQ7ZBxSqWZ.Gnu8x2gBGbo-NKDB\n",
      "Passcode: YYs.5QV!\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "1683146007.335059\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1683146007.335059]\n",
      "diptodip said: U029DL61X5L should have the link since it was his Zoom room.\n",
      "Here's a summary:\n",
      "We'll be moving the library to its own GitHub organization soon, which will hopefully make collaboration easier. But it does mean everyone will have to update their GitHub remotes.We've got some basic support for polarization, and U019XPGTGRL and UGF6L2YUS will be trying this out. We still need to add support for rotating elements relative to the propagation axis.I've made some changes to the Microscope so that they plug in a PSF module and a Sensor module, for which I'll be adding some more involved examples soon (instead of just `holoscope.ipynb`).U029DL61X5L reworked specifying trainable parameters so now you could initialize something with a function (e.g. a phase mask initialization function) without having that become trainable. Now you'll always have to wrap using `trainable` to mark something as trainable explicitly. Previously initializing something as a callable was coupled with marking that as a trainable parameter.We'll investigate why `Field`s can't be `vmap`ped, because this should be fixable. That way if you want to propagate to multiple `z` values and have multiple batches of these `z` values you won't have to write a wrapper function that creates a `Field` internally.After adding support for polarization, arbitrary batch dimensions, and the trainable rework, the public part of the library should start to become more stable. We have just a few more things to polish, especially documentation, before this is actually true .\n",
      "Others reacted to the previous message with +1 a total of 5 times, and with +1::skin-tone-3 a total of 1 times.\n",
      "\n",
      "1683489250.773469\n",
      "--------------------------------------------------\n",
      "Document[channel=C02K252DJ86,ts=1683489250.773469]\n",
      "diptodip said: We have a new 101 that talks a little bit about the differences between using `OpticalSystem` and `Microscope`: https://chromatix.readthedocs.io/en/latest/101/\n",
      "And we also have vmappable `Field`s: https://chromatix.readthedocs.io/en/latest/higher_rank/#combining-higher-rank-fields-with-vmap\n",
      "\n",
      "Loaded 185 documents\n"
     ]
    }
   ],
   "source": [
    "# Create Documents from cached Slack logs\n",
    "documents = []\n",
    "for channel_name in channel2id.keys():\n",
    "    for doc in index_channel(channel_name):\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6927a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo-0301\")\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "embed_model = LangchainEmbedding(OpenAIEmbeddings())\n",
    "\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d4dee433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n"
     ]
    }
   ],
   "source": [
    "# Calculate embedding for all of the documents and save them into Weaviate\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "class_prefix = \"Slack\"\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=class_prefix)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# persists the vector_store into Weaviate\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "# persist the docstore and index_store\n",
    "# this is currently required although in theory Weaviate should be able to handle these as well\n",
    "storage_context.persist(persist_dir='../storage/index/slack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d39dbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk import WebClient\n",
    "slack_token = os.environ.get('SLACK_TOKEN')\n",
    "client = WebClient(token=slack_token)\n",
    "res = client.api_test()\n",
    "if not res[\"ok\"]:\n",
    "    raise ValueError(f\"Error initializing Slack API: {res['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7dd53304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_nodes(nodes):\n",
    "    docs_ids = set()\n",
    "    unique_nodes = list()\n",
    "    for node in nodes:\n",
    "        if node.node.ref_doc_id not in docs_ids:\n",
    "            docs_ids.add(node.node.ref_doc_id)\n",
    "            unique_nodes.append(node)\n",
    "    return unique_nodes\n",
    "        \n",
    "def get_message_link(channel, ts):\n",
    "    res = client.chat_getPermalink(channel=channel, message_ts=ts)\n",
    "    if res['ok']:\n",
    "        return res['permalink']\n",
    "\n",
    "def print_response(response, node_text=False):\n",
    "    print(response.response)    \n",
    "    for node in get_unique_nodes(response.source_nodes):\n",
    "        channel_id = node.node.extra_info['channel']\n",
    "        ts = node.node.extra_info['ts']\n",
    "        print(get_message_link(channel_id, ts))\n",
    "        if node_text:\n",
    "            print(node.node.text)\n",
    "        \n",
    "def query(question, n=5, node_text=False):   \n",
    "    query_engine = index.as_query_engine(similarity_top_k=n)\n",
    "    res = query_engine.query(question)\n",
    "    print_response(res, node_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "25d748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index import ResponseSynthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "synth = ResponseSynthesizer.from_args()\n",
    "\n",
    "# construct query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synth,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2278b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is recommended to submit everything at once, as there is a limit on how many GPUs can be used at one time and it is programmatic. The current limit is 150, but it may change without notice. If the limit is exceeded, new jobs will not start until the number of GPUs being used is below the limit. However, it is also recommended to submit jobs in the most logical work unit.\n",
      "https://janelia-dev.slack.com/archives/C0128K68NE5/p1682018703146099?thread_ts=1682018703.146099&cid=C0128K68NE5\n",
      "William Patton said: whats the best practice on large cluster jobs? I have a job that will take approximately 8 days using all 248 gpu_rtx nodes. Its a blockwise convolutional neural network prediction job so its embarrassingly parallel and scales almost linearly with number of gpus. Is it better to take 120 for 16 days ish or somewhere in between?\n",
      "Ben Arthur said: how long does each of your jobs take?\n",
      "William Patton said: It's just one job. I just need to start a bunch of workers to run prediction in parallel.\n",
      "Ben Arthur said: how long does each worker take to process one task?\n",
      "William Patton said: Oh only a few seconds per block\n",
      "Ken Carlile said: There is a limit on how many gpus you can use at one time. It's programatic, so just submit everything at once.\n",
      "William Patton said: Ah whats the limit?\n",
      "Ken Carlile said: the limit is 150\n",
      "Ken Carlile said: sorry, had to look that up\n",
      "William Patton said: Ok, perfect. I'll stick to 150 then\n",
      "Robert Lines said: That is the limit today.  It can be varied based on the mix of jobs running but generally it is at ~80% of the total gpus.  Also of note there are only 176 gpus available in the gpu_rtx queue.  22 nodes * 8 gpus.  1 of the rtx2080ti nodes is upgraded to OL9 and in the test queue.\n",
      "William Patton said: Oh I see. If I'm using ~80% and cluster usage goes up will it kill any of my workers?\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C0128K68NE5/p1682019947863859\n",
      "Robert Lines said: It won't kill your jobs we just may reduce the cap to relieve a backlog should it come up.  Right now it is a little higher at ~86% of the gpus because we had another node fail recent and the upgraded os node being out of the queue.  But there hasn't been high contention to encourage reducing it.  All of that is to say that the 150 number may change without notice but it won't kill your jobs if you end up exceeding the limit it will just not start new jobs until you are below the limit.\n",
      "I would encourage you to submit the jobs in the most logical work unit.  Though that may not fit if you are just running a bunch of workers who are waiting around to grab tasks off a work queue.\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C0128K68NE5/p1682020310047969\n",
      "William Patton said: Exactly, it's just a bunch of workers I'm starting that will be grabbing tasks, (in this case just spatial blocks) to read, predict, write.\n",
      "No point in trying to start more workers than the limit if they will just be sitting waiting for a gpu until the rest of the workers have finished the task queue.\n",
      "William Patton said: And it sounds like the only down side to requesting all 150 currently available would be that if one fails, it might not be able to restart\n",
      "Robert Lines said: yep.  If a worker dies does it have to start a new one or will it continue with n-1 workers?  And does it have to have all N workers running before it starts processing?\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C011W6YDV99/p1683909261450489?thread_ts=1683909261.450489&cid=C011W6YDV99\n",
      "Brenda Arevalo said: JWST has selected the 2nd year study roster & published the winners. I managed Grants & Contracts at UNC some years ago & helped PIs create abstract catalog pages. When competing against 1000s+, creative titles stand out-like some did here: Crouching Galaxy, Hidden Stars…  Solving a Solar Neighborhood Crime Scene…Shaken and Stirred: Shocks and Turbulence… To be or not to be in equilibrium… Yale will be studying runaway SMBHs (super massive black hole). They think they found the markers of one. It’s been 50+ years since the theory was proposed & JWST is giving them a chance to prove it, should be interesting. https://www.stsci.edu/files/live/sites/www/files/home/jwst/science-execution/approved-programs/general-observers/cycle-2-go/_documents/jwst-cycle2-general-observer-abstracts.pdf\n",
      "Others reacted to the previous message with telescope a total of 2 times, and with rolling_on_the_floor_laughing a total of 1 times.\n",
      "Davis Bennett said: \"A Hot View of Cold Gas\" \n",
      "Others reacted to the previous message with joy a total of 2 times.\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C041XB9U8BX/p1684160490533509\n",
      "William Katz said: Good news: Habib has just confirmed he'll be presenting on May 23, 2 pm. He'll go through how he approached the IceCube Kaggle competition (holding 1st for a while and finishing 2nd) and the various tools he used. I'll see if we can bring in external speakers occasionally to show how they approached and implemented solutions to various problems. If you know someone who would be a good presenter, feel free to reach out to them directly and/or let me know so I can do the scheduling.\n",
      "Others reacted to the previous message with +1 a total of 3 times.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"Should you limit your cluster jobs or submit everything at once?\", node_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e46d305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some interesting software packages that people are using include tmux, screen, iTerm2, X11 forwarding, NoMachine, conda, micromamba, and GNU Guix. Other interesting projects mentioned include open_llama, RedPajama dataset, Meerkat visualization tool, and the Stanford Hazy Research group.\n",
      "https://janelia-dev.slack.com/archives/C0128K68NE5/p1684244275308909?thread_ts=1684244275.308909&cid=C0128K68NE5\n",
      "Davis Bennett said: besides myself, how many people use https://en.wikipedia.org/wiki/Tmux?\n",
      "Tmux\n",
      "tmux is an open-source terminal multiplexer for Unix-like operating systems. It allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time. It can also be used to detach processes from their controlling terminals, allowing remote sessions to remain active without being visible.\n",
      "Others reacted to the previous message with + a total of 4 times.\n",
      "Jody Clements said: I still use screen, too much muscle memory to switch over\n",
      "Mark Kittisopikul said: I generally go to screen first\n",
      "John Bogovic said: I use screen for the same reason as Jody Clements\n",
      "Mark Kittisopikul said: The main reason I need a split terminal is usually to look at two editor buffers, and for that I usually use within vim.\n",
      "Others reacted to the previous message with vim a total of 3 times.\n",
      "Konrad Rokicki said: I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.\n",
      "Others reacted to the previous message with exploding_head a total of 1 times, and with octopus a total of 1 times.\n",
      "Kristin Branson said: thanks, seems like a nice alternative to screen, will try it out!\n",
      "Stuart Berg said: +1 for iTerm2, though it sounds like I'd need to up my game to match Konrad.\n",
      "I like it for its \"triggers\" capability, which lets you type something into your remote session that triggers a command on your local MacBook.\n",
      "Adam Taylor said: I have used screen/tmux in the past, but don't use it regularly.  I prefer to use X11 forwarding or NoMachine.\n",
      "Others reacted to the previous message with joy a total of 1 times.\n",
      "Ken Carlile said: I've used all 3. tmux occasionally when I remember, screen more often, and iTerm2 exclusively for when I'm working on nearline or the backup qumulo. i do batches of 20 nodes at once\n",
      "Ken Carlile said: Even at 4K, my screen isn't big enough to make it feasible to do more at a time\n",
      "Davis Bennett said:  I use iTerm2 for split screen terminals. Sometimes I have 8 terminals controlling 8 servers, all synchronized on a single input so I can deploy to 8 servers at once.Konrad Rokicki can we get a demo of this?\n",
      "Jody Clements said: I used to use csshX to input commands into multiple terminals at the same time, but iTerm2 seems like a better solution now.\n",
      "Ken Carlile said: just make sure the response you get on all of the terminals is the same...\n",
      "Others reacted to the previous message with +1 a total of 1 times.\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C041XB9U8BX/p1682016202519839?thread_ts=1682016202.519839&cid=C041XB9U8BX\n",
      "Philip Hubbard said: Update: while a basic \"hello world\" console application made with Visual Studio Community 2019 still gets quarantined by CrowdStrike, one built with Visual Studio Community 2022 works.  So I guess 2019 must be considered unsafe.  But it is just a guess: several of us had a meeting this morning with folks from the Security team and from CrowdStrike, and CrowdStrike refuses to reveal why any particular executable is quarantined, saying that it is \"proprietary ML decisions\".  Yay.\n",
      "Mark Kittisopikul said: Maybe hackers (crackers?) use old tools? Honestly, it sounds to me like CrowdStrike might not even know why it was marked unsafe. The world is getting kind of scary.\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C02K818Q3B6/p1684250786648799\n",
      "Mark Kittisopikul said: Right, so tool1 may be a short script that calls `conda run -n env1 tool1` . tool2 maybe a short script that is `mvn exec:java -Dexec.mainClass=\"org.example.Tool2\"`, and tool3 is a script that calls `conda run -n env3 mvn exec:java -Dexec.mainClass=\"org.example.Tool3WithNativeDeps\"`  . The scripts all live in your `~/bin` which is on your `PATH`\n",
      "Mark Kittisopikul said: Most people `conda activate some_env` but you don't actually need to do that to just run one of the tools\n",
      "Stephan Saalfeld said: that would be fine for most things I guess, there are also snap, flatpak, and appimage?\n",
      "Mark Kittisopikul said: https://guix.gnu.org/ ?\n",
      "GNU Guix transactional package manager and distribution — GNU Guix\n",
      "Guix is a distribution of the GNU operating system.\n",
      " Guix is technology that respects the freedom of computer users.\n",
      " You are free to run the system for any purpose, study how it\n",
      " works, improve it, and share it with the whole world.\n",
      "Stephan Saalfeld said: dunno, starts with a lot of propaganda talk, probably cool?\n",
      "Stephan Saalfeld said: it could become confusing, because people typically expect that their conda environment is what they made it, if we now have tools that secretly change the environment, that could be confusing, what does conda best practices say about this?\n",
      "Mark Kittisopikul said: I typically do one tool (a process) in it's own environment\n",
      "Mark Kittisopikul said: If they need to live within the same process, then they have to coexist in the same conda environment.\n",
      "Mark Kittisopikul said: If not, then they can be in separate environments\n",
      "Mark Kittisopikul said: I know some folks nest their environments as well. `env{1,2,3}` may be directories within `top_env`\n",
      "Davis Bennett said:  because people typically expect that their conda environment is what they made it,not exactly true, I expect conda to install what I ask it to install, but if a new package needs to upgrade / downgrade other packages to satisfy compatibility constraints, I'm OK with that, as long as conda tells me what it's doing to my environment beforehand\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C011TMUB3UP/p1683662804934019?thread_ts=1683662804.934019&cid=C011TMUB3UP\n",
      "Adam Taylor said: If micromamba (https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html) will work for you, it's available on dm11 here:\n",
      "/groups/scicompsoft/home/taylora/bin/micromamba\n",
      "micromamba is nice in that it's a standalone executable, and doesn't \"helpfully\" alter your `.bashrc` file for you.  There are apparently some small differences with conda/mamba syntax, but I've never had an issue.  Plus it won't tell you that it wants to update itself all the darn time...\n",
      "Konrad Rokicki said: Maybe we could have a copy in /misc/sc for everyone to use?\n",
      "Mary Lay said: Was about to ask that.\n",
      "Adam Taylor said: Done.\n",
      "Others reacted to the previous message with tada a total of 1 times.\n",
      "\n",
      "https://janelia-dev.slack.com/archives/C011W6YDV99/p1683314914754999\n",
      "Philip Hubbard said: It looks like https://github.com/openlm-research/open_llama is not only interesting itself but also references a number of other interesting projects, like the RedPajama dataset, the Meerkat visualization tool, the Stanford Hazy Research group, etc.\n",
      "openlm-research/open_llama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"What are some interesting software packages that people are using?\", node_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388197a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
