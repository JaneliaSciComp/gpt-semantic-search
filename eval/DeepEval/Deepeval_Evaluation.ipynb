{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2690737a",
   "metadata": {},
   "source": [
    "Purpose: Turn text-data into data with appropriate values neccessary for functional RAGAS evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe365bb",
   "metadata": {},
   "source": [
    "Fetch documents (from only the SciComp wiki as of now) and put into accessable format for generation of expected outputs and creation of context later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WEB LOADER\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "import bs4 as bs\n",
    "import html2text\n",
    "from llama_index.core import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "data_path = '../data/janelia.org'  # Use './' to indicate the current directory\n",
    "text_maker = html2text.HTML2Text()\n",
    "text_maker.ignore_links = True\n",
    "text_maker.images_to_alt = True\n",
    "text_maker.single_line_break = True\n",
    "text_maker.ignore_emphasis = True\n",
    "SOURCE = \"Web\"\n",
    "\n",
    "\n",
    "def webpage_to_text(soup):\n",
    "    \"\"\" Convert a generic web page to searchable text\n",
    "    \"\"\"\n",
    "    title = soup.title.text\n",
    "    text = text_maker.handle(str(soup))\n",
    "    return title,text\n",
    "\n",
    "\n",
    "def janelia_org_to_text(soup):\n",
    "    \"\"\" Convert a janelia.org page to searchable text\n",
    "    \"\"\"\n",
    "    title = soup.title.text.replace(\" | Janelia Research Campus\",\"\")\n",
    "    content_sections = soup.find_all(\"section\", class_=\"content-section\")\n",
    "    if not content_sections:\n",
    "        return title,None\n",
    "    if len(content_sections) > 1:\n",
    "        raise Exception(\"More than one content section\")\n",
    "    content = content_sections[0]\n",
    "    # Remove useless content\n",
    "    for div in content.find_all(\"div\", {'class':['panels-ipe-label','secondary_menu']}):\n",
    "        div.decompose()\n",
    "    # Html2text smashes text together if only tags separate it\n",
    "    # This fix not only adds the spacing but also adds a separator for nav buttons\n",
    "    for span in content.find_all(\"span\", {'class':'button-wrapper'}):\n",
    "        sep = bs.NavigableString(\" / \")\n",
    "        span.insert(0, sep)\n",
    "    text = text_maker.handle(str(content))\n",
    "    return title,text\n",
    "\n",
    "\n",
    "def html_to_text(link, body):\n",
    "    \"\"\" Convert a web page to plain text for use as a GPT prompt.\n",
    "    \"\"\"\n",
    "    soup = bs.BeautifulSoup(body,'lxml')\n",
    "    if \"janelia.org\" in link:\n",
    "        title,text = janelia_org_to_text(soup)\n",
    "    else:\n",
    "        title,text = webpage_to_text(soup)\n",
    "    return title,text\n",
    "\n",
    "\n",
    "class WebSiteLoader():\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def create_document(self, name, title, link, doc_text):\n",
    "        metadata = {\"source\": self.data_path, \"title\": title, \"link\": link}\n",
    "        # Debugging: Print doc_text to ensure it's not empty\n",
    "        return [Document(page_content=doc_text, metadata=metadata)]\n",
    "    \n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "\n",
    "        for root, dirs, files in os.walk(self.data_path):\n",
    "            for name in files:\n",
    "                filepath = os.path.join(root, name)\n",
    "                with open(filepath) as f:\n",
    "                    link = f.readline().strip()\n",
    "                    body = f.read()\n",
    "                    title, text = html_to_text(link, body)\n",
    "                    \n",
    "                    \n",
    "                    # print(f\"Title: {title}\")\n",
    "                    # print(f\"Text: {text}\")\n",
    "                    if text:\n",
    "                        final_text = title + \"\\n\" + text\n",
    "                        with open('tempTestGen.txt', 'w') as file:\n",
    "                            file.write(final_text)\n",
    "                        loader = TextLoader(\"./tempTestGen.txt\")\n",
    "                        doc = loader.load()\n",
    "                        documents.append(doc)\n",
    "        return documents\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Open output.txt in write mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d58422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ARCHIVED WIKI LOADRER\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "import html2text\n",
    "from llama_index.core import Document\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SOURCE = \"Wiki\"\n",
    "\n",
    "text_maker = html2text.HTML2Text()\n",
    "text_maker.ignore_links = True\n",
    "text_maker.ignore_images = True\n",
    "\n",
    "\n",
    "def wiki_to_text(ancestors, title, authors, labels, body):\n",
    "    \"\"\" Convert a wiki document to plain text for use as a GPT prompt.\n",
    "    \"\"\"\n",
    "    body_text = text_maker.handle(body)\n",
    "    text =  f\"Title: {title}\\n\"\n",
    "    if authors: text += f\"Authors: {authors}\\n\" \n",
    "    if ancestors: text += f\"Ancestors: {ancestors}\\n\" \n",
    "    if labels: text += f\"Labels: {ancestors}\\n\"\n",
    "    text += f\"{body_text}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "class WikiLoader():\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def create_document(self, name, title, link, doc_text):\n",
    "        metadata = {\"source\": self.data_path, \"title\": title, \"link\": link}\n",
    "        return [Document(page_content=doc_text, metadata=metadata)]\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "        for root, dirs, files in os.walk(self.data_path):\n",
    "            for name in files:\n",
    "                filepath = os.path.join(root, name)\n",
    "                with open(filepath) as f:\n",
    "                    link = f.readline().rstrip()\n",
    "                    ancestors = f.readline().rstrip()\n",
    "                    title = f.readline().rstrip()\n",
    "                    authors = f.readline().rstrip()\n",
    "                    labels = f.readline().rstrip()\n",
    "                    body = re.sub('[\\n]+', '\\n', \"\".join(f.readlines()))\n",
    "                    text = wiki_to_text(ancestors, title, authors, labels, body)\n",
    "                    # doc = self.create_document(name, title, link, text)\n",
    "                    # documents.append(doc)\n",
    "                    if text:\n",
    "                        final_text = title + \"\\n\" + text\n",
    "                        with open('tempTestGen.txt', 'w') as file:\n",
    "                            file.write(final_text)\n",
    "                        loader = TextLoader(\"./tempTestGen.txt\")\n",
    "                        doc = loader.load()\n",
    "                        documents.append(doc)\n",
    "        return documents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d48ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ARCHIVED SLACK LOADER\"\"\"\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from decimal import Decimal\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SOURCE = \"Slack\"\n",
    "DOCUMENT_PAUSE_SECS = 300\n",
    "IGNORED_SUBTYPES = set(['channel_join','channel_leave','bot_message'])\n",
    "\n",
    "\n",
    "def get(dictionary, key):\n",
    "    \"\"\" Get the key out of the dictionary, if it exists. If not, return None.\n",
    "    \"\"\"\n",
    "    if dictionary and key in dictionary:\n",
    "        return dictionary[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def fix_text(text):\n",
    "    \"\"\" Standard transformations on text like squashing multiple newlines.\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\n+\", \"\\n\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class ArchivedSlackLoader():\n",
    "\n",
    "    def __init__(self, data_path, debug=False):\n",
    "        self.data_path = data_path\n",
    "        self.id2username = {}\n",
    "        self.id2realname = {}\n",
    "        self.channel2id = {}\n",
    "        self.debug = debug\n",
    "\n",
    "        for user in self.get_users():\n",
    "            id = user['id']\n",
    "            self.id2username[id] = user['name']\n",
    "            self.id2realname[id] = user['profile']['real_name']\n",
    "\n",
    "        logger.info(f\"Loaded {len(self.id2username)} users\")\n",
    "        for channel in self.get_channels():\n",
    "            logger.debug(f\"{channel['id']}: {channel['name']}\")\n",
    "            self.channel2id[channel['name']] = channel['id']\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.channel2id)} channels\")\n",
    "\n",
    "\n",
    "    def get_users(self):\n",
    "        \"\"\" Generator which returns users from the users.json file.\n",
    "        \"\"\"\n",
    "        with open(f\"{self.data_path}/users.json\", 'r') as f:\n",
    "            users = json.load(f)\n",
    "            for user in users:\n",
    "                yield user\n",
    "\n",
    "\n",
    "    def get_channels(self):\n",
    "        \"\"\" Generator which returns channels from the channels.json file.\n",
    "        \"\"\"\n",
    "        with open(f\"{self.data_path}/channels.json\", 'r') as f:\n",
    "            channels = json.load(f)\n",
    "            for channel in channels:\n",
    "                yield channel\n",
    "\n",
    "\n",
    "    def get_messages(self, channel_name):\n",
    "        \"\"\" Generator which returns messages from the json files in the given channel directory.\n",
    "        \"\"\"\n",
    "        for messages_file in glob.glob(f\"{self.data_path}/{channel_name}/*.json\"):\n",
    "            with open(messages_file, 'r') as f:\n",
    "                for message in json.load(f):\n",
    "                    yield message\n",
    "\n",
    "\n",
    "    def extract_text(self, elements):\n",
    "        \"\"\" Recursively parse an 'elements' structure, \n",
    "            converting user elements to their real names.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for element in elements:\n",
    "            if 'elements' in element:\n",
    "                text += self.extract_text(element['elements'])\n",
    "            el_type = get(element, 'type')\n",
    "            if el_type == 'text':\n",
    "                if get(get(element, 'style'), 'code'): text += '`'\n",
    "                text += element['text']\n",
    "                if get(get(element, 'style'), 'code'): text += '`'\n",
    "            elif el_type == 'link':\n",
    "                text += get(element, 'url')\n",
    "            elif el_type == 'rich_text_preformatted':\n",
    "                text += \"\\n\"\n",
    "            elif el_type == 'user':\n",
    "                user_id = element['user_id']\n",
    "                try:\n",
    "                    text += self.id2realname[user_id]\n",
    "                except KeyError:\n",
    "                    logger.error(f\"No such user '{user_id}'\")\n",
    "                    text += user_id\n",
    "\n",
    "        return text\n",
    "\n",
    "    def parse_message(self, message):\n",
    "        \"\"\" Parse a message into text that will be read by a GPT model. \n",
    "        \"\"\"\n",
    "        thread_id, text_msg = None, None\n",
    "        if get(message, 'type') == 'message':\n",
    "            if 'subtype' in message and get(message, 'subtype') in IGNORED_SUBTYPES:\n",
    "                pass\n",
    "            else:\n",
    "                ts = message['ts']\n",
    "                thread_ts = get(message, 'thread_ts') or ts\n",
    "                thread_id = Decimal(thread_ts)\n",
    "\n",
    "                # Translate user\n",
    "                user_id = message['user']\n",
    "                try:\n",
    "                    realname = self.id2realname[user_id]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        realname = message['user_profile']['display_name']\n",
    "                    except KeyError:\n",
    "                        realname = user_id\n",
    "                    \n",
    "                if 'blocks' in message:\n",
    "                    text = self.extract_text(message['blocks'])\n",
    "                else:\n",
    "                    text = message['text']\n",
    "                \n",
    "                text_msg = re.sub(\"<@(.*?)>\", lambda m: self.id2realname[m.group(1)], text)\n",
    "                text_msg = fix_text(text_msg)\n",
    "\n",
    "                if 'attachments' in message:\n",
    "                    for attachment in message['attachments']:\n",
    "                        if 'title' in attachment: text_msg += f\"\\n{fix_text(attachment['title'])}\"\n",
    "                        if 'text' in attachment: text_msg += f\"\\n{fix_text(attachment['text'])}\"\n",
    "                        \n",
    "                if 'files' in message:\n",
    "                    for file in message['files']:\n",
    "                        if 'name' in file:\n",
    "                            # There are several cases where a file doesn't have a name:\n",
    "                            # 1) The file has been deleted (mode=tombstone)\n",
    "                            # 2) We have no access (file_access=access_denied)\n",
    "                            text_msg += f\"\\n<{file['name']}>\"\n",
    "\n",
    "                if 'reactions' in message:\n",
    "                    text_msg += f\"\\nOthers reacted to the previous message with \"\n",
    "                    r = [f\"{reaction['name']} a total of {reaction['count']} times\" for reaction in message['reactions']]\n",
    "                    text_msg += \", and with \".join(r) + \".\"\n",
    "\n",
    "                text_msg = f\"{realname} said: {text_msg}\\n\"\n",
    "        \n",
    "        return thread_id, text_msg\n",
    "\n",
    "\n",
    "    def create_document(self, channel_id, ts, doc_text):\n",
    "        final_text = doc_text\n",
    "        with open('tempTestGen.txt', 'w') as file:\n",
    "            file.write(final_text)\n",
    "        loader = TextLoader(\"./tempTestGen.txt\")\n",
    "        \n",
    "        return loader.load()\n",
    "\n",
    "    def load_documents(self, channel_name):\n",
    "        channel_id = self.channel2id[channel_name]\n",
    "        messages = {}\n",
    "        for message in self.get_messages(channel_name):\n",
    "            try:\n",
    "                thread_id, text_msg = self.parse_message(message)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing message: {message}\")\n",
    "                raise e\n",
    "                \n",
    "            if thread_id and text_msg:\n",
    "                if thread_id not in messages:\n",
    "                    messages[thread_id] = []\n",
    "                messages[thread_id].append(text_msg)\n",
    "\n",
    "        prev_id = Decimal(0)\n",
    "        documents = []\n",
    "        doc_text = \"\"\n",
    "        start_ts = None\n",
    "\n",
    "        for thread_id in sorted(list(messages.keys())):\n",
    "\n",
    "            # Create a new document whenever messages are separated by a longer pause\n",
    "            if doc_text and thread_id-prev_id > DOCUMENT_PAUSE_SECS:\n",
    "                doc = self.create_document(channel_id, start_ts, doc_text)\n",
    "                documents.append(doc)\n",
    "                doc_text = \"\"\n",
    "                start_ts = None\n",
    "\n",
    "            logger.debug(thread_id)\n",
    "\n",
    "            # Starting timestamp for the next document\n",
    "            if not start_ts:\n",
    "                start_ts = str(thread_id)\n",
    "\n",
    "            # Add all messages from the current thread\n",
    "            for text_msg in messages[thread_id]:\n",
    "                doc_text += text_msg\n",
    "\n",
    "            prev_id = thread_id\n",
    "\n",
    "        # Add final document\n",
    "        doc = self.create_document(channel_id, start_ts, doc_text)\n",
    "        documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "        for channel_name in self.channel2id.keys():\n",
    "            for doc in self.load_documents(channel_name):\n",
    "                documents.append(doc)\n",
    "        return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e912570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 170 users\n",
      "INFO:__main__:Loaded 44 channels\n",
      "ERROR:__main__:No such user 'WAPC2SXJN'\n",
      "ERROR:__main__:No such user 'W8C6WFVM4'\n",
      "ERROR:__main__:No such user 'W9GJ4UF33'\n",
      "ERROR:__main__:No such user 'W0129A3DR8B'\n",
      "ERROR:__main__:No such user 'WBHHEM2AU'\n",
      "ERROR:__main__:No such user 'W010W8A1EBF'\n",
      "ERROR:__main__:No such user 'W010W8A1EBF'\n",
      "ERROR:__main__:No such user 'W010W8A1EBF'\n",
      "ERROR:__main__:No such user 'WA7Q7CKGS'\n",
      "ERROR:__main__:No such user 'U02QZ8GH64X'\n",
      "ERROR:__main__:No such user 'W013JPYQ5PA'\n",
      "ERROR:__main__:No such user 'W013JPYQ5PA'\n",
      "ERROR:__main__:No such user 'WD5FSBZTJ'\n",
      "ERROR:__main__:No such user 'W010W8A1EBF'\n",
      "ERROR:__main__:No such user 'UMVJ4KRV2'\n",
      "ERROR:__main__:No such user 'U03PL1HLZBP'\n",
      "ERROR:__main__:No such user 'U040HM3D0TU'\n",
      "ERROR:__main__:No such user 'UN7R87EUE'\n",
      "ERROR:__main__:No such user 'U040HM3D0TU'\n",
      "ERROR:__main__:No such user 'U028YV8LZUP'\n",
      "ERROR:__main__:No such user 'UN7R87EUE'\n",
      "ERROR:__main__:No such user 'UN7R87EUE'\n",
      "ERROR:__main__:No such user 'UN7R87EUE'\n",
      "ERROR:__main__:No such user 'UN7R87EUE'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UGF6L2YUS'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0182GKL7QR'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UGF6L2YUS'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UH5AYU4JD'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U04PR67HG3B'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'UFJ8DBDC3'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UGF6L2YUS'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'U04PR67HG3B'\n",
      "ERROR:__main__:No such user 'U0182GKL7QR'\n",
      "ERROR:__main__:No such user 'U04PR67HG3B'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UDM220J2W'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'U04PR67HG3B'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U4T03270V'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U4T03270V'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UH5AYU4JD'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UGF6L2YUS'\n",
      "ERROR:__main__:No such user 'U019XPGTGRL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UH5AYU4JD'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'UH5AYU4JD'\n",
      "ERROR:__main__:No such user 'UNZ3K8W5R'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'U04PR67HG3B'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U016WBXDARX'\n",
      "ERROR:__main__:No such user 'U029QJUSZFB'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029QJUSZFB'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U04PR67UZ4H'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UV8CP1F2N'\n",
      "ERROR:__main__:No such user 'U029QJUSZFB'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'UAXU10MLL'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n",
      "ERROR:__main__:No such user 'U0JER9YUX'\n",
      "ERROR:__main__:No such user 'U029DL61X5L'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DOCUMENT LOADER ALL SOURCES\"\"\"\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# generator with openai models\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# DONE: Recursively load all scraped files in the directory and its subdirectories\n",
    "# loader = TextLoader(\"./test.txt\")\n",
    "# documents = loader.load()\n",
    "# print(documents)\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import mimetypes\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\"\"\"ArchivedSlackLoader\n",
    "slack_to_2023-05-18\n",
    "\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "class DocumentLoader:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_documents(self):\n",
    "        all_documents = []\n",
    "        for folder_name in os.listdir(self.root_dir):\n",
    "            folder_path = os.path.join(self.root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                if folder_name == \"wiki\":\n",
    "                    loader = WikiLoader(folder_path)\n",
    "                elif folder_name == \"slack\":\n",
    "                    # Specify the two subdirectories for slack\n",
    "                    subdirs = [\"janelia-software/slack_to_2023-05-18\"]\n",
    "                    for subdir in subdirs:\n",
    "                        subfolder_path = os.path.join(folder_path, subdir)\n",
    "                        # Check if the subdirectory exists\n",
    "                        if os.path.isdir(subfolder_path):\n",
    "                            loader = ArchivedSlackLoader(subfolder_path)\n",
    "                            documents = loader.load_all_documents()\n",
    "                            # Take a random 10% sample\n",
    "                            # sample_size = max(1, len(documents) // 10)\n",
    "                            # documents_sample = random.sample(documents, sample_size)\n",
    "                            # all_documents.extend(documents_sample)\n",
    "                            all_documents.extend(documents)\n",
    "                elif folder_name == \"janelia.com\":\n",
    "                    loader = WebSiteLoader(folder_path)\n",
    "                else:\n",
    "                    continue  # Skip if folder doesn't match any criteria\n",
    "                # For non-slack directories\n",
    "                if folder_name != \"slack\":\n",
    "                    documents = loader.load_all_documents()\n",
    "                    # Take a random 10% sample\n",
    "                    # sample_size = max(1, len(documents) // 10)\n",
    "                    # documents_sample = random.sample(documents, sample_size)\n",
    "                    # all_documents.extend(documents_sample)\n",
    "                    all_documents.extend(documents)\n",
    "        return all_documents\n",
    "    \n",
    "    def test_load_documents(self):\n",
    "        # This method is for testing purposes and will only load the first document in each folder path\n",
    "        all_documents = []\n",
    "        for folder_name in os.listdir(self.root_dir):\n",
    "            folder_path = os.path.join(self.root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                if folder_name == \"wiki\":\n",
    "                    loader = WikiLoader(folder_path)\n",
    "                elif folder_name == \"slack\":\n",
    "                    # Specify the two subdirectories for slack\n",
    "                    subdirs = [\"janelia-software/slack_to_2023-05-18\"]\n",
    "                    for subdir in subdirs:\n",
    "                        subfolder_path = os.path.join(folder_path, subdir)\n",
    "                        # Check if the subdirectory exists\n",
    "                        if os.path.isdir(subfolder_path):\n",
    "                            loader = ArchivedSlackLoader(subfolder_path)\n",
    "                            documents = loader.load_all_documents()\n",
    "                            # Only load the first document for testing\n",
    "                            if documents:\n",
    "                                all_documents.append(documents[0])\n",
    "                elif folder_name == \"janelia.com\":\n",
    "                    loader = WebSiteLoader(folder_path)\n",
    "                else:\n",
    "                    continue  # Skip if folder doesn't match any criteria\n",
    "                # For non-slack directories\n",
    "                if folder_name != \"slack\":\n",
    "                    documents = loader.load_all_documents()\n",
    "                    # Only load the first document for testing\n",
    "                    if documents:\n",
    "                        all_documents.append(documents[0])\n",
    "        return all_documents\n",
    "\n",
    "# Assuming your data folder is at \"./data/\"\n",
    "loader = DocumentLoader(\"../../data\")\n",
    "documents = loader.test_load_documents()\n",
    "with open('documents.txt', 'w') as file:\n",
    "    for document in documents:\n",
    "        file.write(str(document) + '\\n')\n",
    "# # Assuming documents is a list of strings or convertible to string\n",
    "\n",
    "\n",
    "# Now `final_df` contains all the generated testsets in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "725f388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create ollama Mistral7B model for synthetic testset generation\"\"\"\n",
    "import requests\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import asyncio\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name=\"llama3:8b\"):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = requests.post(self.api_url, json={\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        })\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response']\n",
    "        else:\n",
    "            raise Exception(f\"Error generating response: {response.text}\")\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(self.executor, self.generate, prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "# Initialize the model\n",
    "mistral_7b = Mistral7B()\n",
    "\n",
    "# Generate a response\n",
    "print(mistral_7b.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b4bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports + Prepare data\"\"\"\n",
    "from datasets import Dataset\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import pandas as pd\n",
    "import pyarrow \n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"avr/sfr-embedding-mistral\"\n",
    ")\n",
    "\n",
    "\n",
    "def datasetFix(df):\n",
    "    columns_to_keep = ['question', 'ground_truth', 'contexts']\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    # Apply transformations\n",
    "    df['contexts'] = df['contexts'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "    df['ground_truth'] = df['ground_truth'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05678b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2352bdb599c43428b5b53a398409b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d738bc572e4f51a6360aa10ad699f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Golden(input=\"What specific roles does the master server e03u07 play in Janelia Workstation's production environment using Docker Swarm?\", actual_output=None, expected_output=None, context=[' Joint SciComp Systems/Software pages / SCSW Servers\\\\nLabels: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nDescription| JACS Prod Swarm  \\\\n  \\\\n---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n    \\\\n    sudo dnf install s3fs-fuse  \\\\n    sudo mkdir -p /data/s3/janelia-mouselight-imagery  \\\\n    sudo chown -R jacs:jacsdata /data/s3  \\\\n    sudo chmod -R 2775 /data/s3  \\\\n    sudo sed -e \"s/# user_allow_other/user_allow_other/\" -i /etc/fuse.conf  \\\\n    sudo mkdir /opt/jacs  \\\\n    sudo chown -R jacs:jacsdata /opt/jacs  \\\\n    sudo chmod -R 2775 /opt/jacs  \\\\n    sudo mkdir /data/jacs  \\\\n    sudo chown -R jacs:jacsdata /data/jacs  \\\\n    sudo chmod -R 2775 /data/jacs  \\\\n    echo \"vm.max_map_count=262144\" | sudo tee -a /etc/sysctl.d/99-sysctl.conf  \\\\n    sudo sysctl -p  \\\\n      \\\\n    \\\\n\\\\n\\', metadata={\\'source\\': \\'./tempTestGen.txt\\'})]\\n[Document(page_content=\\'Scientific Computing Server - e03u07\\\\nTitle: Scientific Computing Server - e03u07\\\\nAuthors: Konrad Rokicki\\\\nAncestors: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nLabels: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nDescription| JACS Prod Swarm  \\\\n  \\\\n---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n    \\\\n    sudo dnf install s3fs-fuse  \\\\n    sudo mkdir -p /data/s3/janelia-mouselight-imagery  \\\\', '[Document(page_content=\\'Scientific Computing Server - e03u07\\\\nTitle: Scientific Computing Server - e03u07\\\\nAuthors: Konrad Rokicki\\\\nAncestors: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nLabels: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nDescription| JACS Prod Swarm  \\\\n  \\\\n---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n    \\\\n    sudo dnf install s3fs-fuse  \\\\n    sudo mkdir -p /data/s3/janelia-mouselight-imagery  \\\\n    sudo chown -R jacs:jacsdata /data/s3  \\\\n    sudo chmod -R 2775 /data/s3  \\\\n    sudo sed -e \"s/# user_allow_other/user_allow_other/\" -i /etc/fuse.conf  \\\\n    sudo mkdir /opt/jacs  \\\\n    sudo chown -R jacs:jacsdata /opt/jacs  \\\\n    sudo chmod -R 2775 /opt/jacs  \\\\n    sudo mkdir /data/jacs  \\\\n    sudo chown -R jacs:jacsdata /data/jacs  \\\\n    sudo chmod -R 2775 /data/jacs  \\\\n    echo \"vm.max_map_count=262144\" | sudo tee -a /etc/sysctl.d/99-sysctl.conf  \\\\n    sudo sysctl -p  \\\\n      \\\\n    \\\\n\\\\n\\', metadata={\\'source\\': \\'./tempTestGen.txt\\'})]\\n[Document(page_content=\\'Scientific Computing Server - e03u07\\\\nTitle: Scientific Computing Server - e03u07\\\\nAuthors: Konrad Rokicki\\\\nAncestors: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nLabels: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nDescription| JACS Prod Swarm  \\\\n  \\\\n---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n', '---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n    \\\\n    sudo dnf install s3fs-fuse  \\\\n    sudo mkdir -p /data/s3/janelia-mouselight-imagery  \\\\n    sudo chown -R jacs:jacsdata /data/s3  \\\\n    sudo chmod -R 2775 /data/s3  \\\\n    sudo sed -e \"s/# user_allow_other/user_allow_other/\" -i /etc/fuse.conf  \\\\n    sudo mkdir /opt/jacs  \\\\n    sudo chown -R jacs:jacsdata /opt/jacs  \\\\n    sudo chmod -R 2775 /opt/jacs  \\\\n    sudo mkdir /data/jacs  \\\\n    sudo chown -R jacs:jacsdata /data/jacs  \\\\n    sudo chmod -R 2775 /data/jacs  \\\\n    echo \"vm.max_map_count=262144\" | sudo tee -a /etc/sysctl.d/99-sysctl.conf  \\\\n    sudo sysctl -p  \\\\n      \\\\n    \\\\n\\\\n\\', metadata={\\'source\\': \\'./tempTestGen.txt\\'})]\\n[Document(page_content=\\'Scientific Computing Server - e03u07\\\\nTitle: Scientific Computing Server - e03u07\\\\nAuthors: Konrad Rokicki\\\\nAncestors: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nLabels: Home / Joint SciComp Systems/Software pages / SCSW Servers\\\\nDescription| JACS Prod Swarm  \\\\n  \\\\n---|---  \\\\nOS| OL 9.1  \\\\nSoftware| Docker  \\\\nHardware| Dell R6515  \\\\nCores| 64  \\\\nRAM (GB)| 128  \\\\nDisks| 2 x 2TB NVMe in raid1  \\\\nNetwork| Internal  \\\\nIP| 10.40.2.134  \\\\nCanonical name| e03u07.int.janelia.org  \\\\nAliases| workstation  \\\\nCertificates| *.int.janelia.org  \\\\nPOC| Cristian Goina  \\\\nWarranty Ends| 2026-07-10  \\\\n  \\\\n## Purpose\\\\n\\\\nRuns the production services for the Janelia Workstation using Docker Swarm.\\\\nThis is the master server.\\\\n\\\\n## Software\\\\n\\\\n  * JACS containers\\\\n  * s3fs for mounting AWS S3 buckets\\\\n\\\\n## Configuration\\\\n\\\\nJACS services are managed from workstation:/opt/deploy/jacs-cm\\\\n\\\\n**s3fs**\\\\n\\\\nInstall and configure fuse for s3fs:\\\\n\\\\n    \\\\n    \\\\n    sudo dnf install s3fs-fuse  \\\\n    sudo mkdir -p /data/s3/janelia-mouselight-imagery  \\\\n    sudo chown -R jacs:jacsdata /data/s3  \\\\n    sudo chmod -R 2775 /data/s3  \\\\n    sudo sed -'], retrieval_context=None, additional_metadata=None, comments=None, source_file='synthetic_docs.txt')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"BROKEN BECAUSE OLLAMA DOES NOT RETURN VALID JSON\"\"\"\n",
    "all_data_df = pd.DataFrame()\n",
    "# synthesizer = Synthesizer(model=mistral_7b)\n",
    "synthesizer = Synthesizer()\n",
    "all_data_df = pd.DataFrame()\n",
    "for document in documents:\n",
    "    test_size_gen = int(len(str(document)) // 1000)\n",
    "\n",
    "    with open('synthetic_docs.txt', 'a') as file:  # Open in append mode\n",
    "        file.write(str(document) + '\\n')\n",
    "\n",
    "    current_testset=synthesizer.generate_goldens_from_docs(\n",
    "        document_paths=['synthetic_docs.txt'],\n",
    "        max_goldens_per_document=test_size_gen\n",
    "    )    \n",
    "    print (current_testset)\n",
    "    # current_testset = current_testset.to_pandas()\n",
    "\n",
    "    # # current_testset.to_parquet('importMe.parquet', index=False)\n",
    "    # current_df = datasetFix(current_testset)\n",
    "\n",
    "    # current_df = datasetFix(current_testset)\n",
    "    \n",
    "    # all_data_df = pd.concat([all_data_df, current_df], ignore_index=True)    # If this is the first iteration, set the DataFrame, otherwise append to it\n",
    "\n",
    "display(all_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b05cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d241aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is a question that can be fully answered from the given context:\\n\\n\"What is the purpose of the Scientific Computing Server, e03u07?\"\\n\\nThis question can be answered by reading the \"Purpose\" section of the context, which states that the server \"Runs the production services for the Janelia Workstation using Docker Swarm. This is the master server.', 'Here is a question that can be fully answered from the given context:\\n\\n\"What is the purpose of the Scientific Computing Server - e03u07?\"\\n\\nThis question can be answered by reading the \"Purpose\" section of the context, which states: \"Runs the production services for the Janelia Workstation using Docker Swarm. This is the master server.', 'Here\\'s a question that can be fully answered from the given context:\\n\\n\"What is Davis Bennett\\'s comparison between using Numba and CuPy for accelerating array computing operations?\"\\n\\nThis question can be answered by referencing Davis Bennett\\'s statement about comparing the performance of a CUDA kernel jitted with Numba to the same operation on a CuPy array.']\n",
      "Here is a question that can be fully answered from the given context:\n",
      "\n",
      "\"What is the purpose of the Scientific Computing Server, e03u07?\"\n",
      "\n",
      "This question can be answered by reading the \"Purpose\" section of the context, which states that the server \"Runs the production services for the Janelia Workstation using Docker Swarm. This is the master server.\n",
      "Here is a question that can be fully answered from the given context:\n",
      "\n",
      "\"What is the purpose of the Scientific Computing Server - e03u07?\"\n",
      "\n",
      "This question can be answered by reading the \"Purpose\" section of the context, which states: \"Runs the production services for the Janelia Workstation using Docker Swarm. This is the master server.\n",
      "Here's a question that can be fully answered from the given context:\n",
      "\n",
      "\"What is Davis Bennett's comparison between using Numba and CuPy for accelerating array computing operations?\"\n",
      "\n",
      "This question can be answered by referencing Davis Bennett's statement about comparing the performance of a CUDA kernel jitted with Numba to the same operation on a CuPy array.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions_list = all_data_df['question'].tolist()\n",
    "print (questions_list)\n",
    "\n",
    "seen = set()\n",
    "questions_list = [x for x in questions_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "for item in questions_list:\n",
    "    print(item)\n",
    "\n",
    "    \n",
    "\n",
    "# testset.to_json(\"testset.json\")\n",
    "# Creates dataset of ground truths contexts and questions for the testset\n",
    "# Missing answers column \n",
    "# On one medium size document, it took about 1 minutes to generate 10 questions and cost 3 dollars on OpenAI\n",
    "# Seems expensive when using gpt-4, is its use justified or does 3.5 get the job done?\n",
    "# Evaluate gpt-4 vs gpt-3.5-turbo-16k for RAGAS evaluation test data generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a0523",
   "metadata": {},
   "source": [
    "fetch the LLM's response and append to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862687da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The primary purpose of Akaike information crit...</td>\n",
       "      <td>[[Mark Kittisopikul said: Greg Fleishman, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of the Akaike information criterio...</td>\n",
       "      <td>[[Mark Kittisopikul said: Greg Fleishman, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>AIC aims to reconcile the trade-off between th...</td>\n",
       "      <td>[[Mark Kittisopikul said: Greg Fleishman, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The embedded ChatGPT-like interface has tricks...</td>\n",
       "      <td>[[William Katz said: https://twitter.com/maxho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Sequential storage can have benefits even in h...</td>\n",
       "      <td>[[William Katz said: Some links from my second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here's a rewritten question that conveys the s...</td>\n",
       "      <td>Sequential storage (i.e., trying to maximize a...</td>\n",
       "      <td>[[William Katz said: Some links from my second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The matrix operations performed in this code s...</td>\n",
       "      <td>[[.10534273, -0.5556871 ,\\n         0.8352932 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking the accuracy of the GGESE...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of the QZ decomposition in numeric...</td>\n",
       "      <td>[[.10534273, -0.5556871 ,\\n         0.8352932 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The embedded ChatGPT-like interface has featur...</td>\n",
       "      <td>[[William Katz said: https://twitter.com/maxho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking the accuracy of the GGESE...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Python 3.10.6</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Based on the given context, here's a question ...</td>\n",
       "      <td>The issue causing the kernel crash when using ...</td>\n",
       "      <td>[[.        ,  0.        ,  0.        ,  0.    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Here's a question that can be fully answered b...</td>\n",
       "      <td>During the Next Gen File Formats session at th...</td>\n",
       "      <td>[[Ulrike Boehm said: For those of you interest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[.10534273, -0.5556871 ,\\n         0.8352932 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What's on the agenda for the Next Gen File For...</td>\n",
       "      <td>During the OME conference next week they will ...</td>\n",
       "      <td>[[Ulrike Boehm said: For those of you interest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[.        ,  0.        ,  0.        ,  0.    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Here's a question that can be fully answered b...</td>\n",
       "      <td>During the 'Next Gen File Formats' session at ...</td>\n",
       "      <td>[[Ulrike Boehm said: For those of you interest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>To attach VSCode to a bsub session for remote ...</td>\n",
       "      <td>[[Eric Wait said: Has anyone attached VScode t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Based on the given context, a question that ca...</td>\n",
       "      <td>The differences between the arrays 'a', 'b', a...</td>\n",
       "      <td>[[ol=1.19209e-05\\nE   \\nE   Mismatched element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Gert-Jan Both made some changes to the API of ...</td>\n",
       "      <td>[[Gert-Jan Both said: Welcome everyone! As you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Running Pkg.update() in a Julia session update...</td>\n",
       "      <td>[[Mark Kittisopikul said: Something like the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking if the result of the GGE ...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking if the result of the GGE ...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>During warping, values for img must be reconst...</td>\n",
       "      <td>[[Davis Bennett said: after rotation of the `[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The expected result of the q @ s @ z.conj().T ...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The purpose of the QZ decomposition in linear ...</td>\n",
       "      <td>[[.10534273, -0.5556871 ,\\n         0.8352932 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does integrating package management within...</td>\n",
       "      <td>Integrating package management within a progra...</td>\n",
       "      <td>[[Mark Kittisopikul said: Something like the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>You can run a remote VScode session on the sub...</td>\n",
       "      <td>[[Eric Wait said: Has anyone attached VScode t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Some common topics of humor and criticism abou...</td>\n",
       "      <td>[[Mark Kittisopikul said: I will await your up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>Science has developed a *ton* of internal tool...</td>\n",
       "      <td>[[William Katz said: https://twitter.com/maxho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What are this week's office hour timings, as c...</td>\n",
       "      <td>Srini's office hours will begin next week on T...</td>\n",
       "      <td>[[William Katz said:  I propose we start this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Here's the question:\\n\\n\"What are the proposed...</td>\n",
       "      <td>The proposed office hour arrangements are Srin...</td>\n",
       "      <td>[[William Katz said:  I propose we start this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Davis Bennett suggests that the h5py API shoul...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Gert-Jan Both made some changes to the API of ...</td>\n",
       "      <td>[[Gert-Jan Both said: Welcome everyone! As you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The purpose of the QZ decomposition in this ma...</td>\n",
       "      <td>[[.10534273, -0.5556871 ,\\n         0.8352932 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The kernel crashes</td>\n",
       "      <td>[[.        ,  0.        ,  0.        ,  0.    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What are the main differences between Docker a...</td>\n",
       "      <td>The main differences between Docker and Singul...</td>\n",
       "      <td>[[Robert Lines said: The biggest gotcha we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The proposed pace of the course is to have a v...</td>\n",
       "      <td>[[William Katz said:  I propose we start this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>To attach VSCode to a bsub session for debuggi...</td>\n",
       "      <td>[[Eric Wait said: Has anyone attached VScode t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Python 3.10.6</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>Davis Bennett's complaint about h5py is that i...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Here's a rewritten question that requires mult...</td>\n",
       "      <td>Konrad Rokicki's explanation reveals that by d...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Here's a rewritten question that conveys the s...</td>\n",
       "      <td>Robert Lines and Konrad Rokicki balance memory...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The autogenerated filenames in the h5py source...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The issue with running tasks on workers in a d...</td>\n",
       "      <td>[[ in get\\n      results = self.gather(packed,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>h5py doesn't provide a straightforward way to ...</td>\n",
       "      <td>[[ the big headaches when trying to use h5py w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The issue of not being able to remove intermed...</td>\n",
       "      <td>[[Greg Fleishman said: Is anyone aware of any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The issue might be caused by a version mismatc...</td>\n",
       "      <td>[[ in get\\n      results = self.gather(packed,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The limitations of translation and conversion ...</td>\n",
       "      <td>[[William Katz said: https://www.biorxiv.org/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>To attach VScode to an interactive bsub sessio...</td>\n",
       "      <td>[[Eric Wait said: Has anyone attached VScode t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>What is the main difference between Docker and...</td>\n",
       "      <td>The main difference between Docker and Singula...</td>\n",
       "      <td>[[Robert Lines said: The biggest gotcha we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Here is a rewritten version of the question:\\n...</td>\n",
       "      <td>The answer to given question can be inferred f...</td>\n",
       "      <td>[[Mary Lay said: Best way to set up mamba when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Nextflow's current approach to memory manageme...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The `warp` command in Interpolations.jl produc...</td>\n",
       "      <td>[[ points are handled - pass img as an\\n  Abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The How To interest group meetings will move t...</td>\n",
       "      <td>[[Mark Kittisopikul said: We are moving How To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>Davis Bennett's complaint is that h5py hangs o...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The changes made to the chromatic package rega...</td>\n",
       "      <td>[[_ramps`) now return phase directly as `ndarr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>What is the main difference between Docker and...</td>\n",
       "      <td>The context does not mention Docker or Singula...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>To address challenges created by heterogeneity...</td>\n",
       "      <td>[[Mark Kittisopikul said: \\nHello *&lt;!channel&gt;*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>The presenter is Luca Marconato, and the focus...</td>\n",
       "      <td>[[Mark Kittisopikul said: \\nHello *&lt;!channel&gt;*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>The main differences between the proposed chun...</td>\n",
       "      <td>[[Mark Kittisopikul said: I posted a compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The main differences between the proposed chun...</td>\n",
       "      <td>[[Mark Kittisopikul said: I posted a compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Here is the question that can be fully answere...</td>\n",
       "      <td>Yann was bored and working as a project manage...</td>\n",
       "      <td>[[William Katz said: https://corecursive.com/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Some common topics of humor and criticism rega...</td>\n",
       "      <td>[[Mark Kittisopikul said: I will await your up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The limitations of translation and conversion ...</td>\n",
       "      <td>[[William Katz said: https://www.biorxiv.org/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Here's a rewritten question that meets the rul...</td>\n",
       "      <td>OME-NGFF introduces next-generation file forma...</td>\n",
       "      <td>[[William Katz said: https://www.biorxiv.org/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The differences in building FicTrac using diff...</td>\n",
       "      <td>[[Davis Bennett said: Eric Wait you were recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Davis Bennett said that the talk about catalog...</td>\n",
       "      <td>[[Davis Bennett said: reminder:  today at 2:45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>Lego robotics was introduced as an interesting...</td>\n",
       "      <td>[[Philip Hubbard said: My twins are in kinderg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Yann was bored and working as a project manage...</td>\n",
       "      <td>[[William Katz said: https://corecursive.com/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Yann was bored and working as a project manage...</td>\n",
       "      <td>[[William Katz said: https://corecursive.com/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Based on the given context, a suitable questio...</td>\n",
       "      <td>The purpose of the `@multimethod` macro in Jul...</td>\n",
       "      <td>[[               \"argument must be a block\")\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The main differences between the proposed chun...</td>\n",
       "      <td>[[Mark Kittisopikul said: I posted a compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>What's Julia's way to identify anonymous varia...</td>\n",
       "      <td>The answer is present in context, specifically...</td>\n",
       "      <td>[[               \"argument must be a block\")\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Davis Bennett's complaint is that h5py hangs o...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some of the challenges and differences when ru...</td>\n",
       "      <td>[[Robert Lines said: The biggest gotcha we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Here is the rewritten question:\\n\\n\"What makes...</td>\n",
       "      <td>Juno is suitable for simulating optical system...</td>\n",
       "      <td>[[Srini said: https://twitter.com/mariescopy/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>To attach VSCode to a bsub session for debuggi...</td>\n",
       "      <td>[[Eric Wait said: Has anyone attached VScode t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Here's a question that can be fully answered b...</td>\n",
       "      <td>The factors that contributed to the spread of ...</td>\n",
       "      <td>[[William Katz said: Some interesting data fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>Gert-Jan Both highlights that using asserts in...</td>\n",
       "      <td>[[\\n Examples and tests still need to be upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Developers face challenges such as a complete ...</td>\n",
       "      <td>[[Davis Bennett said: can anyone make sense of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>You should be able to automatically create the...</td>\n",
       "      <td>[[David Ackerman said: I can now run a sample ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>According to Mark Kittisopikul, common feature...</td>\n",
       "      <td>[[Mark Kittisopikul said: I've seen Synology u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>What challenges did Hannah face building FicTr...</td>\n",
       "      <td>Hannah faced challenges with building FicTrac ...</td>\n",
       "      <td>[[Davis Bennett said: Eric Wait you were recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Here's a rewritten question that combines info...</td>\n",
       "      <td>Multimethod dispatch in Julia enables multimet...</td>\n",
       "      <td>[[               \"argument must be a block\")\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>What causes SSH connection issues, and how doe...</td>\n",
       "      <td>The `login1` ssh host key has changed recently...</td>\n",
       "      <td>[[Stuart Berg said: Robert Lines Ken Carlile H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The issue with running tasks on workers in the...</td>\n",
       "      <td>[[ in get\\n      results = self.gather(packed,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The main differences between the HDF5 fixed ar...</td>\n",
       "      <td>[[Mark Kittisopikul said: Numcodecs is in urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The following changes were made to the chromat...</td>\n",
       "      <td>[[_ramps`) now return phase directly as `ndarr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Arraylake is a data lake platform for managing...</td>\n",
       "      <td>[[Davis Bennett said: reminder:  today at 2:45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Based on the given context, a question that ca...</td>\n",
       "      <td>Juno allows users to design and visualise arbi...</td>\n",
       "      <td>[[Srini said: https://twitter.com/mariescopy/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The reason for sending some features upstream ...</td>\n",
       "      <td>[[Mark Kittisopikul said: https://root.cern/bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Sequential storage (i.e., trying to maximize a...</td>\n",
       "      <td>[[William Katz said: Some links from my second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>According to Mark Kittisopikul, the reason for...</td>\n",
       "      <td>[[Mark Kittisopikul said: Numcodecs is in urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Based on the given context, a suitable questio...</td>\n",
       "      <td>nan</td>\n",
       "      <td>[[Philip Hubbard said: FastAI supports \"augmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The limitations that led K-Optional Software t...</td>\n",
       "      <td>[[William Katz said: Recent blog post on why a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>According to Robert Lines, the current approac...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Discourse is a common hosting solution used by...</td>\n",
       "      <td>[[Habib Bukhari said: Or just use our own host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some potential challenges or requirements for ...</td>\n",
       "      <td>[[Habib Bukhari said: Or just use our own host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking the accuracy of the GGESE...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The RSA host key for login1 has changed</td>\n",
       "      <td>[[Stuart Berg said: Robert Lines Ken Carlile H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The four main steps involved in aligning overl...</td>\n",
       "      <td>[[Alignment Guide\\nTitle: Alignment Guide\\nAut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Davis Bennett suggests exposing a __exit__() m...</td>\n",
       "      <td>[[Davis Bennett said: loving these filenames i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The 7000 series train cars used by Mark Kittis...</td>\n",
       "      <td>[[-get-more-frequent-rail-service.cfm\\nMark Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>According to Mark Kittisopikul, there are no o...</td>\n",
       "      <td>[[Mark Kittisopikul said: Grabbed some photos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The @multimethod macro is used to define multi...</td>\n",
       "      <td>[[               \"argument must be a block\")\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The purpose of hosting a \"virtual\" N5 file via...</td>\n",
       "      <td>[[Stuart Berg said: Marwan Zouinkhi In the thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Here's a rewritten question that conveys the s...</td>\n",
       "      <td>Developers typically encounter challenges when...</td>\n",
       "      <td>[[Davis Bennett said: can anyone make sense of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>The scheduled call time for Mark Kittisopikul'...</td>\n",
       "      <td>[[Mark Kittisopikul said: Is there anyone else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Here is a rewritten version of the question th...</td>\n",
       "      <td>The retry mechanism in Nextflow prevents jobs ...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Nextflow can retry jobs with more memory, as m...</td>\n",
       "      <td>[[Konrad Rokicki said: Robert Lines I was wron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose was to introduce and demonstrate t...</td>\n",
       "      <td>[[WikiTalkSept2006\\nTitle: WikiTalkSept2006\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The primary functions of the SAGE imagery inde...</td>\n",
       "      <td>[[SAGE imagery indexer\\nTitle: SAGE imagery in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some potential issues or considerations when u...</td>\n",
       "      <td>[[\\n Examples and tests still need to be upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Mamba ended up taking maybe 2 min. Conda took ...</td>\n",
       "      <td>[[Mary Lay said: Best way to set up mamba when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>What's the solution for Philip Hubbard's probl...</td>\n",
       "      <td>Philip Hubbard's solution for the problem with...</td>\n",
       "      <td>[[Philip Hubbard said: Does anyone else have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>What's used for exploring matches on the dashb...</td>\n",
       "      <td>To view and navigate tile collections in the R...</td>\n",
       "      <td>[[\\napproximation as a rough alignment. The ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Here is the question that can be fully answere...</td>\n",
       "      <td>Go is an open source programming language that...</td>\n",
       "      <td>[[Mark Kittisopikul said: Go 1.18 gets generic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The tentative schedule for the call with HDF G...</td>\n",
       "      <td>[[Mark Kittisopikul said: Is there anyone else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The purpose of the call is to allow participan...</td>\n",
       "      <td>[[Mark Kittisopikul said: Is there anyone else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>What percentage of RAM should the `shared_buff...</td>\n",
       "      <td>1/4 to 1/3 of RAM</td>\n",
       "      <td>[[PostgreSQL Performance Tuning\\nTitle: Postgr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Here's a question that can be fully answered b...</td>\n",
       "      <td>According to William Katz, the estimated numbe...</td>\n",
       "      <td>[[William Katz said: Some interesting data fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The SAGE imagery indexer will extract informat...</td>\n",
       "      <td>[[SAGE imagery indexer\\nTitle: SAGE imagery in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Based on the given context, a suitable questio...</td>\n",
       "      <td>The key differences between the FastAI approac...</td>\n",
       "      <td>[[Philip Hubbard said: FastAI supports \"augmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The limitations that led K-Optional Software t...</td>\n",
       "      <td>[[William Katz said: Recent blog post on why a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>What are the conditions that need to be met in...</td>\n",
       "      <td>The conditions that need to be met in order to...</td>\n",
       "      <td>[[Habib Bukhari said: Or just use our own host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Here is a rewritten question that conveys the ...</td>\n",
       "      <td>Introducing chess by playing with general rule...</td>\n",
       "      <td>[[Philip Hubbard said: My twins are in kinderg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>The answer to given question is present in con...</td>\n",
       "      <td>[[Philip Hubbard said: FastAI supports \"augmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>How can PostgreSQL performance be optimized fo...</td>\n",
       "      <td>Configuration settings for PostgreSQL performa...</td>\n",
       "      <td>[[PostgreSQL Performance Tuning\\nTitle: Postgr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Here is the question that can be fully answere...</td>\n",
       "      <td>The code snippet closes objects. It first call...</td>\n",
       "      <td>[[            self.id._close_open_objects(h5f....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>What changes does the Viewer class make to sup...</td>\n",
       "      <td>The Viewer class makes several changes to supp...</td>\n",
       "      <td>[[ // \"RedFormat\" is not documented, but used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>According to Mark Kittisopikul, common feature...</td>\n",
       "      <td>[[Mark Kittisopikul said: I've seen Synology u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of setting the `pixelRatio` and `s...</td>\n",
       "      <td>[[     this.renderer.setClearColor(\"#000000\")\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The focus of the first Code Review Interest Gr...</td>\n",
       "      <td>[[William Katz said: The next two Code Review ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>What is the significance of the camera's posit...</td>\n",
       "      <td>The camera's position is significant in determ...</td>\n",
       "      <td>[[, this.camera)\\n        // Updated, for came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of the ray marching iterations is ...</td>\n",
       "      <td>[[ length is just delta.\\n      float deltaDir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of the change in indexing after ro...</td>\n",
       "      <td>[[Davis Bennett said: after rotation of the `[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>React uses an explicit top-down data flow, wit...</td>\n",
       "      <td>[[, this.camera)\\n        // Updated, for came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>During warping, values for img must be reconst...</td>\n",
       "      <td>[[Davis Bennett said: after rotation of the `[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The purpose of the 'Introduction to the Wiki' ...</td>\n",
       "      <td>[[WikiTalkSept2006\\nTitle: WikiTalkSept2006\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of setting this.clipFillingPlane.v...</td>\n",
       "      <td>[[.src = canvas.toDataURL()\\n        img.style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>This command is used to get help and diagnosti...</td>\n",
       "      <td>[[Alphafold\\nTitle: Alphafold\\nAuthors: Goran ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>nan</td>\n",
       "      <td>[[ mount the storage\\n\\n  * Is the computer is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of the two Code Review Interest Gr...</td>\n",
       "      <td>[[William Katz said: The next two Code Review ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The UniClust30_database_path provides a refere...</td>\n",
       "      <td>[[uniref90/uniref90.fasta \\\\n    --mgnify_data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>According to the context, there are no overnig...</td>\n",
       "      <td>[[Mark Kittisopikul said: Grabbed some photos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The main difference between Akaike Information...</td>\n",
       "      <td>[[Mark Kittisopikul said: Greg Fleishman, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Here is the question that can be fully answere...</td>\n",
       "      <td>The steps involved in generating and aligning ...</td>\n",
       "      <td>[[ process.  \\n  \\n\\n    2. Create a render st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The key settings used to generate and configur...</td>\n",
       "      <td>[[    }\\n      componentWillUnmount() {\\n     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some of the challenges when using h5py with di...</td>\n",
       "      <td>[[ the big headaches when trying to use h5py w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The limitations of translation and conversion ...</td>\n",
       "      <td>[[William Katz said: https://www.biorxiv.org/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The sequence file contains the FASTA paths of ...</td>\n",
       "      <td>[[Alphafold\\nTitle: Alphafold\\nAuthors: Goran ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The issue with Cling's use of LLVM is that it ...</td>\n",
       "      <td>[[Mark Kittisopikul said: https://root.cern/bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>What changes does the Viewer class make to sup...</td>\n",
       "      <td>The Viewer class supports a camera that has mo...</td>\n",
       "      <td>[[ // \"RedFormat\" is not documented, but used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Synology Inc. centralizes data storage and bac...</td>\n",
       "      <td>[[Mark Kittisopikul said: I've seen Synology u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Here's a possible rewritten version:\\n\\n\"What ...</td>\n",
       "      <td>The main differences between the proposed chun...</td>\n",
       "      <td>[[Mark Kittisopikul said: I posted a compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Here's a rewritten question that meets the rul...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[rootDirectory /nrs/flyem/render/scapes/khair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Here is a rewritten version of the question th...</td>\n",
       "      <td>To manage share access for SciComp OU groups, ...</td>\n",
       "      <td>[[Troubleshooting common Janelia Shared Storag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>The key challenge when applying Dask.array.coa...</td>\n",
       "      <td>[[Davis Bennett said: if anyone needs to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>Flipping the depth test when rendering the cli...</td>\n",
       "      <td>[[.src = canvas.toDataURL()\\n        img.style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>What drives tile pair generation in the alignm...</td>\n",
       "      <td>Tile pair generation in the alignment process ...</td>\n",
       "      <td>[[ are intensity based (due to the lower\\nreso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>React's approach to handling mouse events in c...</td>\n",
       "      <td>[[, this.camera)\\n        // Updated, for came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Here's a possible rewritten version:\\n\\n\"What ...</td>\n",
       "      <td>The main app component loads volume data into ...</td>\n",
       "      <td>[[SCA 3D Graphics Solution: Direct Volume Rend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>What's the sweet spot between model simplicity...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[Mark Kittisopikul said: Sometimes the simpli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>What is the main difference between Docker and...</td>\n",
       "      <td>Docker containers expect you to enter as root ...</td>\n",
       "      <td>[[Robert Lines said: The biggest gotcha we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Here's a rewritten version of the question:\\n\\...</td>\n",
       "      <td>The answer to given question is present in con...</td>\n",
       "      <td>[[Mark Kittisopikul said: We are moving How To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The recommended approach for storing immutable...</td>\n",
       "      <td>[[William Katz said: Konrad Rokicki Davis Benn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The `warp` command in Julia produces an output...</td>\n",
       "      <td>[[ points are handled - pass img as an\\n  Abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>The agenda has not been set, but it's more abo...</td>\n",
       "      <td>[[Mark Kittisopikul said: Is there anyone else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The code snippet generates a test texture for ...</td>\n",
       "      <td>[[ data.byteLength + \" bytes, converted \" + da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>What is the main difference between Docker and...</td>\n",
       "      <td>The main difference between Docker and Singula...</td>\n",
       "      <td>[[Robert Lines said: The biggest gotcha we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>When test data kicks in, there is an initial r...</td>\n",
       "      <td>[[ data.byteLength + \" bytes, converted \" + da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[Philip Hubbard said: FastAI supports \"augmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The purpose of adding items to the 'Need Post ...</td>\n",
       "      <td>[[63x-left_dorsal. Adding to folder Need Post ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>NEW_WEBSITE_NAME, dbi, DBI_STRING, username, U...</td>\n",
       "      <td>[[ the lines have been loaded previously\\n\\n##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The file permission issue might have been caus...</td>\n",
       "      <td>[[Greg Fleishman said: Is anyone aware of any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>To create a mini-stack using the spark process...</td>\n",
       "      <td>[[rootDirectory /nrs/flyem/render/scapes/khair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The system will extract information from the i...</td>\n",
       "      <td>[[SAGE imagery indexer\\nTitle: SAGE imagery in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>When you enable test data, it triggers an extr...</td>\n",
       "      <td>[[ data.byteLength + \" bytes, converted \" + da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>It is not currently possible to create a Gauss...</td>\n",
       "      <td>[[Davis Bennett said: if anyone needs to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>[[Mark Kittisopikul said: I will await your up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some examples of how to perform diagnostics on...</td>\n",
       "      <td>[[ full alignment solve. The following are som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Juno is a Python-based graphical package for o...</td>\n",
       "      <td>[[Srini said: https://twitter.com/mariescopy/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The two image alignment pipelines discussed in...</td>\n",
       "      <td>[[\\napproximation as a rough alignment. The ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The steps involved in uploading imagery for Ne...</td>\n",
       "      <td>[[Uploading imagery for NeuronBridge\\nTitle: U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Here's a rewritten question that requires mult...</td>\n",
       "      <td>To synchronize publication decisions with SAGE...</td>\n",
       "      <td>[[How to Synchronize Publication Decisions to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Here's a rewritten version of the question tha...</td>\n",
       "      <td>To balance performance for heavy write scenari...</td>\n",
       "      <td>[[PostgreSQL Performance Tuning\\nTitle: Postgr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Here's a rewritten question that requires mult...</td>\n",
       "      <td>The main differences between ZEP2 chunk index ...</td>\n",
       "      <td>[[Mark Kittisopikul said: I posted a compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Here's the question that can be fully answered...</td>\n",
       "      <td>Rust isn't the best choice if your app needs a...</td>\n",
       "      <td>[[William Katz said: tl;dr: Rust is good at so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Science team leads are invited to discuss thei...</td>\n",
       "      <td>[[Talk Science with SciComp Software\\nTitle: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Science team leads are invited to discuss thei...</td>\n",
       "      <td>[[Talk Science with SciComp Software\\nTitle: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>When troubleshooting issues with accessing sha...</td>\n",
       "      <td>[[ (devfs, local, nobrowse)\\n        /dev/disk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The recommended way to close open objects rela...</td>\n",
       "      <td>[[ the big headaches when trying to use h5py w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The different options available to run the Sag...</td>\n",
       "      <td>[[Sage-based External Website Cross-loader\\nTi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The necessary environment variables and JWT to...</td>\n",
       "      <td>[[Uploading imagery for NeuronBridge\\nTitle: U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The challenges and limitations associated with...</td>\n",
       "      <td>[[Flattening\\nTitle: Flattening\\nAuthors: Unkn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>The test is checking if the result of the GGE ...</td>\n",
       "      <td>[[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>What parameters can be used in sage_json.php, ...</td>\n",
       "      <td>The parameters that can be used in sage_json.p...</td>\n",
       "      <td>[[SAGE Ajax\\nTitle: SAGE Ajax\\nAuthors: Rob Sv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Here's a rewritten question that requires mult...</td>\n",
       "      <td>The typical workflow for uploading searchable ...</td>\n",
       "      <td>[[-flylight-color-depth\\n\\nThe code you'll nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Here's a rewritten version that requires multi...</td>\n",
       "      <td>The proposed fix for file format heterogeneity...</td>\n",
       "      <td>[[Mark Kittisopikul said: \\nHello *&lt;!channel&gt;*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The limitations that led K-Optional Software t...</td>\n",
       "      <td>[[William Katz said: Recent blog post on why a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>When evaluating the complexity of building app...</td>\n",
       "      <td>[[Evaluation of components for building apps w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Here's a possible rewritten question:\\n\\n\"What...</td>\n",
       "      <td>The significance of `MAX_STEPS` in ray marchin...</td>\n",
       "      <td>[[ length is just delta.\\n      float deltaDir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>Some challenges faced when working with ITK fo...</td>\n",
       "      <td>[[Davis Bennett said: can anyone make sense of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>Synology is used to centralize data storage an...</td>\n",
       "      <td>[[Mark Kittisopikul said: I've seen Synology u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Here's a question that can be fully answered f...</td>\n",
       "      <td>The key evaluation criteria for choosing a bac...</td>\n",
       "      <td>[[Evaluation of components for building apps w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Here's a rewritten question that conveys the s...</td>\n",
       "      <td>When deciding between different tech stacks fo...</td>\n",
       "      <td>[[Evaluation of components for building apps w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Here's a possible rewritten question:\\n\\n\"How ...</td>\n",
       "      <td>To restart an NX NoMachine session on a Login ...</td>\n",
       "      <td>[[Unresponsive NX NoMachine session on a Login...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Here's a rewritten question that conveys the s...</td>\n",
       "      <td>Current data access approaches have limitation...</td>\n",
       "      <td>[[William Katz said: https://www.biorxiv.org/c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Here's a question that can be fully answered f...   \n",
       "1    Here is a question that can be fully answered ...   \n",
       "2    Here's a rewritten version of the question:\\n\\...   \n",
       "3    Here's the question that can be fully answered...   \n",
       "4    Here's a question that can be fully answered f...   \n",
       "5    Here's a rewritten question that conveys the s...   \n",
       "6    Here is a question that can be fully answered ...   \n",
       "7    Here is a question that can be fully answered ...   \n",
       "8    Here is a question that can be fully answered ...   \n",
       "9    Here's the question that can be fully answered...   \n",
       "10   Here is a question that can be fully answered ...   \n",
       "11   Here is a question that can be fully answered ...   \n",
       "12   Based on the given context, here's a question ...   \n",
       "13   Here's a question that can be fully answered b...   \n",
       "14   Here's a rewritten version of the question:\\n\\...   \n",
       "15   What's on the agenda for the Next Gen File For...   \n",
       "16   Here's a rewritten version of the question:\\n\\...   \n",
       "17   Here's a question that can be fully answered b...   \n",
       "18   Here is a question that can be fully answered ...   \n",
       "19   Based on the given context, a question that ca...   \n",
       "20   Here's a question that can be fully answered f...   \n",
       "21   Here's a question that can be fully answered f...   \n",
       "22   Here is a question that can be fully answered ...   \n",
       "23   Here is a question that can be fully answered ...   \n",
       "24   Here is a question that can be fully answered ...   \n",
       "25   Here is a question that can be fully answered ...   \n",
       "26   Here's a question that can be fully answered f...   \n",
       "27   How does integrating package management within...   \n",
       "28   Here's a rewritten version of the question:\\n\\...   \n",
       "29   Here's a question that can be fully answered f...   \n",
       "30   Here's the question that can be fully answered...   \n",
       "31   What are this week's office hour timings, as c...   \n",
       "32   Here's the question:\\n\\n\"What are the proposed...   \n",
       "33   Here's a question that can be fully answered f...   \n",
       "34   Here's a question that can be fully answered f...   \n",
       "35   Here's a question that can be fully answered f...   \n",
       "36   Here is a question that can be fully answered ...   \n",
       "37   What are the main differences between Docker a...   \n",
       "38   Here's the question that can be fully answered...   \n",
       "39   Here's a question that can be fully answered f...   \n",
       "40   Here is a question that can be fully answered ...   \n",
       "41   Here's the question that can be fully answered...   \n",
       "42   Here's a rewritten question that requires mult...   \n",
       "43   Here's a rewritten question that conveys the s...   \n",
       "44   Here's the question that can be fully answered...   \n",
       "45   Here is a question that can be fully answered ...   \n",
       "46   Here's a rewritten version of the question tha...   \n",
       "47   Here is a question that can be fully answered ...   \n",
       "48   Here is a question that can be fully answered ...   \n",
       "49   Here's a question that can be fully answered f...   \n",
       "50   Here is a question that can be fully answered ...   \n",
       "51   What is the main difference between Docker and...   \n",
       "52   Here is a rewritten version of the question:\\n...   \n",
       "53   Here's a question that can be fully answered f...   \n",
       "54   Here's a question that can be fully answered f...   \n",
       "55   Here is a question that can be fully answered ...   \n",
       "56   Here's the question that can be fully answered...   \n",
       "57   Here is a question that can be fully answered ...   \n",
       "58   What is the main difference between Docker and...   \n",
       "59   Here's a question that can be fully answered f...   \n",
       "60   Here's a rewritten version of the question tha...   \n",
       "61   Here's a rewritten version of the question tha...   \n",
       "62   Here's a question that can be fully answered f...   \n",
       "63   Here is the question that can be fully answere...   \n",
       "64   Here's a question that can be fully answered f...   \n",
       "65   Here's a question that can be fully answered f...   \n",
       "66   Here's a rewritten question that meets the rul...   \n",
       "67   Here is a question that can be fully answered ...   \n",
       "68   Here's a question that can be fully answered f...   \n",
       "69   Here's the question that can be fully answered...   \n",
       "70   Here is a question that can be fully answered ...   \n",
       "71   Here is a question that can be fully answered ...   \n",
       "72   Based on the given context, a suitable questio...   \n",
       "73   Here's a question that can be fully answered f...   \n",
       "74   What's Julia's way to identify anonymous varia...   \n",
       "75   Here's a question that can be fully answered f...   \n",
       "76   Here is a question that can be fully answered ...   \n",
       "77   Here is the rewritten question:\\n\\n\"What makes...   \n",
       "78   Here is a question that can be fully answered ...   \n",
       "79   Here's a question that can be fully answered b...   \n",
       "80   Here's a rewritten version of the question tha...   \n",
       "81   Here's a question that can be fully answered f...   \n",
       "82   Here is a question that can be fully answered ...   \n",
       "83   Here's a question that can be fully answered f...   \n",
       "84   What challenges did Hannah face building FicTr...   \n",
       "85   Here's a rewritten question that combines info...   \n",
       "86   What causes SSH connection issues, and how doe...   \n",
       "87   Here is a question that can be fully answered ...   \n",
       "88   Here's a question that can be fully answered f...   \n",
       "89   Here is a question that can be fully answered ...   \n",
       "90   Here's a question that can be fully answered f...   \n",
       "91   Based on the given context, a question that ca...   \n",
       "92   Here is a question that can be fully answered ...   \n",
       "93   Here's a question that can be fully answered f...   \n",
       "94   Here is a question that can be fully answered ...   \n",
       "95   Based on the given context, a suitable questio...   \n",
       "96   Here's a question that can be fully answered f...   \n",
       "97   Here's a question that can be fully answered f...   \n",
       "98   Here's a question that can be fully answered f...   \n",
       "99   Here is a question that can be fully answered ...   \n",
       "100  Here is a question that can be fully answered ...   \n",
       "101  Here's a question that can be fully answered f...   \n",
       "102  Here is a question that can be fully answered ...   \n",
       "103  Here's a question that can be fully answered f...   \n",
       "104  Here's a question that can be fully answered f...   \n",
       "105  Here is a question that can be fully answered ...   \n",
       "106  Here's a question that can be fully answered f...   \n",
       "107  Here's a question that can be fully answered f...   \n",
       "108  Here's a rewritten question that conveys the s...   \n",
       "109  Here's a rewritten version of the question:\\n\\...   \n",
       "110  Here is a rewritten version of the question th...   \n",
       "111  Here's a question that can be fully answered f...   \n",
       "112  Here is a question that can be fully answered ...   \n",
       "113  Here is a question that can be fully answered ...   \n",
       "114  Here is a question that can be fully answered ...   \n",
       "115  Here's a question that can be fully answered f...   \n",
       "116  What's the solution for Philip Hubbard's probl...   \n",
       "117  What's used for exploring matches on the dashb...   \n",
       "118  Here is the question that can be fully answere...   \n",
       "119  Here's the question that can be fully answered...   \n",
       "120  Here's the question that can be fully answered...   \n",
       "121  What percentage of RAM should the `shared_buff...   \n",
       "122  Here's a question that can be fully answered b...   \n",
       "123  Here is a question that can be fully answered ...   \n",
       "124  Based on the given context, a suitable questio...   \n",
       "125  Here's a question that can be fully answered f...   \n",
       "126  What are the conditions that need to be met in...   \n",
       "127  Here is a rewritten question that conveys the ...   \n",
       "128  Here's a rewritten version of the question:\\n\\...   \n",
       "129  How can PostgreSQL performance be optimized fo...   \n",
       "130  Here is the question that can be fully answere...   \n",
       "131  What changes does the Viewer class make to sup...   \n",
       "132  Here's a question that can be fully answered f...   \n",
       "133  Here is a question that can be fully answered ...   \n",
       "134  Here is a question that can be fully answered ...   \n",
       "135  What is the significance of the camera's posit...   \n",
       "136  Here is a question that can be fully answered ...   \n",
       "137  Here is a question that can be fully answered ...   \n",
       "138  Here is a question that can be fully answered ...   \n",
       "139  Here is a question that can be fully answered ...   \n",
       "140  Here's a question that can be fully answered f...   \n",
       "141  Here is a question that can be fully answered ...   \n",
       "142  Here is a question that can be fully answered ...   \n",
       "143  Here's a question that can be fully answered f...   \n",
       "144  Here is a question that can be fully answered ...   \n",
       "145  Here's a question that can be fully answered f...   \n",
       "146  Here is a question that can be fully answered ...   \n",
       "147  Here is a question that can be fully answered ...   \n",
       "148  Here is the question that can be fully answere...   \n",
       "149  Here is a question that can be fully answered ...   \n",
       "150  Here is a question that can be fully answered ...   \n",
       "151  Here is a question that can be fully answered ...   \n",
       "152  Here is a question that can be fully answered ...   \n",
       "153  Here's a question that can be fully answered f...   \n",
       "154  What changes does the Viewer class make to sup...   \n",
       "155  Here's a question that can be fully answered f...   \n",
       "156  Here's a possible rewritten version:\\n\\n\"What ...   \n",
       "157  Here's a rewritten question that meets the rul...   \n",
       "158  Here is a rewritten version of the question th...   \n",
       "159  Here's a rewritten version of the question tha...   \n",
       "160  Here's a rewritten version of the question tha...   \n",
       "161  What drives tile pair generation in the alignm...   \n",
       "162  Here's a rewritten version of the question tha...   \n",
       "163  Here's a possible rewritten version:\\n\\n\"What ...   \n",
       "164  What's the sweet spot between model simplicity...   \n",
       "165  What is the main difference between Docker and...   \n",
       "166  Here's a rewritten version of the question:\\n\\...   \n",
       "167  Here's a question that can be fully answered f...   \n",
       "168  Here's a question that can be fully answered f...   \n",
       "169  Here's the question that can be fully answered...   \n",
       "170  Here is a question that can be fully answered ...   \n",
       "171  What is the main difference between Docker and...   \n",
       "172  Here's a rewritten version of the question tha...   \n",
       "173  Here's a rewritten version of the question tha...   \n",
       "174  Here is a question that can be fully answered ...   \n",
       "175  Here is a question that can be fully answered ...   \n",
       "176  Here is a question that can be fully answered ...   \n",
       "177  Here's a question that can be fully answered f...   \n",
       "178  Here is a question that can be fully answered ...   \n",
       "179  Here's a rewritten version of the question tha...   \n",
       "180  Here's a question that can be fully answered f...   \n",
       "181  Here's a question that can be fully answered f...   \n",
       "182  Here is a question that can be fully answered ...   \n",
       "183  Here is a question that can be fully answered ...   \n",
       "184  Here is a question that can be fully answered ...   \n",
       "185  Here is a question that can be fully answered ...   \n",
       "186  Here's a rewritten question that requires mult...   \n",
       "187  Here's a rewritten version of the question tha...   \n",
       "188  Here's a rewritten question that requires mult...   \n",
       "189  Here's the question that can be fully answered...   \n",
       "190  Here is a question that can be fully answered ...   \n",
       "191  Here is a question that can be fully answered ...   \n",
       "192  Here is a question that can be fully answered ...   \n",
       "193  Here is a question that can be fully answered ...   \n",
       "194  Here is a question that can be fully answered ...   \n",
       "195  Here is a question that can be fully answered ...   \n",
       "196  Here is a question that can be fully answered ...   \n",
       "197  Here is a question that can be fully answered ...   \n",
       "198  What parameters can be used in sage_json.php, ...   \n",
       "199  Here's a rewritten question that requires mult...   \n",
       "200  Here's a rewritten version that requires multi...   \n",
       "201  Here's a question that can be fully answered f...   \n",
       "202  Here's a question that can be fully answered f...   \n",
       "203  Here's a possible rewritten question:\\n\\n\"What...   \n",
       "204  Here is a question that can be fully answered ...   \n",
       "205  Here's a question that can be fully answered f...   \n",
       "206  Here's a question that can be fully answered f...   \n",
       "207  Here's a rewritten question that conveys the s...   \n",
       "208  Here's a possible rewritten question:\\n\\n\"How ...   \n",
       "209  Here's a rewritten question that conveys the s...   \n",
       "\n",
       "                                          ground_truth  \\\n",
       "0    The primary purpose of Akaike information crit...   \n",
       "1    The purpose of the Akaike information criterio...   \n",
       "2    AIC aims to reconcile the trade-off between th...   \n",
       "3    The embedded ChatGPT-like interface has tricks...   \n",
       "4    Sequential storage can have benefits even in h...   \n",
       "5    Sequential storage (i.e., trying to maximize a...   \n",
       "6    The matrix operations performed in this code s...   \n",
       "7    The test is checking the accuracy of the GGESE...   \n",
       "8    The purpose of the QZ decomposition in numeric...   \n",
       "9    The embedded ChatGPT-like interface has featur...   \n",
       "10   The test is checking the accuracy of the GGESE...   \n",
       "11                                       Python 3.10.6   \n",
       "12   The issue causing the kernel crash when using ...   \n",
       "13   During the Next Gen File Formats session at th...   \n",
       "14   The answer to given question is not present in...   \n",
       "15   During the OME conference next week they will ...   \n",
       "16   The answer to given question is not present in...   \n",
       "17   During the 'Next Gen File Formats' session at ...   \n",
       "18   To attach VSCode to a bsub session for remote ...   \n",
       "19   The differences between the arrays 'a', 'b', a...   \n",
       "20   Gert-Jan Both made some changes to the API of ...   \n",
       "21   Running Pkg.update() in a Julia session update...   \n",
       "22   The test is checking if the result of the GGE ...   \n",
       "23   The test is checking if the result of the GGE ...   \n",
       "24   During warping, values for img must be reconst...   \n",
       "25   The expected result of the q @ s @ z.conj().T ...   \n",
       "26   The purpose of the QZ decomposition in linear ...   \n",
       "27   Integrating package management within a progra...   \n",
       "28   You can run a remote VScode session on the sub...   \n",
       "29   Some common topics of humor and criticism abou...   \n",
       "30   Science has developed a *ton* of internal tool...   \n",
       "31   Srini's office hours will begin next week on T...   \n",
       "32   The proposed office hour arrangements are Srin...   \n",
       "33   Davis Bennett suggests that the h5py API shoul...   \n",
       "34   Gert-Jan Both made some changes to the API of ...   \n",
       "35   The purpose of the QZ decomposition in this ma...   \n",
       "36                                  The kernel crashes   \n",
       "37   The main differences between Docker and Singul...   \n",
       "38   The proposed pace of the course is to have a v...   \n",
       "39   To attach VSCode to a bsub session for debuggi...   \n",
       "40                                       Python 3.10.6   \n",
       "41   Davis Bennett's complaint about h5py is that i...   \n",
       "42   Konrad Rokicki's explanation reveals that by d...   \n",
       "43   Robert Lines and Konrad Rokicki balance memory...   \n",
       "44   The autogenerated filenames in the h5py source...   \n",
       "45   The issue with running tasks on workers in a d...   \n",
       "46   h5py doesn't provide a straightforward way to ...   \n",
       "47   The issue of not being able to remove intermed...   \n",
       "48   The issue might be caused by a version mismatc...   \n",
       "49   The limitations of translation and conversion ...   \n",
       "50   To attach VScode to an interactive bsub sessio...   \n",
       "51   The main difference between Docker and Singula...   \n",
       "52   The answer to given question can be inferred f...   \n",
       "53   Nextflow's current approach to memory manageme...   \n",
       "54   The `warp` command in Interpolations.jl produc...   \n",
       "55   The How To interest group meetings will move t...   \n",
       "56   Davis Bennett's complaint is that h5py hangs o...   \n",
       "57   The changes made to the chromatic package rega...   \n",
       "58   The context does not mention Docker or Singula...   \n",
       "59   To address challenges created by heterogeneity...   \n",
       "60   The presenter is Luca Marconato, and the focus...   \n",
       "61   The main differences between the proposed chun...   \n",
       "62   The main differences between the proposed chun...   \n",
       "63   Yann was bored and working as a project manage...   \n",
       "64   Some common topics of humor and criticism rega...   \n",
       "65   The limitations of translation and conversion ...   \n",
       "66   OME-NGFF introduces next-generation file forma...   \n",
       "67   The differences in building FicTrac using diff...   \n",
       "68   Davis Bennett said that the talk about catalog...   \n",
       "69   Lego robotics was introduced as an interesting...   \n",
       "70   Yann was bored and working as a project manage...   \n",
       "71   Yann was bored and working as a project manage...   \n",
       "72   The purpose of the `@multimethod` macro in Jul...   \n",
       "73   The main differences between the proposed chun...   \n",
       "74   The answer is present in context, specifically...   \n",
       "75   Davis Bennett's complaint is that h5py hangs o...   \n",
       "76   Some of the challenges and differences when ru...   \n",
       "77   Juno is suitable for simulating optical system...   \n",
       "78   To attach VSCode to a bsub session for debuggi...   \n",
       "79   The factors that contributed to the spread of ...   \n",
       "80   Gert-Jan Both highlights that using asserts in...   \n",
       "81   Developers face challenges such as a complete ...   \n",
       "82   You should be able to automatically create the...   \n",
       "83   According to Mark Kittisopikul, common feature...   \n",
       "84   Hannah faced challenges with building FicTrac ...   \n",
       "85   Multimethod dispatch in Julia enables multimet...   \n",
       "86   The `login1` ssh host key has changed recently...   \n",
       "87   The issue with running tasks on workers in the...   \n",
       "88   The main differences between the HDF5 fixed ar...   \n",
       "89   The following changes were made to the chromat...   \n",
       "90   Arraylake is a data lake platform for managing...   \n",
       "91   Juno allows users to design and visualise arbi...   \n",
       "92   The reason for sending some features upstream ...   \n",
       "93   Sequential storage (i.e., trying to maximize a...   \n",
       "94   According to Mark Kittisopikul, the reason for...   \n",
       "95                                                 nan   \n",
       "96   The limitations that led K-Optional Software t...   \n",
       "97   According to Robert Lines, the current approac...   \n",
       "98   Discourse is a common hosting solution used by...   \n",
       "99   Some potential challenges or requirements for ...   \n",
       "100  The test is checking the accuracy of the GGESE...   \n",
       "101            The RSA host key for login1 has changed   \n",
       "102  The four main steps involved in aligning overl...   \n",
       "103  Davis Bennett suggests exposing a __exit__() m...   \n",
       "104  The 7000 series train cars used by Mark Kittis...   \n",
       "105  According to Mark Kittisopikul, there are no o...   \n",
       "106  The @multimethod macro is used to define multi...   \n",
       "107  The purpose of hosting a \"virtual\" N5 file via...   \n",
       "108  Developers typically encounter challenges when...   \n",
       "109  The scheduled call time for Mark Kittisopikul'...   \n",
       "110  The retry mechanism in Nextflow prevents jobs ...   \n",
       "111  Nextflow can retry jobs with more memory, as m...   \n",
       "112  The purpose was to introduce and demonstrate t...   \n",
       "113  The primary functions of the SAGE imagery inde...   \n",
       "114  Some potential issues or considerations when u...   \n",
       "115  Mamba ended up taking maybe 2 min. Conda took ...   \n",
       "116  Philip Hubbard's solution for the problem with...   \n",
       "117  To view and navigate tile collections in the R...   \n",
       "118  Go is an open source programming language that...   \n",
       "119  The tentative schedule for the call with HDF G...   \n",
       "120  The purpose of the call is to allow participan...   \n",
       "121                                  1/4 to 1/3 of RAM   \n",
       "122  According to William Katz, the estimated numbe...   \n",
       "123  The SAGE imagery indexer will extract informat...   \n",
       "124  The key differences between the FastAI approac...   \n",
       "125  The limitations that led K-Optional Software t...   \n",
       "126  The conditions that need to be met in order to...   \n",
       "127  Introducing chess by playing with general rule...   \n",
       "128  The answer to given question is present in con...   \n",
       "129  Configuration settings for PostgreSQL performa...   \n",
       "130  The code snippet closes objects. It first call...   \n",
       "131  The Viewer class makes several changes to supp...   \n",
       "132  According to Mark Kittisopikul, common feature...   \n",
       "133  The purpose of setting the `pixelRatio` and `s...   \n",
       "134  The focus of the first Code Review Interest Gr...   \n",
       "135  The camera's position is significant in determ...   \n",
       "136  The purpose of the ray marching iterations is ...   \n",
       "137  The purpose of the change in indexing after ro...   \n",
       "138  React uses an explicit top-down data flow, wit...   \n",
       "139  During warping, values for img must be reconst...   \n",
       "140  The purpose of the 'Introduction to the Wiki' ...   \n",
       "141  The purpose of setting this.clipFillingPlane.v...   \n",
       "142  This command is used to get help and diagnosti...   \n",
       "143                                                nan   \n",
       "144  The purpose of the two Code Review Interest Gr...   \n",
       "145  The UniClust30_database_path provides a refere...   \n",
       "146  According to the context, there are no overnig...   \n",
       "147  The main difference between Akaike Information...   \n",
       "148  The steps involved in generating and aligning ...   \n",
       "149  The key settings used to generate and configur...   \n",
       "150  Some of the challenges when using h5py with di...   \n",
       "151  The limitations of translation and conversion ...   \n",
       "152  The sequence file contains the FASTA paths of ...   \n",
       "153  The issue with Cling's use of LLVM is that it ...   \n",
       "154  The Viewer class supports a camera that has mo...   \n",
       "155  Synology Inc. centralizes data storage and bac...   \n",
       "156  The main differences between the proposed chun...   \n",
       "157  The answer to given question is not present in...   \n",
       "158  To manage share access for SciComp OU groups, ...   \n",
       "159  The key challenge when applying Dask.array.coa...   \n",
       "160  Flipping the depth test when rendering the cli...   \n",
       "161  Tile pair generation in the alignment process ...   \n",
       "162  React's approach to handling mouse events in c...   \n",
       "163  The main app component loads volume data into ...   \n",
       "164  The answer to given question is not present in...   \n",
       "165  Docker containers expect you to enter as root ...   \n",
       "166  The answer to given question is present in con...   \n",
       "167  The recommended approach for storing immutable...   \n",
       "168  The `warp` command in Julia produces an output...   \n",
       "169  The agenda has not been set, but it's more abo...   \n",
       "170  The code snippet generates a test texture for ...   \n",
       "171  The main difference between Docker and Singula...   \n",
       "172  When test data kicks in, there is an initial r...   \n",
       "173  The answer to given question is not present in...   \n",
       "174  The purpose of adding items to the 'Need Post ...   \n",
       "175  NEW_WEBSITE_NAME, dbi, DBI_STRING, username, U...   \n",
       "176  The file permission issue might have been caus...   \n",
       "177  To create a mini-stack using the spark process...   \n",
       "178  The system will extract information from the i...   \n",
       "179  When you enable test data, it triggers an extr...   \n",
       "180  It is not currently possible to create a Gauss...   \n",
       "181  The answer to given question is not present in...   \n",
       "182  Some examples of how to perform diagnostics on...   \n",
       "183  Juno is a Python-based graphical package for o...   \n",
       "184  The two image alignment pipelines discussed in...   \n",
       "185  The steps involved in uploading imagery for Ne...   \n",
       "186  To synchronize publication decisions with SAGE...   \n",
       "187  To balance performance for heavy write scenari...   \n",
       "188  The main differences between ZEP2 chunk index ...   \n",
       "189  Rust isn't the best choice if your app needs a...   \n",
       "190  Science team leads are invited to discuss thei...   \n",
       "191  Science team leads are invited to discuss thei...   \n",
       "192  When troubleshooting issues with accessing sha...   \n",
       "193  The recommended way to close open objects rela...   \n",
       "194  The different options available to run the Sag...   \n",
       "195  The necessary environment variables and JWT to...   \n",
       "196  The challenges and limitations associated with...   \n",
       "197  The test is checking if the result of the GGE ...   \n",
       "198  The parameters that can be used in sage_json.p...   \n",
       "199  The typical workflow for uploading searchable ...   \n",
       "200  The proposed fix for file format heterogeneity...   \n",
       "201  The limitations that led K-Optional Software t...   \n",
       "202  When evaluating the complexity of building app...   \n",
       "203  The significance of `MAX_STEPS` in ray marchin...   \n",
       "204  Some challenges faced when working with ITK fo...   \n",
       "205  Synology is used to centralize data storage an...   \n",
       "206  The key evaluation criteria for choosing a bac...   \n",
       "207  When deciding between different tech stacks fo...   \n",
       "208  To restart an NX NoMachine session on a Login ...   \n",
       "209  Current data access approaches have limitation...   \n",
       "\n",
       "                                              contexts  \n",
       "0    [[Mark Kittisopikul said: Greg Fleishman, I wa...  \n",
       "1    [[Mark Kittisopikul said: Greg Fleishman, I wa...  \n",
       "2    [[Mark Kittisopikul said: Greg Fleishman, I wa...  \n",
       "3    [[William Katz said: https://twitter.com/maxho...  \n",
       "4    [[William Katz said: Some links from my second...  \n",
       "5    [[William Katz said: Some links from my second...  \n",
       "6    [[.10534273, -0.5556871 ,\\n         0.8352932 ...  \n",
       "7    [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "8    [[.10534273, -0.5556871 ,\\n         0.8352932 ...  \n",
       "9    [[William Katz said: https://twitter.com/maxho...  \n",
       "10   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "11   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "12   [[.        ,  0.        ,  0.        ,  0.    ...  \n",
       "13   [[Ulrike Boehm said: For those of you interest...  \n",
       "14   [[.10534273, -0.5556871 ,\\n         0.8352932 ...  \n",
       "15   [[Ulrike Boehm said: For those of you interest...  \n",
       "16   [[.        ,  0.        ,  0.        ,  0.    ...  \n",
       "17   [[Ulrike Boehm said: For those of you interest...  \n",
       "18   [[Eric Wait said: Has anyone attached VScode t...  \n",
       "19   [[ol=1.19209e-05\\nE   \\nE   Mismatched element...  \n",
       "20   [[Gert-Jan Both said: Welcome everyone! As you...  \n",
       "21   [[Mark Kittisopikul said: Something like the e...  \n",
       "22   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "23   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "24   [[Davis Bennett said: after rotation of the `[...  \n",
       "25   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "26   [[.10534273, -0.5556871 ,\\n         0.8352932 ...  \n",
       "27   [[Mark Kittisopikul said: Something like the e...  \n",
       "28   [[Eric Wait said: Has anyone attached VScode t...  \n",
       "29   [[Mark Kittisopikul said: I will await your up...  \n",
       "30   [[William Katz said: https://twitter.com/maxho...  \n",
       "31   [[William Katz said:  I propose we start this ...  \n",
       "32   [[William Katz said:  I propose we start this ...  \n",
       "33   [[Davis Bennett said: loving these filenames i...  \n",
       "34   [[Gert-Jan Both said: Welcome everyone! As you...  \n",
       "35   [[.10534273, -0.5556871 ,\\n         0.8352932 ...  \n",
       "36   [[.        ,  0.        ,  0.        ,  0.    ...  \n",
       "37   [[Robert Lines said: The biggest gotcha we hav...  \n",
       "38   [[William Katz said:  I propose we start this ...  \n",
       "39   [[Eric Wait said: Has anyone attached VScode t...  \n",
       "40   [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "41   [[Davis Bennett said: loving these filenames i...  \n",
       "42   [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "43   [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "44   [[Davis Bennett said: loving these filenames i...  \n",
       "45   [[ in get\\n      results = self.gather(packed,...  \n",
       "46   [[ the big headaches when trying to use h5py w...  \n",
       "47   [[Greg Fleishman said: Is anyone aware of any ...  \n",
       "48   [[ in get\\n      results = self.gather(packed,...  \n",
       "49   [[William Katz said: https://www.biorxiv.org/c...  \n",
       "50   [[Eric Wait said: Has anyone attached VScode t...  \n",
       "51   [[Robert Lines said: The biggest gotcha we hav...  \n",
       "52   [[Mary Lay said: Best way to set up mamba when...  \n",
       "53   [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "54   [[ points are handled - pass img as an\\n  Abs...  \n",
       "55   [[Mark Kittisopikul said: We are moving How To...  \n",
       "56   [[Davis Bennett said: loving these filenames i...  \n",
       "57   [[_ramps`) now return phase directly as `ndarr...  \n",
       "58   [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "59   [[Mark Kittisopikul said: \\nHello *<!channel>*...  \n",
       "60   [[Mark Kittisopikul said: \\nHello *<!channel>*...  \n",
       "61   [[Mark Kittisopikul said: I posted a compariso...  \n",
       "62   [[Mark Kittisopikul said: I posted a compariso...  \n",
       "63   [[William Katz said: https://corecursive.com/d...  \n",
       "64   [[Mark Kittisopikul said: I will await your up...  \n",
       "65   [[William Katz said: https://www.biorxiv.org/c...  \n",
       "66   [[William Katz said: https://www.biorxiv.org/c...  \n",
       "67   [[Davis Bennett said: Eric Wait you were recom...  \n",
       "68   [[Davis Bennett said: reminder:  today at 2:45...  \n",
       "69   [[Philip Hubbard said: My twins are in kinderg...  \n",
       "70   [[William Katz said: https://corecursive.com/d...  \n",
       "71   [[William Katz said: https://corecursive.com/d...  \n",
       "72   [[               \"argument must be a block\")\\n...  \n",
       "73   [[Mark Kittisopikul said: I posted a compariso...  \n",
       "74   [[               \"argument must be a block\")\\n...  \n",
       "75   [[Davis Bennett said: loving these filenames i...  \n",
       "76   [[Robert Lines said: The biggest gotcha we hav...  \n",
       "77   [[Srini said: https://twitter.com/mariescopy/s...  \n",
       "78   [[Eric Wait said: Has anyone attached VScode t...  \n",
       "79   [[William Katz said: Some interesting data fro...  \n",
       "80   [[\\n Examples and tests still need to be upda...  \n",
       "81   [[Davis Bennett said: can anyone make sense of...  \n",
       "82   [[David Ackerman said: I can now run a sample ...  \n",
       "83   [[Mark Kittisopikul said: I've seen Synology u...  \n",
       "84   [[Davis Bennett said: Eric Wait you were recom...  \n",
       "85   [[               \"argument must be a block\")\\n...  \n",
       "86   [[Stuart Berg said: Robert Lines Ken Carlile H...  \n",
       "87   [[ in get\\n      results = self.gather(packed,...  \n",
       "88   [[Mark Kittisopikul said: Numcodecs is in urge...  \n",
       "89   [[_ramps`) now return phase directly as `ndarr...  \n",
       "90   [[Davis Bennett said: reminder:  today at 2:45...  \n",
       "91   [[Srini said: https://twitter.com/mariescopy/s...  \n",
       "92   [[Mark Kittisopikul said: https://root.cern/bl...  \n",
       "93   [[William Katz said: Some links from my second...  \n",
       "94   [[Mark Kittisopikul said: Numcodecs is in urge...  \n",
       "95   [[Philip Hubbard said: FastAI supports \"augmen...  \n",
       "96   [[William Katz said: Recent blog post on why a...  \n",
       "97   [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "98   [[Habib Bukhari said: Or just use our own host...  \n",
       "99   [[Habib Bukhari said: Or just use our own host...  \n",
       "100  [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "101  [[Stuart Berg said: Robert Lines Ken Carlile H...  \n",
       "102  [[Alignment Guide\\nTitle: Alignment Guide\\nAut...  \n",
       "103  [[Davis Bennett said: loving these filenames i...  \n",
       "104  [[-get-more-frequent-rail-service.cfm\\nMark Ki...  \n",
       "105  [[Mark Kittisopikul said: Grabbed some photos ...  \n",
       "106  [[               \"argument must be a block\")\\n...  \n",
       "107  [[Stuart Berg said: Marwan Zouinkhi In the thr...  \n",
       "108  [[Davis Bennett said: can anyone make sense of...  \n",
       "109  [[Mark Kittisopikul said: Is there anyone else...  \n",
       "110  [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "111  [[Konrad Rokicki said: Robert Lines I was wron...  \n",
       "112  [[WikiTalkSept2006\\nTitle: WikiTalkSept2006\\nA...  \n",
       "113  [[SAGE imagery indexer\\nTitle: SAGE imagery in...  \n",
       "114  [[\\n Examples and tests still need to be upda...  \n",
       "115  [[Mary Lay said: Best way to set up mamba when...  \n",
       "116  [[Philip Hubbard said: Does anyone else have t...  \n",
       "117  [[\\napproximation as a rough alignment. The ri...  \n",
       "118  [[Mark Kittisopikul said: Go 1.18 gets generic...  \n",
       "119  [[Mark Kittisopikul said: Is there anyone else...  \n",
       "120  [[Mark Kittisopikul said: Is there anyone else...  \n",
       "121  [[PostgreSQL Performance Tuning\\nTitle: Postgr...  \n",
       "122  [[William Katz said: Some interesting data fro...  \n",
       "123  [[SAGE imagery indexer\\nTitle: SAGE imagery in...  \n",
       "124  [[Philip Hubbard said: FastAI supports \"augmen...  \n",
       "125  [[William Katz said: Recent blog post on why a...  \n",
       "126  [[Habib Bukhari said: Or just use our own host...  \n",
       "127  [[Philip Hubbard said: My twins are in kinderg...  \n",
       "128  [[Philip Hubbard said: FastAI supports \"augmen...  \n",
       "129  [[PostgreSQL Performance Tuning\\nTitle: Postgr...  \n",
       "130  [[            self.id._close_open_objects(h5f....  \n",
       "131  [[ // \"RedFormat\" is not documented, but used ...  \n",
       "132  [[Mark Kittisopikul said: I've seen Synology u...  \n",
       "133  [[     this.renderer.setClearColor(\"#000000\")\\...  \n",
       "134  [[William Katz said: The next two Code Review ...  \n",
       "135  [[, this.camera)\\n        // Updated, for came...  \n",
       "136  [[ length is just delta.\\n      float deltaDir...  \n",
       "137  [[Davis Bennett said: after rotation of the `[...  \n",
       "138  [[, this.camera)\\n        // Updated, for came...  \n",
       "139  [[Davis Bennett said: after rotation of the `[...  \n",
       "140  [[WikiTalkSept2006\\nTitle: WikiTalkSept2006\\nA...  \n",
       "141  [[.src = canvas.toDataURL()\\n        img.style...  \n",
       "142  [[Alphafold\\nTitle: Alphafold\\nAuthors: Goran ...  \n",
       "143  [[ mount the storage\\n\\n  * Is the computer is...  \n",
       "144  [[William Katz said: The next two Code Review ...  \n",
       "145  [[uniref90/uniref90.fasta \\\\n    --mgnify_data...  \n",
       "146  [[Mark Kittisopikul said: Grabbed some photos ...  \n",
       "147  [[Mark Kittisopikul said: Greg Fleishman, I wa...  \n",
       "148  [[ process.  \\n  \\n\\n    2. Create a render st...  \n",
       "149  [[    }\\n      componentWillUnmount() {\\n     ...  \n",
       "150  [[ the big headaches when trying to use h5py w...  \n",
       "151  [[William Katz said: https://www.biorxiv.org/c...  \n",
       "152  [[Alphafold\\nTitle: Alphafold\\nAuthors: Goran ...  \n",
       "153  [[Mark Kittisopikul said: https://root.cern/bl...  \n",
       "154  [[ // \"RedFormat\" is not documented, but used ...  \n",
       "155  [[Mark Kittisopikul said: I've seen Synology u...  \n",
       "156  [[Mark Kittisopikul said: I posted a compariso...  \n",
       "157  [[rootDirectory /nrs/flyem/render/scapes/khair...  \n",
       "158  [[Troubleshooting common Janelia Shared Storag...  \n",
       "159  [[Davis Bennett said: if anyone needs to make ...  \n",
       "160  [[.src = canvas.toDataURL()\\n        img.style...  \n",
       "161  [[ are intensity based (due to the lower\\nreso...  \n",
       "162  [[, this.camera)\\n        // Updated, for came...  \n",
       "163  [[SCA 3D Graphics Solution: Direct Volume Rend...  \n",
       "164  [[Mark Kittisopikul said: Sometimes the simpli...  \n",
       "165  [[Robert Lines said: The biggest gotcha we hav...  \n",
       "166  [[Mark Kittisopikul said: We are moving How To...  \n",
       "167  [[William Katz said: Konrad Rokicki Davis Benn...  \n",
       "168  [[ points are handled - pass img as an\\n  Abs...  \n",
       "169  [[Mark Kittisopikul said: Is there anyone else...  \n",
       "170  [[ data.byteLength + \" bytes, converted \" + da...  \n",
       "171  [[Robert Lines said: The biggest gotcha we hav...  \n",
       "172  [[ data.byteLength + \" bytes, converted \" + da...  \n",
       "173  [[Philip Hubbard said: FastAI supports \"augmen...  \n",
       "174  [[63x-left_dorsal. Adding to folder Need Post ...  \n",
       "175  [[ the lines have been loaded previously\\n\\n##...  \n",
       "176  [[Greg Fleishman said: Is anyone aware of any ...  \n",
       "177  [[rootDirectory /nrs/flyem/render/scapes/khair...  \n",
       "178  [[SAGE imagery indexer\\nTitle: SAGE imagery in...  \n",
       "179  [[ data.byteLength + \" bytes, converted \" + da...  \n",
       "180  [[Davis Bennett said: if anyone needs to make ...  \n",
       "181  [[Mark Kittisopikul said: I will await your up...  \n",
       "182  [[ full alignment solve. The following are som...  \n",
       "183  [[Srini said: https://twitter.com/mariescopy/s...  \n",
       "184  [[\\napproximation as a rough alignment. The ri...  \n",
       "185  [[Uploading imagery for NeuronBridge\\nTitle: U...  \n",
       "186  [[How to Synchronize Publication Decisions to ...  \n",
       "187  [[PostgreSQL Performance Tuning\\nTitle: Postgr...  \n",
       "188  [[Mark Kittisopikul said: I posted a compariso...  \n",
       "189  [[William Katz said: tl;dr: Rust is good at so...  \n",
       "190  [[Talk Science with SciComp Software\\nTitle: T...  \n",
       "191  [[Talk Science with SciComp Software\\nTitle: T...  \n",
       "192  [[ (devfs, local, nobrowse)\\n        /dev/disk...  \n",
       "193  [[ the big headaches when trying to use h5py w...  \n",
       "194  [[Sage-based External Website Cross-loader\\nTi...  \n",
       "195  [[Uploading imagery for NeuronBridge\\nTitle: U...  \n",
       "196  [[Flattening\\nTitle: Flattening\\nAuthors: Unkn...  \n",
       "197  [[e-01,... 0.36549678, 0.9152956 ,\\n       1.0...  \n",
       "198  [[SAGE Ajax\\nTitle: SAGE Ajax\\nAuthors: Rob Sv...  \n",
       "199  [[-flylight-color-depth\\n\\nThe code you'll nee...  \n",
       "200  [[Mark Kittisopikul said: \\nHello *<!channel>*...  \n",
       "201  [[William Katz said: Recent blog post on why a...  \n",
       "202  [[Evaluation of components for building apps w...  \n",
       "203  [[ length is just delta.\\n      float deltaDir...  \n",
       "204  [[Davis Bennett said: can anyone make sense of...  \n",
       "205  [[Mark Kittisopikul said: I've seen Synology u...  \n",
       "206  [[Evaluation of components for building apps w...  \n",
       "207  [[Evaluation of components for building apps w...  \n",
       "208  [[Unresponsive NX NoMachine session on a Login...  \n",
       "209  [[William Katz said: https://www.biorxiv.org/c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display(pd.read_parquet(\"ragasTestSet.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c03839e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-10922' coro=<as_completed.<locals>.sema_coro() running at /Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py:37> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:618]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=93 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py:726: ResourceWarning: unclosed event loop <_UnixSelectorEventLoop running=False closed=False debug=False>\n",
      "  _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Exception ignored in: <coroutine object Executor.wrap_callable_with_index.<locals>.wrapped_callable_async at 0x315b04f40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 142, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 466, in _aevolve\n",
      "    simple_question, current_nodes, _ = await self.se._aevolve(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 304, in _aevolve\n",
      "    results = await self.generator_llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: coroutine ignored GeneratorExit\n",
      "Exception ignored in: <coroutine object Executor.wrap_callable_with_index.<locals>.wrapped_callable_async at 0x315b055d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 142, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 304, in _aevolve\n",
      "    results = await self.generator_llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: coroutine ignored GeneratorExit\n",
      "Exception ignored in: <coroutine object Executor.wrap_callable_with_index.<locals>.wrapped_callable_async at 0x315b056c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 142, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/testset/evolutions.py\", line 304, in _aevolve\n",
      "    results = await self.generator_llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/llms/base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: coroutine ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-10920' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py:35> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:618]>\n",
      "ERROR:asyncio:Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-10921' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/ragas/executor.py:35> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:618]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=91>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=98>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=99>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=101>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Exception ignored in: <function ClientResponse.__del__ at 0x176ab3f60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_reqrep.py\", line 891, in __del__\n",
      "    self._connection.release()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 173, in release\n",
      "    self._connector._release(\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 667, in _release\n",
      "    protocol.close()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_proto.py\", line 71, in close\n",
      "    transport.close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 1210, in close\n",
      "    super().close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 795, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 541, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Exception ignored in: <function ClientResponse.__del__ at 0x176ab3f60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_reqrep.py\", line 891, in __del__\n",
      "    self._connection.release()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 173, in release\n",
      "    self._connector._release(\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 667, in _release\n",
      "    protocol.close()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_proto.py\", line 71, in close\n",
      "    transport.close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 1210, in close\n",
      "    super().close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 795, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 541, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Exception ignored in: <function ClientResponse.__del__ at 0x176ab3f60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_reqrep.py\", line 891, in __del__\n",
      "    self._connection.release()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 173, in release\n",
      "    self._connector._release(\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py\", line 667, in _release\n",
      "    protocol.close()\n",
      "  File \"/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client_proto.py\", line 71, in close\n",
      "    transport.close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 1210, in close\n",
      "    super().close()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 795, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 541, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x311d9fce0>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x30c3c0050>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x311d95ca0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client.py:364: ResourceWarning: Unclosed client session <aiohttp.client.ClientSession object at 0x311d9fce0>\n",
      "  _warnings.warn(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/connector.py:119: ResourceWarning: Unclosed connection Connection<ConnectionKey(host='localhost', port=11434, is_ssl=False, ssl=True, proxy=None, proxy_auth=None, proxy_headers_hash=None)>\n",
      "  _warnings.warn(f\"Unclosed connection {self!r}\", ResourceWarning, **kwargs)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client.py:364: ResourceWarning: Unclosed client session <aiohttp.client.ClientSession object at 0x30c3c0050>\n",
      "  _warnings.warn(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/aiohttp/client.py:364: ResourceWarning: Unclosed client session <aiohttp.client.ClientSession object at 0x311d95ca0>\n",
      "  _warnings.warn(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=104 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=106>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=111 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=113 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=117>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=116>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connection.py:398\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connection.py:211\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x3211317f0>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8777): Max retries exceeded with url: /v1/.well-known/ready (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x3211317f0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/weaviate/connect/connection.py:644\u001b[0m, in \u001b[0;36mConnection.wait_for_weaviate\u001b[0;34m(self, startup_period)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mready_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_request_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_CHECK_TIMEOUT\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8777): Max retries exceeded with url: /v1/.well-known/ready (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x3211317f0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerate_answer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticSearchService\n\u001b[1;32m     11\u001b[0m weaviate_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8777\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m service \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSearchService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweaviate_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m (questions_list)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# List to store answers (optional)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/eval/generate_answer.py:33\u001b[0m, in \u001b[0;36mSemanticSearchService.__init__\u001b[0;34m(self, weaviate_url)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weaviate_url):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweaviate_url \u001b[38;5;241m=\u001b[39m weaviate_url\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweaviate_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weaviate_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_engine()\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/eval/generate_answer.py:37\u001b[0m, in \u001b[0;36mSemanticSearchService.get_weaviate_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weaviate_client\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mweaviate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweaviate_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client\u001b[38;5;241m.\u001b[39mis_live():\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeaviate is not live at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweaviate_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/weaviate/client.py:150\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, url, auth_client_secret, timeout_config, proxies, trust_env, additional_headers, startup_period, embedded_options, additional_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m config \u001b[38;5;241m=\u001b[39m Config() \u001b[38;5;28;01mif\u001b[39;00m additional_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m additional_config\n\u001b[1;32m    148\u001b[0m url, embedded_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__parse_url_and_embedded_db(url, embedded_options)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m \u001b[43mConnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_client_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_client_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_valid_timeout_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartup_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartup_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedded_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrcp_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrpc_port_experimental\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification \u001b[38;5;241m=\u001b[39m Classification(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m Schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection)\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/weaviate/connect/connection.py:166\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, url, auth_client_secret, timeout_config, proxies, trust_env, additional_headers, startup_period, connection_config, embedded_db, grcp_port)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startup_period \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     _check_positive_num(startup_period, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup_period\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m, include_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_weaviate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstartup_period\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_sessions(auth_client_secret)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_adapter_to_session(connection_config)\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/weaviate/connect/connection.py:649\u001b[0m, in \u001b[0;36mConnection.wait_for_weaviate\u001b[0;34m(self, startup_period)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (RequestsHTTPError, RequestsConnectionError, ReadTimeout):\n\u001b[0;32m--> 649\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    653\u001b[0m         ready_url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_header(), timeout\u001b[38;5;241m=\u001b[39mINIT_CHECK_TIMEOUT\n\u001b[1;32m    654\u001b[0m     )\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming SemanticSearchParser is your class and it has a method named 'generate_response' that takes a question and returns an answer\n",
    "\n",
    "# Initialize your SemanticSearchParser class\n",
    "# Adjust this step if your class initialization requires different parameters\n",
    "\n",
    "\n",
    "# Now you can import the class\n",
    "\n",
    "from generate_answer import SemanticSearchService\n",
    "\n",
    "weaviate_url = \"http://localhost:8777\"\n",
    "service = SemanticSearchService(weaviate_url)\n",
    "print (questions_list)\n",
    "\n",
    "# List to store answers (optional)\n",
    "answers_list = []\n",
    "\n",
    "\n",
    "# Loop through each question in the questions_list\n",
    "for question in questions_list:\n",
    "    # Use the question as input to get the answer\n",
    "    answer = service.generate_response(question)\n",
    "    \n",
    "    # Print the answer\n",
    "    \n",
    "    # Optionally, append the answer to answers_list for further processing\n",
    "    answers_list.append(answer)\n",
    "\n",
    "print (answers_list)\n",
    "# temp_ans_list = [\"The Janelia Scientific Computing team operates and maintains a world-class computational infrastructure that includes a high-performance compute cluster with over 5000 cores and 300 GPUs. This infrastructure is used to analyze and mine the large amounts of data produced by Janelia's scientists. The team also supports a state-of-the-art storage and compute infrastructure across two data centers, which currently supports over 15 petabytes of scientific data. This data is split across various storage tiers and is connected with an optical fiber ring. The team also maintains a 4500 sq ft data center with significant power and cooling capacity. \\n\\nIn addition to hardware, the team also has deep software skills in a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. These skills are used to help with research and engineering tasks, from quick questions to full software life cycle support. The team's software skills, combined with their domain knowledge in areas such as image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics, allow them to efficiently work with both experimentalists and computer scientists. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are used for various aspects of data analysis and simulation in biological research.\", \"The Janelia Scientific Computing team provides a wide range of support for advanced imaging techniques and image analysis. They offer consultation on experiment design as well as image visualization and processing. They also provide comprehensive image and data analysis support for multiple software packages through hands-on assistance and/or custom-written macros/plugins/scripts for ImageJ/FIJI, MATLAB, Imaris, etc. \\n\\nIn addition, they maintain several computer workstations dedicated to viewing and processing large image datasets acquired with the facility's instruments. These workstations are equipped with a suite of imaging software, including a full version of Imaris, and have robust hardware specifications to handle large datasets. \\n\\nThe team also has deep domain knowledge in image processing, machine learning, data handling, and 3D graphics & visualization, which allows them to efficiently work with experimentalists and computer scientists in various research areas.\", \"The Janelia Scientific Computing team provides world-class computational infrastructure to support the institute's scientific endeavors. They operate and maintain all of Janelias storage and associated backup infrastructure, high performance compute cluster, and all Linux systems. They also manage Janelias data center and backup and disaster recovery resources. The team supports a Linux compute cluster with over 5000 cores and 300 GPUs, and is responsible for maintaining many other Linux servers and workstations. They also handle a significant amount of data, with almost 100TB of new Janelias data being safely backed up every month.\\n\\nIn addition to infrastructure, the Scientific Computing Software team works closely with Janelia's labs and project teams, providing everything from answering quick questions to full software life cycle support. They have a broad range of software skills, including programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in areas like image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics. \\n\\nThe team also identifies opportunities for code reuse, reducing development overhead and support costs across Janelia. They are strong proponents of open science and have created the Open Science Software Initiative. Most of their software is open source and available via GitHub. They also run a Scientific Computing Associates program to embed associates in SciComp and the lab or team they work with. \\n\\nThe team is led by Stephan Preibisch and consists of three teams: Software Engineering headed by Konrad Rokicki, Computational Methods and Solutions, both headed by Stephan Preibisch. They have developed several tools and projects like NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. \\n\\nIn summary, the Janelia Scientific Computing team supports the institute's mission and drives innovation in modern biological research by providing robust computational infrastructure, software support, and developing innovative tools and solutions.\", 'The Janelia Scientific Computing team supports advanced imaging techniques in neuroscience and cell biology through a variety of ways. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. They also develop and maintain a range of software tools and applications that aid in these areas. Some of these tools include NeuronBridge for finding neuron matches across modalities, HortaCloud for cloud-based collaborative annotation, VVD Viewer for volumetric rendering of 3D/4D microscopy data, and BigStitcher for efficient alignment of multi-tile and multi-angle image datasets. They also work closely with labs and project teams, providing full software life cycle support.', \"The Janelia Scientific Computing team supports modern biological research in several ways. They maintain a world-class computational infrastructure, including storage and backup infrastructure, a high-performance compute cluster, and all Linux systems. They also manage Janelia's data center and backup and disaster recovery resources. The team supports data storage infrastructure for storing and accessing scientific data, with over 15 petabytes of scientific data split across various storage tiers. They also support a Linux compute cluster with over 5000 cores and 300 GPUs, and maintain many other Linux servers and workstations. \\n\\nIn addition to infrastructure support, the Scientific Computing Software team works closely with Janelia's labs, project teams, and shared resources to help with research and engineering tasks. They provide everything from answering quick questions to full software life cycle support. The team's software skills span a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. They are strong proponents of open science and most of their software is open source and available via GitHub. They also run the Scientific Computing Associates program, which offers challenging assignments for those interested in computational science.\", \"The Janelia Scientific Computing team provides comprehensive support for advanced imaging techniques in neuroscience, cell biology, and bioinformatics through a variety of collaborations and custom software tools. They work closely with Janelia's labs, project teams, and shared resources to assist with research and engineering tasks. This can range from answering quick questions to providing full software life cycle support.\\n\\nThe team's software skills cover a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. Many team members have backgrounds in biology, enabling them to work efficiently with experimentalists and computer scientists.\\n\\nThe team is also involved in various projects and tools such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are designed to support advanced imaging techniques and data analysis in neuroscience, cell biology, and bioinformatics.\\n\\nFurthermore, the team is a strong proponent of open science and has teamed up with the Computation & Theory research area to create the Open Science Software Initiative. Most of their software is open source and available via GitHub, promoting collaboration and knowledge sharing. They also run the Scientific Computing Associates program, which embeds associates in SciComp and the lab or team they work with.\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699a438",
   "metadata": {},
   "source": [
    "Add values from the list of JaneliaGPT responses to the dataframe[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = None\n",
    "# Assuming df is your DataFrame and answers_list is a list with values to populate the 'Answer' column\n",
    "if len(df) == len(answers_list):\n",
    "    df['answer'] = answers_list\n",
    "else:\n",
    "    print(\"The length of answers_list does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "display(df)\n",
    "# df.to_json('WithAnswersDatasetFromTestTxt.json', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095ca61",
   "metadata": {},
   "source": [
    "Preprocess the dataframe to conver to a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fix the issue with the answers_list not bsjdkflasdjjfklasdkl fasdjkfh jkas\n",
    "df_fix = df\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['question', 'ground_truth', 'answer', 'contexts']\n",
    "\n",
    "# Reassign df to a DataFrame containing only the columns to keep\n",
    "\n",
    "df_fix= df_fix[columns_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df is already defined and contains the necessary columns\n",
    "\n",
    "# Convert 'question' and 'answer' to lists of strings if they are not already\n",
    "# df_fix['question'] = df_fix['question'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "# df_fix['answer'] = df_fix['answer'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "# Ensure 'contexts' and 'ground_truth' are lists of lists of strings\n",
    "# This step assumes 'contexts' and 'ground_truth' are already in the correct format\n",
    "# If not, you would need to apply a similar conversion as above, ensuring each element is a list\n",
    "\n",
    "# Example conversion if 'contexts' and 'ground_truth' were not already lists of lists\n",
    "df_fix['contexts'] = df_fix['contexts'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "df_fix['ground_truth'] = df_fix['ground_truth'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "display(df_fix)\n",
    "# Now df should be in the correct format for training\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset_fix = Dataset.from_pandas(df_fix)\n",
    "dataset_fix = dataset_fix.remove_columns(['__index_level_0__'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e5b0c",
   "metadata": {},
   "source": [
    "Convert dataframe to Dataset and compare to a vaild Dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features\n",
    "\n",
    "# Assuming dataset_fix and amnesty_qa[\"eval\"] are your datasets\n",
    "features_dataset_fix = dataset_fix.features\n",
    "# features_amnesty_eval = amnesty_qa[\"eval\"].features\n",
    "def format_columns(example):\n",
    "    # Format 'question', 'answer', and 'ground_truths' columns to single values\n",
    "    for column in ['ground_truth', 'answer', 'question']:\n",
    "        if column in example and example[column]:\n",
    "            example[column] = example[column]\n",
    "    \n",
    "    # Correctly format 'contexts' column to a list of list of strings\n",
    "    if 'contexts' in example:\n",
    "        # Ensure 'contexts' is a list of strings (not a list of lists)\n",
    "        if isinstance(example['contexts'], list):\n",
    "            # If the items are lists (or other non-string types), flatten and convert to strings\n",
    "            example['contexts'] = [str(item) for sublist in example['contexts'] for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        else:\n",
    "            # If 'contexts' is not a list, convert it into a list of a single string\n",
    "            example['contexts'] = [str(example['contexts'])]\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to both datasets\n",
    "dataset_fix = dataset_fix.map(format_columns)\n",
    "# Direct comparison of data types\n",
    "\"\"\"if set(features_dataset_fix.keys()) == set(features_amnesty_eval.keys()):\n",
    "    all_types_match = True\n",
    "    for key in features_dataset_fix.keys():\n",
    "        type_dataset_fix = type(features_dataset_fix[key]).__name__\n",
    "        type_amnesty_eval = type(features_amnesty_eval[key]).__name__\n",
    "        if type_dataset_fix != type_amnesty_eval:\n",
    "            print(f\"Data type for feature '{key}' differs between datasets. dataset_fix: {type_dataset_fix}, amnesty_qa['eval']: {type_amnesty_eval}\")\n",
    "            all_types_match = False\n",
    "    if all_types_match:\n",
    "        print(\"The data types of all features in both datasets match.\")\n",
    "else:\n",
    "    print(\"The Features of the datasets differ in their keys.\")\"\"\"\n",
    "\"\"\"\n",
    "print (dataset_fix[\"question\"])\n",
    "print (dataset_fix[\"answer\"])\n",
    "print (dataset_fix[\"ground_truth\"])\n",
    "print (dataset_fix[\"contexts\"])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric()\n",
    "contextual_recall = ContextualRecallMetric()\n",
    "contextual_relevancy = ContextualRelevancyMetric()\n",
    "\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "modified_items = []\n",
    "\n",
    "# Step 2: Iterate over each item in dataset_fix\n",
    "for item in dataset_fix:\n",
    "    # Step 3: Create a new LLMTestCase instance with modified fields\n",
    "    modified_item = LLMTestCase(\n",
    "        input=item[\"question\"],\n",
    "        actual_output=item[\"answer\"],\n",
    "        expected_output=item[\"ground_truth\"],\n",
    "        retrieval_context=item[\"contexts\"]\n",
    "    )\n",
    "    # Step 4: Append the modified item to the list\n",
    "    modified_items.append(modified_item)\n",
    "# Assuming dataset_fix supports item assignment\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    test_cases=[modified_items],\n",
    "    metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cc907",
   "metadata": {},
   "source": [
    "Run evaluation to gather the below metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "eval_llm = Ollama(model=\"llama3\")\n",
    "# Will reutrn a dataframe with the metrics\n",
    "# Returns error for now because answers column is missing\n",
    "# Error is misleading, fix dataset first and make it match the docs example dataset\n",
    "\n",
    "result = evaluate(\n",
    "    dataset_fix,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=eval_llm,\n",
    ")\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22595923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "result.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36276909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MetricsManager:\n",
    "    def __init__(self, file_path='metrics.json'):\n",
    "        self.file_path = file_path\n",
    "        self.metrics = self.load_metrics()\n",
    "\n",
    "    def load_metrics(self):\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, 'r') as file:\n",
    "                return json.load(file)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def save_metrics(self):\n",
    "        with open(self.file_path, 'w') as file:\n",
    "            json.dump(self.metrics, file, indent=4)\n",
    "\n",
    "    def update_metrics(self, trial_name, context_precision, faithfulness, answer_relevancy, context_recall):\n",
    "        averages = {\n",
    "            'context_precision': context_precision,\n",
    "            'faithfulness': faithfulness,\n",
    "            'answer_relevancy': answer_relevancy,\n",
    "            'context_recall': context_recall\n",
    "        }\n",
    "        self.metrics[trial_name] = averages\n",
    "        self.save_metrics()\n",
    "\n",
    "    def render_table(self):\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics available for plotting.\")\n",
    "            return\n",
    "\n",
    "        trials, metrics, averages = [], [], []\n",
    "        for trial_name, metrics_averages in self.metrics.items():\n",
    "            for metric, average in metrics_averages.items():\n",
    "                trials.append(trial_name)\n",
    "                metrics.append(metric)\n",
    "                averages.append(round(average, 4))\n",
    "\n",
    "        Eval_Categories = pd.DataFrame({\n",
    "            'Trial': trials,\n",
    "            'Metric': metrics,\n",
    "            'Average': averages\n",
    "        })\n",
    "\n",
    "        if Eval_Categories.empty:\n",
    "            print(\"No metrics data to plot.\")\n",
    "            return\n",
    "\n",
    "        fig = px.bar(\n",
    "            Eval_Categories,\n",
    "            x='Trial',\n",
    "            y='Average',\n",
    "            color='Metric',\n",
    "            barmode='group',\n",
    "            text='Average',\n",
    "            category_orders={\"Metric\": [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\"]},\n",
    "            labels={\n",
    "                \"Average\": \"Average Score\",\n",
    "                \"Metric\": \"Metric\",\n",
    "                \"Trial\": \"Trial Name\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            width=1000,\n",
    "            height=600,\n",
    "            title=\"<b>Average Scores of Evaluation Metrics by Trial</b>\",\n",
    "            xaxis_title=\"Trial Name\",\n",
    "            yaxis_title=\"Average Score\",\n",
    "            font=dict(size=15)\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "MetricsManager()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
